{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778ce290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kh√¥ng t√¨m th·∫•y n·ªôi dung b√†i b√°o.\n",
      "Kh√¥ng c√≥ n·ªôi dung ƒë·ªÉ hi·ªÉn th·ªã.\n",
      "None\n",
      "<html>\n",
      " <body>\n",
      "  <script>\n",
      "   document.cookie=\"D1N=4492fe22b0f8939a6e2cac59d196c3e2\"+\"; expires=Fri, 31 Dec 2099 23:59:59 GMT; path=/\";window.location.reload(true);\n",
      "  </script>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "from transformers import pipeline\n",
    "from fastapi import FastAPI\n",
    "from bson import ObjectId\n",
    "url = \"https://laodong.vn/xa-hoi/chuan-bi-khoi-cong-loat-du-an-giao-thong-lon-1586625.ldo\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "content_div = soup.select_one(\"div.detail-content, div.article-content\")\n",
    "  # ho·∫∑c th·ª≠ \"detail-content\", \"content-detail\"\n",
    "if content_div:\n",
    "    article_text = content_div.get_text(strip=True)\n",
    "    print(article_text)\n",
    "else:\n",
    "    article_text = None\n",
    "    print(\"Kh√¥ng t√¨m th·∫•y n·ªôi dung b√†i b√°o.\")\n",
    "if article_text:\n",
    "    print(\"ƒê√£ l·∫•y ƒë∆∞·ª£c n·ªôi dung.\")\n",
    "else:\n",
    "    print(\"Kh√¥ng c√≥ n·ªôi dung ƒë·ªÉ hi·ªÉn th·ªã.\")\n",
    "print(article_text)\n",
    "print(soup.prettify())\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"traffic_news\"]\n",
    "collection = db[\"articles\"]\n",
    "document = {\n",
    "    \"url\": url,\n",
    "    \"title\": \"Chu·∫©n b·ªã kh·ªüi c√¥ng lo·∫°t d·ª± √°n giao th√¥ng l·ªõn\",\n",
    "    \"content\": article_text,\n",
    "    \"date\": datetime.now()\n",
    "}\n",
    "collection.insert_one(document)\n",
    "summarizer = pipeline(\"summarization\", model=\"VietAI/vit5-base-vietnews-summarization\")\n",
    "summary = summarizer(article_text, max_length=200, min_length=50, do_sample=False)[0][\"summary_text\"]\n",
    "print(\"T√≥m t·∫Øt:\", summary)\n",
    "app = FastAPI()\n",
    "@app.get(\"/summary\")\n",
    "def get_summary(article_id: str):\n",
    "    doc = collection.find_one({\"_id\": ObjectId(article_id)})\n",
    "    summary = summarizer(doc[\"content\"], max_length=200)[0][\"summary_text\"]\n",
    "    return {\"title\": doc[\"title\"], \"summary\": summary}\n",
    "    collection.update_one({\"_id\": doc[\"_id\"]}, {\"$set\": {\"summary\": summary}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4826750b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö† D√πng t√≥m t·∫Øt ƒë∆°n gi·∫£n (3 c√¢u ƒë·∫ßu)\n",
      "‚úì ƒê√£ k·∫øt n·ªëi MongoDB\n",
      "\n",
      "============================================================\n",
      "B·∫ÆT ƒê·∫¶U SCRAPE\n",
      "============================================================\n",
      "\n",
      "ƒêang kh·ªüi ƒë·ªông Edge...\n",
      "ƒêang t·∫£i trang: https://laodong.vn/xa-hoi/chuan-bi-khoi-cong-loat-du-an-giao-thong-lon-1586625.ldo\n",
      "ƒêang ƒë·ª£i n·ªôi dung load...\n",
      "‚úì T√¨m th·∫•y n·ªôi dung v·ªõi: article\n",
      "‚úì ƒê√£ l·∫•y ƒë∆∞·ª£c 4137 k√Ω t·ª±\n",
      "‚úì ƒê√£ ƒë√≥ng Edge\n",
      "\n",
      "‚úì Ti√™u ƒë·ªÅ: Chu·∫©n b·ªã kh·ªüi c√¥ng lo·∫°t d·ª± √°n giao th√¥ng l·ªõn...\n",
      "‚úì N·ªôi dung: 4137 k√Ω t·ª±\n",
      "\n",
      "N·ªôi dung ƒë·∫ßu ti√™n:\n",
      "Chu·∫©n b·ªã kh·ªüi c√¥ng lo·∫°t d·ª± √°n giao th√¥ng l·ªõn Th·ª© hai, 06/10/2025 12:30 (GMT+7) TPHCM chu·∫©n b·ªã kh·ªüi c√¥ng lo·∫°t d·ª± √°n giao th√¥ng l·ªõn tr·ªã gi√° h√†ng ch·ª•c ngh√¨n t·ªâ ƒë·ªìng, t·ª´ Metro s·ªë 2, cao t·ªëc TPHCM - M·ªôc B√†i ƒë·∫øn V√†nh ƒëai 2, k·ª≥ v·ªçng m·ªü r·ªông c·ª≠a ng√µ v√† th√∫c ƒë·∫©y ph√°t tri·ªÉn ƒë√¥ th·ªã. √ôn t·∫Øc t·∫°i c·ª≠a ng√µ ph√≠a ƒê√¥n...\n",
      "\n",
      "============================================================\n",
      "T√ìM T·∫ÆT:\n",
      "============================================================\n",
      "Chu·∫©n b·ªã kh·ªüi c√¥ng lo·∫°t d·ª± √°n giao th√¥ng l·ªõn Th·ª© hai, 06/10/2025 12:30 (GMT+7) TPHCM chu·∫©n b·ªã kh·ªüi c√¥ng lo·∫°t d·ª± √°n giao th√¥ng l·ªõn tr·ªã gi√° h√†ng ch·ª•c ngh√¨n t·ªâ ƒë·ªìng, t·ª´ Metro s·ªë 2, cao t·ªëc TPHCM - M·ªôc B√†i ƒë·∫øn V√†nh ƒëai 2, k·ª≥ v·ªçng m·ªü r·ªông c·ª≠a ng√µ v√† th√∫c ƒë·∫©y ph√°t tri·ªÉn ƒë√¥ th·ªã. √ôn t·∫Øc t·∫°i c·ª≠a ng√µ ph√≠a ƒê√¥ng B·∫Øc TPHCM. ·∫¢nh: Anh T√∫ Trong s·ªë c√°c c√¥ng tr√¨nh s·∫Øp kh·ªüi c√¥ng, tuy·∫øn Metro s·ªë 2 (B·∫øn Th√†nh - Tham L∆∞∆°ng) nh·∫≠n ƒë∆∞·ª£c s·ª± quan t√¢m v√† k·ª≥ v·ªçng l·ªõn c·ªßa ng∆∞·ªùi d√¢n.\n",
      "============================================================\n",
      "\n",
      "‚úì ƒê√£ l∆∞u v√†o MongoDB v·ªõi ID: 68e677125ec0ad732b49c4f3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from bson import ObjectId\n",
    "import time\n",
    "\n",
    "# Kh·ªüi t·∫°o model (fallback mode - kh√¥ng c·∫ßn PyTorch)\n",
    "print(\"‚ö† D√πng t√≥m t·∫Øt ƒë∆°n gi·∫£n (3 c√¢u ƒë·∫ßu)\")\n",
    "USE_AI_SUMMARY = False\n",
    "\n",
    "# K·∫øt n·ªëi MongoDB\n",
    "try:\n",
    "    client = MongoClient(\"mongodb://localhost:27017/\", serverSelectionTimeoutMS=5000)\n",
    "    client.server_info()\n",
    "    db = client[\"traffic_news\"]\n",
    "    collection = db[\"articles\"]\n",
    "    print(\"‚úì ƒê√£ k·∫øt n·ªëi MongoDB\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó L·ªói k·∫øt n·ªëi MongoDB: {e}\")\n",
    "    collection = None\n",
    "\n",
    "def scrape_article_selenium(url):\n",
    "    \"\"\"L·∫•y n·ªôi dung b√†i b√°o b·∫±ng Selenium\"\"\"\n",
    "    driver = None\n",
    "    try:\n",
    "        # Setup Edge headless\n",
    "        edge_options = webdriver.EdgeOptions()\n",
    "        edge_options.add_argument(\"--headless\")  # Ch·∫°y ng·∫ßm, kh√¥ng m·ªü c·ª≠a s·ªï\n",
    "        edge_options.add_argument(\"--no-sandbox\")\n",
    "        edge_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        edge_options.add_argument(\"--disable-gpu\")\n",
    "        edge_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\")\n",
    "        \n",
    "        print(\"ƒêang kh·ªüi ƒë·ªông Edge...\")\n",
    "        driver = webdriver.Edge(options=edge_options)\n",
    "        driver.set_page_load_timeout(30)\n",
    "        \n",
    "        print(f\"ƒêang t·∫£i trang: {url}\")\n",
    "        driver.get(url)\n",
    "        \n",
    "        # ƒê·ª£i JavaScript load xong (ƒë·ª£i n·ªôi dung xu·∫•t hi·ªán)\n",
    "        print(\"ƒêang ƒë·ª£i n·ªôi dung load...\")\n",
    "        wait = WebDriverWait(driver, 15)\n",
    "        \n",
    "        # Th·ª≠ nhi·ªÅu selector (Lao ƒê·ªông th∆∞·ªùng d√πng)\n",
    "        selectors = [\n",
    "            (By.CSS_SELECTOR, \"div.detail-content-body\"),\n",
    "            (By.CSS_SELECTOR, \"div.detail__content\"),\n",
    "            (By.CSS_SELECTOR, \"article.detail\"),\n",
    "            (By.CSS_SELECTOR, \"div.article-content\"),\n",
    "            (By.XPATH, \"//div[contains(@class, 'detail') and contains(@class, 'content')]\"),\n",
    "            (By.TAG_NAME, \"article\")\n",
    "        ]\n",
    "        \n",
    "        content_element = None\n",
    "        used_selector = None\n",
    "        \n",
    "        for by, selector in selectors:\n",
    "            try:\n",
    "                content_element = wait.until(\n",
    "                    EC.presence_of_element_located((by, selector))\n",
    "                )\n",
    "                if content_element and len(content_element.text) > 200:\n",
    "                    used_selector = selector\n",
    "                    print(f\"‚úì T√¨m th·∫•y n·ªôi dung v·ªõi: {selector}\")\n",
    "                    break\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        if not content_element:\n",
    "            # Fallback: ƒê·ª£i 3s r·ªìi l·∫•y to√†n b·ªô body\n",
    "            print(\"‚ö† Kh√¥ng t√¨m th·∫•y selector c·ª• th·ªÉ, l·∫•y to√†n b·ªô body...\")\n",
    "            time.sleep(3)\n",
    "            content_element = driver.find_element(By.TAG_NAME, \"body\")\n",
    "        \n",
    "        # L·∫•y title\n",
    "        try:\n",
    "            title_element = driver.find_element(By.CSS_SELECTOR, \"h1.detail__title, h1\")\n",
    "            title = title_element.text.strip()\n",
    "        except:\n",
    "            title = driver.title\n",
    "        \n",
    "        # L·∫•y n·ªôi dung\n",
    "        article_text = content_element.text.strip()\n",
    "        \n",
    "        # L√†m s·∫°ch (lo·∫°i b·ªè navigation, ads...)\n",
    "        lines = [line.strip() for line in article_text.split(\"\\n\") if line.strip()]\n",
    "        # Lo·∫°i b·ªè c√°c d√≤ng ng·∫Øn (th∆∞·ªùng l√† menu, ads)\n",
    "        lines = [line for line in lines if len(line) > 30]\n",
    "        article_text = \" \".join(lines)\n",
    "        \n",
    "        print(f\"‚úì ƒê√£ l·∫•y ƒë∆∞·ª£c {len(article_text)} k√Ω t·ª±\")\n",
    "        \n",
    "        return title, article_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó L·ªói Selenium: {e}\")\n",
    "        print(f\"   Tip: ƒê·∫£m b·∫£o Edge ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t ho·∫∑c c√†i Edge WebDriver\")\n",
    "        return None, None\n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "            print(\"‚úì ƒê√£ ƒë√≥ng Edge\")\n",
    "\n",
    "def simple_summary(text, num_sentences=3):\n",
    "    \"\"\"T√≥m t·∫Øt ƒë∆°n gi·∫£n b·∫±ng c√°ch l·∫•y N c√¢u ƒë·∫ßu\"\"\"\n",
    "    sentences = text.replace(\"?\", \".\").replace(\"!\", \".\").split(\". \")\n",
    "    sentences = [s.strip() for s in sentences if len(s.strip()) > 20]\n",
    "    return \". \".join(sentences[:num_sentences]) + \".\"\n",
    "\n",
    "# Scraping\n",
    "url = \"https://laodong.vn/xa-hoi/chuan-bi-khoi-cong-loat-du-an-giao-thong-lon-1586625.ldo\"\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"B·∫ÆT ƒê·∫¶U SCRAPE\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "title, article_text = scrape_article_selenium(url)\n",
    "\n",
    "if article_text and len(article_text) > 100:\n",
    "    print(f\"\\n‚úì Ti√™u ƒë·ªÅ: {title[:100]}...\")\n",
    "    print(f\"‚úì N·ªôi dung: {len(article_text)} k√Ω t·ª±\")\n",
    "    print(f\"\\nN·ªôi dung ƒë·∫ßu ti√™n:\\n{article_text[:300]}...\\n\")\n",
    "    \n",
    "    # T√≥m t·∫Øt\n",
    "    summary = simple_summary(article_text, num_sentences=3)\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"T√ìM T·∫ÆT:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(summary)\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # L∆∞u v√†o MongoDB\n",
    "    if collection is not None:\n",
    "        try:\n",
    "            document = {\n",
    "                \"url\": url,\n",
    "                \"title\": title,\n",
    "                \"content\": article_text,\n",
    "                \"summary\": summary,\n",
    "                \"date\": datetime.now(),\n",
    "                \"scraped_at\": datetime.now()\n",
    "            }\n",
    "            result = collection.insert_one(document)\n",
    "            print(f\"‚úì ƒê√£ l∆∞u v√†o MongoDB v·ªõi ID: {result.inserted_id}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó L·ªói khi l∆∞u MongoDB: {e}\\n\")\n",
    "else:\n",
    "    print(\"‚úó Kh√¥ng l·∫•y ƒë∆∞·ª£c n·ªôi dung ƒë·ªß d√†i\\n\")\n",
    "\n",
    "# FastAPI\n",
    "app = FastAPI(title=\"Traffic News API\")\n",
    "\n",
    "@app.get(\"/\")\n",
    "def root():\n",
    "    return {\n",
    "        \"message\": \"Traffic News Summarization API\",\n",
    "        \"endpoints\": {\n",
    "            \"GET /articles\": \"Danh s√°ch b√†i vi·∫øt\",\n",
    "            \"GET /summary/{article_id}\": \"T√≥m t·∫Øt b√†i vi·∫øt\",\n",
    "            \"POST /scrape\": \"Scrape URL m·ªõi (body: {\\\"url\\\": \\\"...\\\"})\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "@app.get(\"/summary/{article_id}\")\n",
    "def get_summary(article_id: str):\n",
    "    \"\"\"L·∫•y t√≥m t·∫Øt b√†i vi·∫øt theo ID\"\"\"\n",
    "    if collection is None:\n",
    "        raise HTTPException(status_code=503, detail=\"MongoDB kh√¥ng kh·∫£ d·ª•ng\")\n",
    "    \n",
    "    try:\n",
    "        doc = collection.find_one({\"_id\": ObjectId(article_id)})\n",
    "        \n",
    "        if not doc:\n",
    "            raise HTTPException(status_code=404, detail=\"Kh√¥ng t√¨m th·∫•y b√†i vi·∫øt\")\n",
    "        \n",
    "        # N·∫øu ƒë√£ c√≥ summary\n",
    "        if \"summary\" in doc and doc[\"summary\"]:\n",
    "            return {\n",
    "                \"id\": str(doc[\"_id\"]),\n",
    "                \"title\": doc[\"title\"],\n",
    "                \"summary\": doc[\"summary\"],\n",
    "                \"content_length\": len(doc.get(\"content\", \"\")),\n",
    "                \"cached\": True\n",
    "            }\n",
    "        \n",
    "        # T·∫°o m·ªõi\n",
    "        if not doc.get(\"content\"):\n",
    "            raise HTTPException(status_code=400, detail=\"B√†i vi·∫øt kh√¥ng c√≥ n·ªôi dung\")\n",
    "        \n",
    "        summary = simple_summary(doc[\"content\"], num_sentences=3)\n",
    "        \n",
    "        # C·∫≠p nh·∫≠t v√†o DB\n",
    "        collection.update_one(\n",
    "            {\"_id\": doc[\"_id\"]}, \n",
    "            {\"$set\": {\"summary\": summary}}\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"id\": str(doc[\"_id\"]),\n",
    "            \"title\": doc[\"title\"],\n",
    "            \"summary\": summary,\n",
    "            \"content_length\": len(doc[\"content\"]),\n",
    "            \"cached\": False\n",
    "        }\n",
    "        \n",
    "    except HTTPException:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"L·ªói: {str(e)}\")\n",
    "\n",
    "@app.get(\"/articles\")\n",
    "def list_articles(limit: int = 10):\n",
    "    \"\"\"Li·ªát k√™ b√†i vi·∫øt\"\"\"\n",
    "    if collection is None:\n",
    "        raise HTTPException(status_code=503, detail=\"MongoDB kh√¥ng kh·∫£ d·ª•ng\")\n",
    "    \n",
    "    docs = collection.find().sort(\"date\", -1).limit(limit)\n",
    "    articles = []\n",
    "    for doc in docs:\n",
    "        articles.append({\n",
    "            \"id\": str(doc[\"_id\"]),\n",
    "            \"title\": doc[\"title\"],\n",
    "            \"date\": doc[\"date\"].isoformat(),\n",
    "            \"url\": doc.get(\"url\", \"\"),\n",
    "            \"has_summary\": \"summary\" in doc\n",
    "        })\n",
    "    return {\"articles\": articles, \"count\": len(articles)}\n",
    "\n",
    "@app.post(\"/scrape\")\n",
    "def scrape_new_article(data: dict):\n",
    "    \"\"\"Scrape URL m·ªõi\"\"\"\n",
    "    url = data.get(\"url\")\n",
    "    if not url:\n",
    "        raise HTTPException(status_code=400, detail=\"Thi·∫øu URL\")\n",
    "    \n",
    "    title, content = scrape_article_selenium(url)\n",
    "    \n",
    "    if not content:\n",
    "        raise HTTPException(status_code=400, detail=\"Kh√¥ng scrape ƒë∆∞·ª£c n·ªôi dung\")\n",
    "    \n",
    "    summary = simple_summary(content, num_sentences=3)\n",
    "    \n",
    "    if collection:\n",
    "        doc = {\n",
    "            \"url\": url,\n",
    "            \"title\": title,\n",
    "            \"content\": content,\n",
    "            \"summary\": summary,\n",
    "            \"date\": datetime.now()\n",
    "        }\n",
    "        result = collection.insert_one(doc)\n",
    "        return {\n",
    "            \"id\": str(result.inserted_id),\n",
    "            \"title\": title,\n",
    "            \"summary\": summary,\n",
    "            \"content_length\": len(content)\n",
    "        }\n",
    "    \n",
    "    return {\"title\": title, \"summary\": summary, \"content_length\": len(content)}\n",
    "\n",
    "# Ch·∫°y: uvicorn filename:app --reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aab5ff3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ƒê√£ k·∫øt n·ªëi MongoDB v·ªõi full-text search\n",
      "‚ùå L·ªánh kh√¥ng h·ª£p l·ªá: --f=c:\\Users\\ASUS\\AppData\\Roaming\\jupyter\\runtime\\kernel-v31c77da465df6157aa337d65d287250ab347656fe.json\n",
      "S·ª≠ d·ª•ng: python C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python313\\site-packages\\ipykernel_launcher.py [scrape|stats]\n",
      "\n",
      "======================================================================\n",
      "üöÄ TP.HCM TRAFFIC NEWS SCRAPER\n",
      "======================================================================\n",
      "\n",
      "[1] Scrape 5 b√†i/ngu·ªìn\n",
      "[2] Xem th·ªëng k√™\n",
      "\n",
      "======================================================================\n",
      "üöÄ TP.HCM TRAFFIC NEWS INTELLIGENCE SYSTEM 2025\n",
      "======================================================================\n",
      "üìÖ Th·ªùi gian: 2025-01-01 ‚Üí 2025-10-09\n",
      "üì∞ ƒêang scrape 5 b√†i/ngu·ªìn...\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "üöÄ CHI·∫æN D·ªäCH SCRAPING: GIAO TH√îNG TP.HCM 2025\n",
      "   Th·ªùi gian: 2025-01-01 ‚Üí 2025-10-09\n",
      "======================================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "üîç B·∫ÆT ƒê·∫¶U: B√°o Lao ƒê·ªông\n",
      "============================================================\n",
      "\n",
      "üì° B∆∞·ªõc 1: L·∫•y URLs t·ª´ RSS...\n",
      "  üì° ƒêang ƒë·ªçc RSS: https://laodong.vn/rss/giao-thong.rss\n",
      "  ‚úì T√¨m th·∫•y 0 URLs t·ª´ RSS\n",
      "  üì° ƒêang ƒë·ªçc RSS: https://laodong.vn/rss/xa-hoi.rss\n",
      "  ‚úì T√¨m th·∫•y 0 URLs t·ª´ RSS\n",
      "\n",
      "üìÑ B∆∞·ªõc 2: L·∫•y URLs t·ª´ trang danh s√°ch...\n",
      "  üìÑ Trang 1: https://laodong.vn/giao-thong/...\n",
      "  ‚úì T√¨m th·∫•y 7 URLs\n",
      "  üìÑ Trang 2: https://laodong.vn/giao-thong/trang-2.htm...\n",
      "  ‚úì T√¨m th·∫•y 0 URLs\n",
      "\n",
      "üìä T·ªïng c·ªông: 4 URLs duy nh·∫•t\n",
      "\n",
      "üì∞ B∆∞·ªõc 3: Scrape chi ti·∫øt...\n",
      "\n",
      "[1/4] https://laodong.vn/giao-thong/tau-hoa-phuong-do-duoc-gan-bien-cong-tri...\n",
      "  ‚úÖ ƒê√£ l∆∞u: T√†u Hoa Ph∆∞·ª£ng ƒê·ªè ƒë∆∞·ª£c g·∫Øn bi·ªÉn c√¥ng tr√¨nh ch√†o m·ª´...\n",
      "  üìÇ Danh m·ª•c: projects\n",
      "  üìÖ Ng√†y: 2025-10-09\n",
      "\n",
      "[2/4] https://www.dmca.com/Protection/Status.aspx?ID=2a0ef338-d2a6-4097-82db...\n",
      "  ‚úó L·ªói scrape: Message: timeout: Timed out receiving message from renderer: 28.900\n",
      "  (Session info: MicrosoftEdge=141.0.3537.57)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x7ff6a5b32a15+51877]\n",
      "\tGetHandleVerifier [0x0x7ff6a5b32a74+51972]\n",
      "\tGetHandleVerifier [0x0x7ff6a5ce93c5+1848405]\n",
      "\t(No symbol) [0x0x7ff6a589220b]\n",
      "\t(No symbol) [0x0x7ff6a5891f81]\n",
      "\t(No symbol) [0x0x7ff6a588feeb]\n",
      "\t(No symbol) [0x0x7ff6a589070b]\n",
      "\t(No symbol) [0x0x7ff6a589c522]\n",
      "\t(No symbol) [0x0x7ff6a58af76d]\n",
      "\t(No symbol) [0x0x7ff6a58b5a4a]\n",
      "\t(No symbol) [0x0x7ff6a58910d8]\n",
      "\t(No symbol) [0x0x7ff6a5890e0a]\n",
      "\t(No symbol) [0x0x7ff6a58af51e]\n",
      "\t(No symbol) [0x0x7ff6a592f59e]\n",
      "\t(No symbol) [0x0x7ff6a5911df3]\n",
      "\t(No symbol) [0x0x7ff6a58e5b36]\n",
      "\t(No symbol) [0x0x7ff6a58e4d80]\n",
      "\t(No symbol) [0x0x7ff6a58e5973]\n",
      "\t(No symbol) [0x0x7ff6a59ae4a5]\n",
      "\t(No symbol) [0x0x7ff6a59aa75d]\n",
      "\tGetHandleVerifier [0x0x7ff6a5b5d253+226019]\n",
      "\tGetHandleVerifier [0x0x7ff6a5b4c421+156849]\n",
      "\tGetHandleVerifier [0x0x7ff6a5b54919+190889]\n",
      "\tGetHandleVerifier [0x0x7ff6a5b39b54+80868]\n",
      "\tGetHandleVerifier [0x0x7ff6a5b39ca3+81203]\n",
      "\tGetHandleVerifier [0x0x7ff6a5b28226+8886]\n",
      "\tBaseThreadInitThunk [0x0x7fff2502259d+29]\n",
      "\tRtlUserThreadStart [0x0x7fff25a6af78+40]\n",
      "\n",
      "\n",
      "[3/4] https://laodong.vn/giao-thong/da-nang-phan-quyen-to-chuc-van-hanh-giao...\n",
      "  ‚úÖ ƒê√£ l∆∞u: ƒê√† N·∫µng ph√¢n quy·ªÅn t·ªï ch·ª©c v·∫≠n h√†nh giao th√¥ng ƒë·∫øn...\n",
      "  üìÇ Danh m·ª•c: projects\n",
      "  üìÖ Ng√†y: 2025-10-09\n",
      "\n",
      "[4/4] https://laodong.vn/giao-thong/chieu-810-ha-noi-con-13-vi-tri-ngap-sau-...\n",
      "  ‚úÖ ƒê√£ l∆∞u: Chi·ªÅu 8.10, H√† N·ªôi c√≤n 13 v·ªã tr√≠ ng·∫≠p s√¢u...\n",
      "  üìÇ Danh m·ª•c: projects\n",
      "  üìÖ Ng√†y: 2025-10-09\n",
      "\n",
      "‚úÖ Ho√†n th√†nh B√°o Lao ƒê·ªông\n",
      "   ƒê√£ l∆∞u: 3 / 3 b√†i\n",
      "\n",
      "============================================================\n",
      "üîç B·∫ÆT ƒê·∫¶U: VnExpress\n",
      "============================================================\n",
      "\n",
      "üì° B∆∞·ªõc 1: L·∫•y URLs t·ª´ RSS...\n",
      "  üì° ƒêang ƒë·ªçc RSS: https://vnexpress.net/rss/giao-thong.rss\n",
      "  ‚úì T√¨m th·∫•y 20 URLs t·ª´ RSS\n",
      "\n",
      "üìÑ B∆∞·ªõc 2: L·∫•y URLs t·ª´ trang danh s√°ch...\n",
      "  üìÑ Trang 1: https://vnexpress.net/giao-thong...\n",
      "  ‚úì T√¨m th·∫•y 0 URLs\n",
      "\n",
      "üìä T·ªïng c·ªông: 20 URLs duy nh·∫•t\n",
      "\n",
      "üì∞ B∆∞·ªõc 3: Scrape chi ti·∫øt...\n",
      "\n",
      "[1/5] https://vnexpress.net/hai-cau-day-vang-lon-nhat-cao-toc-ben-luc-sau-10...\n",
      "  ‚úÖ ƒê√£ l∆∞u: Hai c·∫ßu d√¢y vƒÉng l·ªõn nh·∫•t cao t·ªëc B·∫øn L·ª©c sau 10 n...\n",
      "  üìÇ Danh m·ª•c: projects\n",
      "  üìÖ Ng√†y: 2025-10-09\n",
      "\n",
      "[2/5] https://vnexpress.net/duong-ket-noi-tp-hcm-tay-ninh-mo-rong-gan-3-lan-...\n",
      "  ‚úÖ ƒê√£ l∆∞u: ƒê∆∞·ªùng k·∫øt n·ªëi TP HCM - T√¢y Ninh m·ªü r·ªông g·∫ßn 3 l·∫ßn...\n",
      "  üìÇ Danh m·ª•c: projects\n",
      "  üìÖ Ng√†y: 2025-10-09\n",
      "\n",
      "[3/5] https://vnexpress.net/tp-hcm-yeu-cau-khoi-phuc-pha-binh-quoi-giam-un-t...\n",
      "  ‚úÖ ƒê√£ l∆∞u: TP HCM y√™u c·∫ßu kh√¥i ph·ª•c ph√† B√¨nh Qu·ªõi gi·∫£m √πn t·∫Øc...\n",
      "  üìÇ Danh m·ª•c: projects\n",
      "  üìÖ Ng√†y: 2025-10-09\n",
      "\n",
      "[4/5] https://vnexpress.net/duong-3-800-ty-dong-noi-tp-hcm-dong-nai-day-dac-...\n",
      "  ‚úÖ ƒê√£ l∆∞u: ƒê∆∞·ªùng 3.800 t·ª∑ ƒë·ªìng n·ªëi TP HCM - ƒê·ªìng Nai d√†y ƒë·∫∑c ...\n",
      "  üìÇ Danh m·ª•c: projects\n",
      "  üìÖ Ng√†y: 2025-10-09\n",
      "\n",
      "[5/5] https://vnexpress.net/lui-thoi-han-chuyen-doi-tai-khoan-giao-thong-voi...\n",
      "  ‚è≠Ô∏è  Kh√¥ng li√™n quan TP.HCM\n",
      "\n",
      "‚úÖ Ho√†n th√†nh VnExpress\n",
      "   ƒê√£ l∆∞u: 4 / 4 b√†i\n",
      "\n",
      "============================================================\n",
      "üîç B·∫ÆT ƒê·∫¶U: B√°o Tu·ªïi Tr·∫ª\n",
      "============================================================\n",
      "\n",
      "üì° B∆∞·ªõc 1: L·∫•y URLs t·ª´ RSS...\n",
      "  üì° ƒêang ƒë·ªçc RSS: https://tuoitre.vn/rss/giao-thong.rss\n",
      "  ‚úì T√¨m th·∫•y 20 URLs t·ª´ RSS\n",
      "\n",
      "üìÑ B∆∞·ªõc 2: L·∫•y URLs t·ª´ trang danh s√°ch...\n",
      "  üìÑ Trang 1: https://tuoitre.vn/giao-thong.htm...\n",
      "  ‚úì T√¨m th·∫•y 0 URLs\n",
      "\n",
      "üìä T·ªïng c·ªông: 20 URLs duy nh·∫•t\n",
      "\n",
      "üì∞ B∆∞·ªõc 3: Scrape chi ti·∫øt...\n",
      "\n",
      "[1/5] https://tuoitre.vn/khoi-dong-giai-dua-o-to-dia-hinh-co-quy-mo-lon-nhat...\n",
      "  ‚è≠Ô∏è  Kh√¥ng li√™n quan TP.HCM\n",
      "\n",
      "[2/5] https://tuoitre.vn/indonesia-chi-con-1-co-hoi-gianh-ve-du-world-cup-20...\n",
      "  ‚è≠Ô∏è  Kh√¥ng li√™n quan TP.HCM\n",
      "\n",
      "[3/5] https://tuoitre.vn/he-lo-so-phan-moi-cua-khu-dat-vang-ben-song-han-tun...\n",
      "  ‚è≠Ô∏è  Kh√¥ng li√™n quan TP.HCM\n",
      "\n",
      "[4/5] https://tuoitre.vn/khoi-to-giam-doc-cong-ty-vang-bac-mao-thiet-bao-tha...\n",
      "  ‚è≠Ô∏è  Kh√¥ng li√™n quan TP.HCM\n",
      "\n",
      "[5/5] https://tuoitre.vn/nguoi-viet-nhiem-ky-sinh-trung-tu-nhung-mon-an-dac-...\n",
      "  ‚è≠Ô∏è  Kh√¥ng li√™n quan TP.HCM\n",
      "\n",
      "‚úÖ Ho√†n th√†nh B√°o Tu·ªïi Tr·∫ª\n",
      "   ƒê√£ l∆∞u: 0 / 0 b√†i\n",
      "\n",
      "============================================================\n",
      "üîç B·∫ÆT ƒê·∫¶U: S·ªü GTVT TP.HCM\n",
      "============================================================\n",
      "\n",
      "üì° B∆∞·ªõc 1: L·∫•y URLs t·ª´ RSS...\n",
      "\n",
      "üìÑ B∆∞·ªõc 2: L·∫•y URLs t·ª´ trang danh s√°ch...\n",
      "  üìÑ Trang 1: https://giaothong.hochiminhcity.gov.vn/render/rendertintuc.a...\n",
      "  ‚úì T√¨m th·∫•y 0 URLs\n",
      "\n",
      "üìä T·ªïng c·ªông: 0 URLs duy nh·∫•t\n",
      "\n",
      "üì∞ B∆∞·ªõc 3: Scrape chi ti·∫øt...\n",
      "\n",
      "‚úÖ Ho√†n th√†nh S·ªü GTVT TP.HCM\n",
      "   ƒê√£ l∆∞u: 0 / 0 b√†i\n",
      "\n",
      "============================================================\n",
      "üîç B·∫ÆT ƒê·∫¶U: B√°o Thanh Ni√™n\n",
      "============================================================\n",
      "\n",
      "üì° B∆∞·ªõc 1: L·∫•y URLs t·ª´ RSS...\n",
      "  üì° ƒêang ƒë·ªçc RSS: https://thanhnien.vn/rss/giao-thong.rss\n",
      "  ‚úì T√¨m th·∫•y 0 URLs t·ª´ RSS\n",
      "\n",
      "üìÑ B∆∞·ªõc 2: L·∫•y URLs t·ª´ trang danh s√°ch...\n",
      "  üìÑ Trang 1: https://thanhnien.vn/giao-thong/...\n",
      "  ‚úì T√¨m th·∫•y 0 URLs\n",
      "\n",
      "üìä T·ªïng c·ªông: 0 URLs duy nh·∫•t\n",
      "\n",
      "üì∞ B∆∞·ªõc 3: Scrape chi ti·∫øt...\n",
      "\n",
      "‚úÖ Ho√†n th√†nh B√°o Thanh Ni√™n\n",
      "   ƒê√£ l∆∞u: 0 / 0 b√†i\n",
      "\n",
      "======================================================================\n",
      "‚úÖ HO√ÄN TH√ÄNH!\n",
      "======================================================================\n",
      "üìä K·∫øt qu·∫£:\n",
      "   ‚Ä¢ T·ªïng URLs: 44\n",
      "   ‚Ä¢ ƒê√£ scrape: 7\n",
      "   ‚Ä¢ ƒê√£ l∆∞u DB: 7\n",
      "   ‚Ä¢ Tr√πng l·∫∑p: 0\n",
      "\n",
      "   üîπ B√°o Lao ƒê·ªông: 3 b√†i\n",
      "\n",
      "   üîπ VnExpress: 4 b√†i\n",
      "\n",
      "   üîπ B√°o Tu·ªïi Tr·∫ª: 0 b√†i\n",
      "\n",
      "   üîπ S·ªü GTVT TP.HCM: 0 b√†i\n",
      "\n",
      "   üîπ B√°o Thanh Ni√™n: 0 b√†i\n",
      "\n",
      "======================================================================\n",
      "üí° Xem d·ªØ li·ªáu:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1129\u001b[39m\n\u001b[32m   1126\u001b[39m         show_stats_simple()\n\u001b[32m   1128\u001b[39m \u001b[38;5;66;03m# Ch·∫°y interactive mode thay v√¨ d√πng sys.argv\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1129\u001b[39m \u001b[43minteractive_mode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1124\u001b[39m, in \u001b[36minteractive_mode\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1121\u001b[39m choice = \u001b[38;5;28minput\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mCh·ªçn (1/2): \u001b[39m\u001b[33m\"\u001b[39m).strip()\n\u001b[32m   1123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m choice == \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1124\u001b[39m     \u001b[43mquick_scrape\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1125\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m choice == \u001b[33m\"\u001b[39m\u001b[33m2\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1126\u001b[39m     show_stats_simple()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 976\u001b[39m, in \u001b[36mquick_scrape\u001b[39m\u001b[34m(num_articles_per_source)\u001b[39m\n\u001b[32m    974\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m70\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    975\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müí° Xem d·ªØ li·ªáu:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m976\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   python \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[34;43m__file__\u001b[39;49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m stats\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    977\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   ho·∫∑c ch·∫°y API: uvicorn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[34m__file__\u001b[39m.replace(\u001b[33m'\u001b[39m\u001b[33m.py\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:app --reload\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    978\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m70\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime, timedelta\n",
    "from fastapi import FastAPI, HTTPException, BackgroundTasks, Query\n",
    "from bson import ObjectId\n",
    "from typing import List, Optional, Dict\n",
    "import time\n",
    "import re\n",
    "import hashlib\n",
    "from collections import Counter\n",
    "import math\n",
    "from difflib import SequenceMatcher\n",
    "import feedparser\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "# ==================== C·∫§U H√åNH N√ÇNG CAO ====================\n",
    "\n",
    "# Th·ªùi gian l·ªçc\n",
    "START_DATE = datetime(2025, 1, 1)\n",
    "CURRENT_DATE = datetime.now()\n",
    "\n",
    "# T·ª´ kh√≥a TP.HCM (ƒë·ªÉ l·ªçc b√†i vi·∫øt)\n",
    "HCMC_KEYWORDS = [\n",
    "    \"tp.hcm\", \"tp hcm\", \"h·ªì ch√≠ minh\", \"ho chi minh\", \"s√†i g√≤n\", \"saigon\",\n",
    "    \"th√†nh ph·ªë h·ªì ch√≠ minh\", \"tphcm\", \"hcmc\"\n",
    "]\n",
    "\n",
    "# T·ª´ kh√≥a giao th√¥ng TP.HCM\n",
    "TRAFFIC_KEYWORDS = {\n",
    "    \"projects\": [\"metro\", \"t√†u ƒëi·ªán\", \"cao t·ªëc\", \"c·∫ßu\", \"ƒë∆∞·ªùng v√†nh ƒëai\", \"brt\", \n",
    "                 \"kh·ªüi c√¥ng\", \"ho√†n th√†nh\", \"d·ª± √°n\", \"ƒë·∫ßu t∆∞\", \"x√¢y d·ª±ng\"],\n",
    "    \"infrastructure\": [\"h·∫° t·∫ßng\", \"giao th√¥ng\", \"ƒë∆∞·ªùng b·ªô\", \"ƒë∆∞·ªùng s·∫Øt\", \"ƒë∆∞·ªùng th·ªßy\",\n",
    "                      \"s√¢n bay\", \"c·∫£ng\", \"b·∫øn xe\", \"tr·∫°m\", \"n√∫t giao th√¥ng\"],\n",
    "    \"issues\": [\"k·∫πt xe\", \"√πn t·∫Øc\", \"tai n·∫°n\", \"ng·∫≠p n∆∞·ªõc\", \"·ªï g√†\", \"h∆∞ h·ªèng\"],\n",
    "    \"planning\": [\"quy ho·∫°ch\", \"k·∫ø ho·∫°ch\", \"ph√™ duy·ªát\", \"tri·ªÉn khai\", \"ƒëi·ªÅu ch·ªânh\"],\n",
    "    \"transport\": [\"xe bu√Ωt\", \"xe bus\", \"taxi\", \"grab\", \"v·∫≠n t·∫£i\", \"xe m√°y\", \"√¥ t√¥\"]\n",
    "}\n",
    "\n",
    "# C·∫•u h√¨nh ngu·ªìn tin chi ti·∫øt h∆°n\n",
    "NEWS_SOURCES = {\n",
    "    \"laodong\": {\n",
    "        \"name\": \"B√°o Lao ƒê·ªông\",\n",
    "        \"base_url\": \"https://laodong.vn\",\n",
    "        \"rss_feeds\": [\n",
    "            \"https://laodong.vn/rss/giao-thong.rss\",\n",
    "            \"https://laodong.vn/rss/xa-hoi.rss\"\n",
    "        ],\n",
    "        \"list_urls\": [\n",
    "            \"https://laodong.vn/giao-thong/\",\n",
    "            \"https://laodong.vn/giao-thong/trang-{page}.htm\"  # Pagination\n",
    "        ],\n",
    "        \"selectors\": {\n",
    "            \"article_links\": \"h3.title-news a, a[href*='/giao-thong/']\",\n",
    "            \"title\": \"h1.detail__title, h1\",\n",
    "            \"content\": \"div.detail-content-body, div.detail__content\",\n",
    "            \"date\": \"span.time-update, time\"\n",
    "        },\n",
    "        \"date_format\": \"%d/%m/%Y %H:%M\"\n",
    "    },\n",
    "    \"vnexpress\": {\n",
    "        \"name\": \"VnExpress\",\n",
    "        \"base_url\": \"https://vnexpress.net\",\n",
    "        \"rss_feeds\": [\"https://vnexpress.net/rss/giao-thong.rss\"],\n",
    "        \"list_urls\": [\n",
    "            \"https://vnexpress.net/giao-thong\",\n",
    "            \"https://vnexpress.net/giao-thong-p{page}\"\n",
    "        ],\n",
    "        \"selectors\": {\n",
    "            \"article_links\": \"h3.title-news a, h2.title-news a\",\n",
    "            \"title\": \"h1.title-detail\",\n",
    "            \"content\": \"article.fck_detail, div.fck_detail\",\n",
    "            \"date\": \"span.date\"\n",
    "        },\n",
    "        \"date_format\": \"%d/%m/%Y, %H:%M\"\n",
    "    },\n",
    "    \"tuoitre\": {\n",
    "        \"name\": \"B√°o Tu·ªïi Tr·∫ª\",\n",
    "        \"base_url\": \"https://tuoitre.vn\",\n",
    "        \"rss_feeds\": [\"https://tuoitre.vn/rss/giao-thong.rss\"],\n",
    "        \"list_urls\": [\n",
    "            \"https://tuoitre.vn/giao-thong.htm\",\n",
    "            \"https://tuoitre.vn/giao-thong/trang-{page}.htm\"\n",
    "        ],\n",
    "        \"selectors\": {\n",
    "            \"article_links\": \"h3.title-news a, a[href*='/giao-thong/']\",\n",
    "            \"title\": \"h1.detail-title\",\n",
    "            \"content\": \"div.detail-content, div#main-detail-content\",\n",
    "            \"date\": \"div.date-time\"\n",
    "        },\n",
    "        \"date_format\": \"%d/%m/%Y %H:%M\"\n",
    "    },\n",
    "    \"hochiminhcity\": {\n",
    "        \"name\": \"S·ªü GTVT TP.HCM\",\n",
    "        \"base_url\": \"https://giaothong.hochiminhcity.gov.vn\",\n",
    "        \"list_urls\": [\n",
    "            \"https://giaothong.hochiminhcity.gov.vn/render/rendertintuc.aspx?CateID=1460\"\n",
    "        ],\n",
    "        \"selectors\": {\n",
    "            \"article_links\": \"a[href*='detail']\",\n",
    "            \"title\": \"h1, .title-detail, h1.article-title\",\n",
    "            \"content\": \"div.content-detail, div.article-content, article, div#content\",\n",
    "            \"date\": \"span.date, div.date-time\"\n",
    "        },\n",
    "        \"date_format\": \"%d/%m/%Y\"\n",
    "    },\n",
    "    \"thanhnien\": {\n",
    "        \"name\": \"B√°o Thanh Ni√™n\",\n",
    "        \"base_url\": \"https://thanhnien.vn\",\n",
    "        \"rss_feeds\": [\"https://thanhnien.vn/rss/giao-thong.rss\"],\n",
    "        \"list_urls\": [\n",
    "            \"https://thanhnien.vn/giao-thong/\",\n",
    "            \"https://thanhnien.vn/giao-thong/trang-{page}.html\"\n",
    "        ],\n",
    "        \"selectors\": {\n",
    "            \"article_links\": \"h2.story__heading a, a.story__link\",\n",
    "            \"title\": \"h1.detail-title\",\n",
    "            \"content\": \"div.detail-content, #contentdetail\",\n",
    "            \"date\": \"time, span.time\"\n",
    "        },\n",
    "        \"date_format\": \"%d/%m/%Y %H:%M\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# ==================== K·∫æT N·ªêI DATABASE ====================\n",
    "\n",
    "try:\n",
    "    client = MongoClient(\"mongodb://localhost:27017/\", serverSelectionTimeoutMS=5000)\n",
    "    client.server_info()\n",
    "    db = client[\"hcmc_traffic_intelligence\"]\n",
    "    articles_col = db[\"articles\"]\n",
    "    categories_col = db[\"categories\"]\n",
    "    summaries_col = db[\"summaries\"]\n",
    "    \n",
    "    # Index n√¢ng cao\n",
    "    articles_col.create_index([(\"url\", 1)], unique=True)\n",
    "    articles_col.create_index([(\"published_date\", -1)])\n",
    "    articles_col.create_index([(\"source\", 1), (\"published_date\", -1)])\n",
    "    articles_col.create_index([(\"category\", 1)])\n",
    "    articles_col.create_index([(\"is_hcmc\", 1)])\n",
    "    articles_col.create_index([(\"content_hash\", 1)])\n",
    "    articles_col.create_index([\n",
    "        (\"title\", \"text\"), \n",
    "        (\"content\", \"text\"), \n",
    "        (\"summary\", \"text\")\n",
    "    ])  # Full-text search\n",
    "    \n",
    "    print(\"‚úì ƒê√£ k·∫øt n·ªëi MongoDB v·ªõi full-text search\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó L·ªói MongoDB: {e}\")\n",
    "    articles_col = None\n",
    "\n",
    "# ==================== THU·∫¨T TO√ÅN T√ìM T·∫ÆT N√ÇNG CAO ====================\n",
    "\n",
    "class AdvancedSummarizer:\n",
    "    \"\"\"Thu·∫≠t to√°n t√≥m t·∫Øt s·ª≠ d·ª•ng TextRank + TF-IDF\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def preprocess_text(text: str) -> List[str]:\n",
    "        \"\"\"T√°ch c√¢u v√† l√†m s·∫°ch\"\"\"\n",
    "        # T√°ch c√¢u theo d·∫•u c√¢u\n",
    "        sentences = re.split(r'[.!?]\\s+', text)\n",
    "        \n",
    "        # L√†m s·∫°ch\n",
    "        cleaned = []\n",
    "        for sent in sentences:\n",
    "            sent = sent.strip()\n",
    "            # Lo·∫°i b·ªè c√¢u qu√° ng·∫Øn ho·∫∑c qu√° d√†i\n",
    "            if 20 < len(sent) < 300:\n",
    "                cleaned.append(sent)\n",
    "        \n",
    "        return cleaned\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_word_freq(sentences: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"T√≠nh t·∫ßn su·∫•t t·ª´ (TF)\"\"\"\n",
    "        words = []\n",
    "        \n",
    "        # Stopwords ti·∫øng Vi·ªát c∆° b·∫£n\n",
    "        stopwords = {\n",
    "            'v√†', 'c·ªßa', 'c√≥', 'ƒë∆∞·ª£c', 'l√†', 'trong', 'v·ªõi', 'cho', 'c√°c', \n",
    "            'm·ªôt', 'n√†y', 'ƒë√£', 't·ª´', 'nh·ªØng', 'ƒë·ªÉ', 'ng∆∞·ªùi', 'kh√¥ng', 'nh∆∞',\n",
    "            'v·ªÅ', 'theo', 'nƒÉm', 't·∫°i', 'ƒë·∫øn', 'khi', 'ng√†y', 'tr√™n', 'sau',\n",
    "            'v√†o', 'th√¨', 's·∫Ω', 'ra', 'ƒëang', 'n√™n', 'b·ªã', 'hay', 'nh∆∞ng'\n",
    "        }\n",
    "        \n",
    "        for sent in sentences:\n",
    "            # T√°ch t·ª´ ƒë∆°n gi·∫£n (c√≥ th·ªÉ d√πng pyvi ƒë·ªÉ t√°ch t·ª´ t·ªët h∆°n)\n",
    "            tokens = re.findall(r'\\w+', sent.lower())\n",
    "            words.extend([w for w in tokens if w not in stopwords and len(w) > 2])\n",
    "        \n",
    "        # T√≠nh t·∫ßn su·∫•t\n",
    "        word_count = Counter(words)\n",
    "        max_freq = max(word_count.values()) if word_count else 1\n",
    "        \n",
    "        return {word: count/max_freq for word, count in word_count.items()}\n",
    "    \n",
    "    @staticmethod\n",
    "    def sentence_similarity(sent1: str, sent2: str, word_freq: Dict[str, float]) -> float:\n",
    "        \"\"\"T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa 2 c√¢u\"\"\"\n",
    "        words1 = set(re.findall(r'\\w+', sent1.lower()))\n",
    "        words2 = set(re.findall(r'\\w+', sent2.lower()))\n",
    "        \n",
    "        if not words1 or not words2:\n",
    "            return 0.0\n",
    "        \n",
    "        # T√≠nh s·ªë t·ª´ chung c√≥ tr·ªçng s·ªë\n",
    "        common_words = words1.intersection(words2)\n",
    "        score = sum(word_freq.get(word, 0) for word in common_words)\n",
    "        \n",
    "        # Normalize\n",
    "        return score / (math.log(len(words1)) + math.log(len(words2)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def textrank(sentences: List[str], word_freq: Dict[str, float], top_n: int = 3) -> List[str]:\n",
    "        \"\"\"TextRank algorithm ƒë·ªÉ ch·ªçn c√¢u quan tr·ªçng\"\"\"\n",
    "        n = len(sentences)\n",
    "        if n == 0:\n",
    "            return []\n",
    "        \n",
    "        # Build similarity matrix\n",
    "        similarity_matrix = [[0.0] * n for _ in range(n)]\n",
    "        \n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                if i != j:\n",
    "                    similarity_matrix[i][j] = AdvancedSummarizer.sentence_similarity(\n",
    "                        sentences[i], sentences[j], word_freq\n",
    "                    )\n",
    "        \n",
    "        # PageRank algorithm\n",
    "        scores = [1.0] * n\n",
    "        damping = 0.85\n",
    "        \n",
    "        for _ in range(10):  # 10 iterations\n",
    "            new_scores = [0.0] * n\n",
    "            \n",
    "            for i in range(n):\n",
    "                rank_sum = sum(\n",
    "                    similarity_matrix[j][i] * scores[j] / \n",
    "                    (sum(similarity_matrix[j]) + 1e-8)\n",
    "                    for j in range(n) if i != j\n",
    "                )\n",
    "                new_scores[i] = (1 - damping) + damping * rank_sum\n",
    "            \n",
    "            scores = new_scores\n",
    "        \n",
    "        # Ch·ªçn top c√¢u v√† gi·ªØ th·ª© t·ª± xu·∫•t hi·ªán\n",
    "        ranked = [(score, idx, sent) for idx, (score, sent) in enumerate(zip(scores, sentences))]\n",
    "        ranked.sort(reverse=True)\n",
    "        \n",
    "        top_sentences = sorted(ranked[:top_n], key=lambda x: x[1])\n",
    "        \n",
    "        return [sent for _, _, sent in top_sentences]\n",
    "    \n",
    "    @staticmethod\n",
    "    def summarize(text: str, num_sentences: int = 3, method: str = \"textrank\") -> str:\n",
    "        \"\"\"T√≥m t·∫Øt vƒÉn b·∫£n\"\"\"\n",
    "        sentences = AdvancedSummarizer.preprocess_text(text)\n",
    "        \n",
    "        if len(sentences) <= num_sentences:\n",
    "            return \". \".join(sentences) + \".\"\n",
    "        \n",
    "        if method == \"textrank\":\n",
    "            word_freq = AdvancedSummarizer.calculate_word_freq(sentences)\n",
    "            summary_sents = AdvancedSummarizer.textrank(sentences, word_freq, num_sentences)\n",
    "        else:\n",
    "            # Fallback: l·∫•y c√¢u ƒë·∫ßu\n",
    "            summary_sents = sentences[:num_sentences]\n",
    "        \n",
    "        return \". \".join(summary_sents) + \".\"\n",
    "\n",
    "# ==================== PH√ÇN LO·∫†I T·ª∞ ƒê·ªòNG ====================\n",
    "\n",
    "def categorize_article(title: str, content: str) -> str:\n",
    "    \"\"\"Ph√¢n lo·∫°i b√†i vi·∫øt theo n·ªôi dung\"\"\"\n",
    "    text = (title + \" \" + content).lower()\n",
    "    \n",
    "    scores = {}\n",
    "    for category, keywords in TRAFFIC_KEYWORDS.items():\n",
    "        score = sum(1 for kw in keywords if kw in text)\n",
    "        scores[category] = score\n",
    "    \n",
    "    if max(scores.values()) == 0:\n",
    "        return \"general\"\n",
    "    \n",
    "    return max(scores, key=scores.get)\n",
    "\n",
    "def is_hcmc_related(title: str, content: str) -> bool:\n",
    "    \"\"\"Ki·ªÉm tra c√≥ li√™n quan ƒë·∫øn TP.HCM kh√¥ng\"\"\"\n",
    "    text = (title + \" \" + content).lower()\n",
    "    return any(kw in text for kw in HCMC_KEYWORDS)\n",
    "\n",
    "# ==================== SCRAPING N√ÇNG CAO ====================\n",
    "\n",
    "def init_driver():\n",
    "    \"\"\"Kh·ªüi t·∫°o Edge driver\"\"\"\n",
    "    edge_options = webdriver.EdgeOptions()\n",
    "    edge_options.add_argument(\"--headless\")\n",
    "    edge_options.add_argument(\"--no-sandbox\")\n",
    "    edge_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    edge_options.add_argument(\"--disable-gpu\")\n",
    "    edge_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64)\")\n",
    "    \n",
    "    driver = webdriver.Edge(options=edge_options)\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "def parse_date(date_str: str, date_format: str) -> Optional[datetime]:\n",
    "    \"\"\"Parse ng√†y th√°ng t·ª´ nhi·ªÅu format\"\"\"\n",
    "    if not date_str:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Th·ª≠ format ƒë∆∞·ª£c cung c·∫•p\n",
    "        return datetime.strptime(date_str.strip(), date_format)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Th·ª≠ c√°c format ph·ªï bi·∫øn\n",
    "    formats = [\n",
    "        \"%d/%m/%Y %H:%M\",\n",
    "        \"%d/%m/%Y\",\n",
    "        \"%d-%m-%Y\",\n",
    "        \"%Y-%m-%d\",\n",
    "        \"%d.%m.%Y\"\n",
    "    ]\n",
    "    \n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            return datetime.strptime(date_str.strip(), fmt)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return None\n",
    "\n",
    "def scrape_from_rss(source_key: str) -> List[str]:\n",
    "    \"\"\"L·∫•y URLs t·ª´ RSS feed\"\"\"\n",
    "    source = NEWS_SOURCES[source_key]\n",
    "    urls = []\n",
    "    \n",
    "    if \"rss_feeds\" not in source:\n",
    "        return []\n",
    "    \n",
    "    for rss_url in source[\"rss_feeds\"]:\n",
    "        try:\n",
    "            print(f\"  üì° ƒêang ƒë·ªçc RSS: {rss_url}\")\n",
    "            feed = feedparser.parse(rss_url)\n",
    "            \n",
    "            for entry in feed.entries[:20]:  # L·∫•y 20 b√†i m·ªõi nh·∫•t\n",
    "                url = entry.get(\"link\", \"\")\n",
    "                pub_date = entry.get(\"published_parsed\", None)\n",
    "                \n",
    "                # Ki·ªÉm tra th·ªùi gian\n",
    "                if pub_date:\n",
    "                    pub_datetime = datetime(*pub_date[:6])\n",
    "                    if pub_datetime >= START_DATE:\n",
    "                        urls.append(url)\n",
    "                else:\n",
    "                    urls.append(url)  # Kh√¥ng c√≥ ng√†y th√¨ l·∫•y lu√¥n\n",
    "            \n",
    "            print(f\"  ‚úì T√¨m th·∫•y {len(urls)} URLs t·ª´ RSS\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚úó L·ªói ƒë·ªçc RSS: {e}\")\n",
    "    \n",
    "    return urls\n",
    "\n",
    "def scrape_from_list_page(source_key: str, max_pages: int = 3) -> List[str]:\n",
    "    \"\"\"L·∫•y URLs t·ª´ trang danh s√°ch v·ªõi pagination\"\"\"\n",
    "    source = NEWS_SOURCES[source_key]\n",
    "    driver = None\n",
    "    all_urls = set()\n",
    "    \n",
    "    try:\n",
    "        driver = init_driver()\n",
    "        \n",
    "        for page in range(1, max_pages + 1):\n",
    "            # L·∫•y URL c·ªßa trang\n",
    "            if len(source[\"list_urls\"]) > 1 and page > 1:\n",
    "                url = source[\"list_urls\"][1].format(page=page)\n",
    "            else:\n",
    "                url = source[\"list_urls\"][0]\n",
    "                if page > 1:\n",
    "                    break  # Kh√¥ng c√≥ pagination\n",
    "            \n",
    "            print(f\"  üìÑ Trang {page}: {url[:60]}...\")\n",
    "            driver.get(url)\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # L·∫•y links\n",
    "            links = driver.find_elements(By.CSS_SELECTOR, source[\"selectors\"][\"article_links\"])\n",
    "            \n",
    "            page_urls = 0\n",
    "            for link in links:\n",
    "                href = link.get_attribute(\"href\")\n",
    "                if href and href.startswith(\"http\"):\n",
    "                    all_urls.add(href)\n",
    "                    page_urls += 1\n",
    "            \n",
    "            print(f\"  ‚úì T√¨m th·∫•y {page_urls} URLs\")\n",
    "            \n",
    "            if page_urls == 0:\n",
    "                break  # Kh√¥ng c√≤n b√†i m·ªõi\n",
    "        \n",
    "        return list(all_urls)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó L·ªói scrape list: {e}\")\n",
    "        return list(all_urls)\n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "\n",
    "def scrape_article_advanced(url: str, source_key: str) -> Optional[Dict]:\n",
    "    \"\"\"Scrape 1 b√†i vi·∫øt v·ªõi metadata ƒë·∫ßy ƒë·ªß\"\"\"\n",
    "    source = NEWS_SOURCES[source_key]\n",
    "    driver = None\n",
    "    \n",
    "    try:\n",
    "        driver = init_driver()\n",
    "        driver.get(url)\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        \n",
    "        # L·∫•y title\n",
    "        try:\n",
    "            title_elem = wait.until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, source[\"selectors\"][\"title\"]))\n",
    "            )\n",
    "            title = title_elem.text.strip()\n",
    "        except:\n",
    "            title = driver.title\n",
    "        \n",
    "        # L·∫•y content\n",
    "        try:\n",
    "            content_elem = driver.find_element(By.CSS_SELECTOR, source[\"selectors\"][\"content\"])\n",
    "            content = content_elem.text.strip()\n",
    "        except:\n",
    "            content = driver.find_element(By.TAG_NAME, \"body\").text.strip()\n",
    "        \n",
    "        # L·∫•y ng√†y ƒëƒÉng\n",
    "        pub_date = None\n",
    "        if \"date\" in source[\"selectors\"]:\n",
    "            try:\n",
    "                date_elem = driver.find_element(By.CSS_SELECTOR, source[\"selectors\"][\"date\"])\n",
    "                date_str = date_elem.text.strip()\n",
    "                pub_date = parse_date(date_str, source.get(\"date_format\", \"%d/%m/%Y\"))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if pub_date is None:\n",
    "            pub_date = datetime.now()\n",
    "        \n",
    "        # Ki·ªÉm tra th·ªùi gian\n",
    "        if pub_date < START_DATE:\n",
    "            print(f\"  ‚è≠Ô∏è  B√†i c≈© ({pub_date.date()}), b·ªè qua\")\n",
    "            return None\n",
    "        \n",
    "        # L√†m s·∫°ch content\n",
    "        lines = [l.strip() for l in content.split(\"\\n\") if len(l.strip()) > 30]\n",
    "        content = \" \".join(lines)\n",
    "        \n",
    "        # Ki·ªÉm tra TP.HCM\n",
    "        is_hcmc = is_hcmc_related(title, content)\n",
    "        if not is_hcmc:\n",
    "            print(f\"  ‚è≠Ô∏è  Kh√¥ng li√™n quan TP.HCM\")\n",
    "            return None\n",
    "        \n",
    "        # Ph√¢n lo·∫°i\n",
    "        category = categorize_article(title, content)\n",
    "        \n",
    "        # T√≥m t·∫Øt n√¢ng cao\n",
    "        summary = AdvancedSummarizer.summarize(content, num_sentences=3, method=\"textrank\")\n",
    "        \n",
    "        return {\n",
    "            \"url\": url,\n",
    "            \"source\": source_key,\n",
    "            \"source_name\": source[\"name\"],\n",
    "            \"title\": title,\n",
    "            \"content\": content,\n",
    "            \"summary\": summary,\n",
    "            \"category\": category,\n",
    "            \"is_hcmc\": is_hcmc,\n",
    "            \"content_hash\": hashlib.md5(content.encode()).hexdigest(),\n",
    "            \"published_date\": pub_date,\n",
    "            \"scraped_at\": datetime.now()\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó L·ªói scrape: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "\n",
    "# ==================== SCRAPE CAMPAIGN ====================\n",
    "\n",
    "def scrape_source_complete(source_key: str, max_articles: int = 50):\n",
    "    \"\"\"Scrape ho√†n ch·ªânh 1 ngu·ªìn (RSS + List pages + Date filter)\"\"\"\n",
    "    source = NEWS_SOURCES[source_key]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üîç B·∫ÆT ƒê·∫¶U: {source['name']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # B∆∞·ªõc 1: L·∫•y URLs t·ª´ RSS\n",
    "    print(f\"\\nüì° B∆∞·ªõc 1: L·∫•y URLs t·ª´ RSS...\")\n",
    "    rss_urls = scrape_from_rss(source_key)\n",
    "    \n",
    "    # B∆∞·ªõc 2: L·∫•y URLs t·ª´ list pages\n",
    "    print(f\"\\nüìÑ B∆∞·ªõc 2: L·∫•y URLs t·ª´ trang danh s√°ch...\")\n",
    "    list_urls = scrape_from_list_page(source_key, max_pages=5)\n",
    "    \n",
    "    # G·ªôp v√† lo·∫°i tr√πng\n",
    "    all_urls = list(set(rss_urls + list_urls))\n",
    "    print(f\"\\nüìä T·ªïng c·ªông: {len(all_urls)} URLs duy nh·∫•t\")\n",
    "    \n",
    "    # B∆∞·ªõc 3: Scrape t·ª´ng b√†i\n",
    "    print(f\"\\nüì∞ B∆∞·ªõc 3: Scrape chi ti·∫øt...\")\n",
    "    \n",
    "    stats = {\n",
    "        \"total_urls\": len(all_urls),\n",
    "        \"scraped\": 0,\n",
    "        \"saved\": 0,\n",
    "        \"duplicates\": 0,\n",
    "        \"not_hcmc\": 0,\n",
    "        \"old_articles\": 0,\n",
    "        \"errors\": 0\n",
    "    }\n",
    "    \n",
    "    for idx, url in enumerate(all_urls[:max_articles], 1):\n",
    "        print(f\"\\n[{idx}/{min(len(all_urls), max_articles)}] {url[:70]}...\")\n",
    "        \n",
    "        # Ki·ªÉm tra ƒë√£ t·ªìn t·∫°i\n",
    "        if articles_col is not None:\n",
    "            existing = articles_col.find_one({\"url\": url})\n",
    "            if existing is not None:\n",
    "                print(f\"  ‚è≠Ô∏è  ƒê√£ t·ªìn t·∫°i trong DB\")\n",
    "                stats[\"duplicates\"] += 1\n",
    "                continue\n",
    "        \n",
    "        # Scrape\n",
    "        article = scrape_article_advanced(url, source_key)\n",
    "        \n",
    "        if article is None:\n",
    "            stats[\"errors\"] += 1\n",
    "            continue\n",
    "        \n",
    "        stats[\"scraped\"] += 1\n",
    "        \n",
    "        # L∆∞u v√†o DB\n",
    "        if articles_col is not None:\n",
    "            try:\n",
    "                articles_col.insert_one(article)\n",
    "                print(f\"  ‚úÖ ƒê√£ l∆∞u: {article['title'][:50]}...\")\n",
    "                print(f\"  üìÇ Danh m·ª•c: {article['category']}\")\n",
    "                print(f\"  üìÖ Ng√†y: {article['published_date'].date()}\")\n",
    "                stats[\"saved\"] += 1\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚úó L·ªói l∆∞u DB: {e}\")\n",
    "                stats[\"duplicates\"] += 1\n",
    "        \n",
    "        time.sleep(2)  # Tr√°nh spam\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def scrape_all_sources_2025(max_per_source: int = 50):\n",
    "    \"\"\"Scrape t·∫•t c·∫£ ngu·ªìn v·ªõi focus v√†o TP.HCM 2025\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üöÄ CHI·∫æN D·ªäCH SCRAPING: GIAO TH√îNG TP.HCM 2025\")\n",
    "    print(f\"   Th·ªùi gian: {START_DATE.date()} ‚Üí {CURRENT_DATE.date()}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    total_stats = {\n",
    "        \"sources\": {},\n",
    "        \"grand_total\": {\n",
    "            \"urls\": 0,\n",
    "            \"scraped\": 0,\n",
    "            \"saved\": 0,\n",
    "            \"duplicates\": 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for source_key in NEWS_SOURCES.keys():\n",
    "        stats = scrape_source_complete(source_key, max_per_source)\n",
    "        total_stats[\"sources\"][source_key] = stats\n",
    "        \n",
    "        total_stats[\"grand_total\"][\"urls\"] += stats[\"total_urls\"]\n",
    "        total_stats[\"grand_total\"][\"scraped\"] += stats[\"scraped\"]\n",
    "        total_stats[\"grand_total\"][\"saved\"] += stats[\"saved\"]\n",
    "        total_stats[\"grand_total\"][\"duplicates\"] += stats[\"duplicates\"]\n",
    "        \n",
    "        print(f\"\\n‚úÖ Ho√†n th√†nh {NEWS_SOURCES[source_key]['name']}\")\n",
    "        print(f\"   ƒê√£ l∆∞u: {stats['saved']} / {stats['scraped']} b√†i\")\n",
    "    \n",
    "    return total_stats\n",
    "\n",
    "# ==================== FASTAPI ====================\n",
    "\n",
    "app = FastAPI(title=\"TP.HCM Traffic Intelligence 2025\")\n",
    "\n",
    "@app.get(\"/\")\n",
    "def root():\n",
    "    return {\n",
    "        \"name\": \"TP.HCM Traffic News Intelligence System 2025\",\n",
    "        \"period\": f\"{START_DATE.date()} to {CURRENT_DATE.date()}\",\n",
    "        \"sources\": len(NEWS_SOURCES),\n",
    "        \"features\": [\n",
    "            \"Advanced TextRank summarization\",\n",
    "            \"Auto-categorization\",\n",
    "            \"HCMC-specific filtering\",\n",
    "            \"Full-text search\",\n",
    "            \"RSS + Pagination scraping\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "@app.get(\"/articles\")\n",
    "def list_articles(\n",
    "    category: Optional[str] = None,\n",
    "    source: Optional[str] = None,\n",
    "    from_date: Optional[str] = Query(None, description=\"YYYY-MM-DD\"),\n",
    "    to_date: Optional[str] = Query(None, description=\"YYYY-MM-DD\"),\n",
    "    search: Optional[str] = None,\n",
    "    limit: int = 20\n",
    "):\n",
    "    \"\"\"Danh s√°ch b√†i vi·∫øt v·ªõi filter n√¢ng cao\"\"\"\n",
    "    if articles_col is None:\n",
    "        raise HTTPException(503, \"Database kh√¥ng kh·∫£ d·ª•ng\")\n",
    "    \n",
    "    query = {\"is_hcmc\": True}\n",
    "    \n",
    "    if category:\n",
    "        query[\"category\"] = category\n",
    "    \n",
    "    if source:\n",
    "        query[\"source\"] = source\n",
    "    \n",
    "    # Date range\n",
    "    date_query = {}\n",
    "    if from_date:\n",
    "        date_query[\"$gte\"] = datetime.strptime(from_date, \"%Y-%m-%d\")\n",
    "    if to_date:\n",
    "        date_query[\"$lte\"] = datetime.strptime(to_date, \"%Y-%m-%d\")\n",
    "    if date_query:\n",
    "        query[\"published_date\"] = date_query\n",
    "    \n",
    "    # Full-text search\n",
    "    if search:\n",
    "        query[\"$text\"] = {\"$search\": search}\n",
    "    \n",
    "    docs = articles_col.find(query).sort(\"published_date\", -1).limit(limit)\n",
    "    \n",
    "    articles = []\n",
    "    for doc in docs:\n",
    "        articles.append({\n",
    "            \"id\": str(doc[\"_id\"]),\n",
    "            \"title\": doc[\"title\"],\n",
    "            \"source\": doc[\"source_name\"],\n",
    "            \"category\": doc.get(\"category\", \"general\"),\n",
    "            \"published_date\": doc[\"published_date\"].isoformat(),\n",
    "            \"summary\": doc.get(\"summary\", \"\")[:200],\n",
    "            \"url\": doc[\"url\"]\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        \"articles\": articles,\n",
    "        \"count\": len(articles),\n",
    "        \"filters\": {\n",
    "            \"category\": category,\n",
    "            \"source\": source,\n",
    "            \"from\": from_date,\n",
    "            \"to\": to_date,\n",
    "            \"search\": search\n",
    "        }\n",
    "    }\n",
    "\n",
    "@app.get(\"/stats\")\n",
    "def get_statistics():\n",
    "    \"\"\"Th·ªëng k√™ chi ti·∫øt\"\"\"\n",
    "    if articles_col is None:\n",
    "        raise HTTPException(503, \"Database kh√¥ng kh·∫£ d·ª•ng\")\n",
    "    \n",
    "    total = articles_col.count_documents({\"is_hcmc\": True})\n",
    "    \n",
    "    # By category\n",
    "    pipeline_category = [\n",
    "        {\"$match\": {\"is_hcmc\": True}},\n",
    "        {\"$group\": {\"_id\": \"$category\", \"count\": {\"$sum\": 1}}},\n",
    "        {\"$sort\": {\"count\": -1}}\n",
    "    ]\n",
    "    by_category = {doc[\"_id\"]: doc[\"count\"] for doc in articles_col.aggregate(pipeline_category)}\n",
    "    \n",
    "    # By source\n",
    "    pipeline_source = [\n",
    "        {\"$match\": {\"is_hcmc\": True}},\n",
    "        {\"$group\": {\"_id\": \"$source_name\", \"count\": {\"$sum\": 1}}},\n",
    "        {\"$sort\": {\"count\": -1}}\n",
    "    ]\n",
    "    by_source = {doc[\"_id\"]: doc[\"count\"] for doc in articles_col.aggregate(pipeline_source)}\n",
    "    \n",
    "    # By month\n",
    "    pipeline_monthly = [\n",
    "        {\"$match\": {\"is_hcmc\": True, \"published_date\": {\"$gte\": START_DATE}}},\n",
    "        {\"$group\": {\n",
    "            \"_id\": {\n",
    "                \"year\": {\"$year\": \"$published_date\"},\n",
    "                \"month\": {\"$month\": \"$published_date\"}\n",
    "            },\n",
    "            \"count\": {\"$sum\": 1}\n",
    "        }},\n",
    "        {\"$sort\": {\"_id.year\": 1, \"_id.month\": 1}}\n",
    "    ]\n",
    "    monthly_data = []\n",
    "    for doc in articles_col.aggregate(pipeline_monthly):\n",
    "        monthly_data.append({\n",
    "            \"month\": f\"{doc['_id']['year']}-{doc['_id']['month']:02d}\",\n",
    "            \"count\": doc[\"count\"]\n",
    "        })\n",
    "    \n",
    "    # Recent articles\n",
    "    recent_7days = articles_col.count_documents({\n",
    "        \"is_hcmc\": True,\n",
    "        \"published_date\": {\"$gte\": datetime.now() - timedelta(days=7)}\n",
    "    })\n",
    "    \n",
    "    return {\n",
    "        \"total_articles\": total,\n",
    "        \"articles_last_7days\": recent_7days,\n",
    "        \"by_category\": by_category,\n",
    "        \"by_source\": by_source,\n",
    "        \"monthly_trend\": monthly_data,\n",
    "        \"period\": {\n",
    "            \"start\": START_DATE.isoformat(),\n",
    "            \"end\": CURRENT_DATE.isoformat()\n",
    "        }\n",
    "    }\n",
    "\n",
    "@app.get(\"/articles/{article_id}\")\n",
    "def get_article_detail(article_id: str):\n",
    "    \"\"\"Chi ti·∫øt b√†i vi·∫øt v·ªõi summary n√¢ng cao\"\"\"\n",
    "    if articles_col is None:\n",
    "        raise HTTPException(503, \"Database kh√¥ng kh·∫£ d·ª•ng\")\n",
    "    \n",
    "    doc = articles_col.find_one({\"_id\": ObjectId(article_id)})\n",
    "    if doc is None:\n",
    "        raise HTTPException(404, \"Kh√¥ng t√¨m th·∫•y b√†i vi·∫øt\")\n",
    "    \n",
    "    return {\n",
    "        \"id\": str(doc[\"_id\"]),\n",
    "        \"title\": doc[\"title\"],\n",
    "        \"source\": doc[\"source_name\"],\n",
    "        \"category\": doc.get(\"category\", \"general\"),\n",
    "        \"url\": doc[\"url\"],\n",
    "        \"content\": doc[\"content\"],\n",
    "        \"summary\": doc.get(\"summary\", \"\"),\n",
    "        \"published_date\": doc[\"published_date\"].isoformat(),\n",
    "        \"scraped_at\": doc[\"scraped_at\"].isoformat(),\n",
    "        \"is_hcmc\": doc.get(\"is_hcmc\", False)\n",
    "    }\n",
    "\n",
    "@app.get(\"/categories\")\n",
    "def list_categories():\n",
    "    \"\"\"Danh s√°ch c√°c danh m·ª•c\"\"\"\n",
    "    return {\n",
    "        \"categories\": [\n",
    "            {\"key\": \"projects\", \"name\": \"D·ª± √°n\", \"keywords\": TRAFFIC_KEYWORDS[\"projects\"]},\n",
    "            {\"key\": \"infrastructure\", \"name\": \"H·∫° t·∫ßng\", \"keywords\": TRAFFIC_KEYWORDS[\"infrastructure\"]},\n",
    "            {\"key\": \"issues\", \"name\": \"V·∫•n ƒë·ªÅ\", \"keywords\": TRAFFIC_KEYWORDS[\"issues\"]},\n",
    "            {\"key\": \"planning\", \"name\": \"Quy ho·∫°ch\", \"keywords\": TRAFFIC_KEYWORDS[\"planning\"]},\n",
    "            {\"key\": \"transport\", \"name\": \"V·∫≠n t·∫£i\", \"keywords\": TRAFFIC_KEYWORDS[\"transport\"]},\n",
    "            {\"key\": \"general\", \"name\": \"Chung\", \"keywords\": []}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "@app.post(\"/scrape/all\")\n",
    "def trigger_full_scrape(background_tasks: BackgroundTasks, max_per_source: int = 50):\n",
    "    \"\"\"Kh·ªüi ƒë·ªông scraping to√†n b·ªô (ch·∫°y background)\"\"\"\n",
    "    background_tasks.add_task(scrape_all_sources_2025, max_per_source)\n",
    "    return {\n",
    "        \"status\": \"started\",\n",
    "        \"message\": f\"ƒêang scrape t·∫•t c·∫£ ngu·ªìn (t·ªëi ƒëa {max_per_source} b√†i/ngu·ªìn)\",\n",
    "        \"target_period\": f\"{START_DATE.date()} to {CURRENT_DATE.date()}\"\n",
    "    }\n",
    "\n",
    "@app.post(\"/scrape/{source_key}\")\n",
    "def trigger_source_scrape(source_key: str, max_articles: int = 50):\n",
    "    \"\"\"Scrape 1 ngu·ªìn c·ª• th·ªÉ\"\"\"\n",
    "    if source_key not in NEWS_SOURCES:\n",
    "        raise HTTPException(404, f\"Ngu·ªìn '{source_key}' kh√¥ng t·ªìn t·∫°i\")\n",
    "    \n",
    "    stats = scrape_source_complete(source_key, max_articles)\n",
    "    \n",
    "    return {\n",
    "        \"source\": NEWS_SOURCES[source_key][\"name\"],\n",
    "        \"stats\": stats\n",
    "    }\n",
    "\n",
    "@app.get(\"/search\")\n",
    "def search_articles(\n",
    "    q: str = Query(..., description=\"T·ª´ kh√≥a t√¨m ki·∫øm\"),\n",
    "    limit: int = 20\n",
    "):\n",
    "    \"\"\"T√¨m ki·∫øm full-text\"\"\"\n",
    "    if articles_col is None:\n",
    "        raise HTTPException(503, \"Database kh√¥ng kh·∫£ d·ª•ng\")\n",
    "    \n",
    "    docs = articles_col.find(\n",
    "        {\"$text\": {\"$search\": q}, \"is_hcmc\": True},\n",
    "        {\"score\": {\"$meta\": \"textScore\"}}\n",
    "    ).sort([(\"score\", {\"$meta\": \"textScore\"})]).limit(limit)\n",
    "    \n",
    "    results = []\n",
    "    for doc in docs:\n",
    "        results.append({\n",
    "            \"id\": str(doc[\"_id\"]),\n",
    "            \"title\": doc[\"title\"],\n",
    "            \"summary\": doc.get(\"summary\", \"\")[:200],\n",
    "            \"source\": doc[\"source_name\"],\n",
    "            \"category\": doc.get(\"category\", \"general\"),\n",
    "            \"published_date\": doc[\"published_date\"].isoformat(),\n",
    "            \"relevance_score\": doc.get(\"score\", 0),\n",
    "            \"url\": doc[\"url\"]\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        \"query\": q,\n",
    "        \"results\": results,\n",
    "        \"count\": len(results)\n",
    "    }\n",
    "\n",
    "@app.get(\"/trending\")\n",
    "def get_trending_topics(days: int = 7, top_n: int = 10):\n",
    "    \"\"\"C√°c ch·ªß ƒë·ªÅ n·ªïi b·∫≠t trong N ng√†y qua\"\"\"\n",
    "    if articles_col is None:\n",
    "        raise HTTPException(503, \"Database kh√¥ng kh·∫£ d·ª•ng\")\n",
    "    \n",
    "    # L·∫•y b√†i vi·∫øt g·∫ßn ƒë√¢y\n",
    "    recent_docs = articles_col.find({\n",
    "        \"is_hcmc\": True,\n",
    "        \"published_date\": {\"$gte\": datetime.now() - timedelta(days=days)}\n",
    "    })\n",
    "    \n",
    "    # ƒê·∫øm t·ª´ kh√≥a\n",
    "    all_text = \"\"\n",
    "    for doc in recent_docs:\n",
    "        all_text += doc[\"title\"] + \" \" + doc[\"content\"]\n",
    "    \n",
    "    # Tr√≠ch xu·∫•t t·ª´ kh√≥a\n",
    "    words = re.findall(r'\\w+', all_text.lower())\n",
    "    \n",
    "    # Stopwords\n",
    "    stopwords = {\n",
    "        'v√†', 'c·ªßa', 'c√≥', 'ƒë∆∞·ª£c', 'l√†', 'trong', 'v·ªõi', 'cho', 'c√°c', \n",
    "        'm·ªôt', 'n√†y', 'ƒë√£', 't·ª´', 'nh·ªØng', 'ƒë·ªÉ', 'ng∆∞·ªùi', 'kh√¥ng', 'nh∆∞',\n",
    "        'v·ªÅ', 'theo', 'nƒÉm', 't·∫°i', 'ƒë·∫øn', 'khi', 'ng√†y', 'tr√™n', 'sau'\n",
    "    }\n",
    "    \n",
    "    words = [w for w in words if w not in stopwords and len(w) > 3]\n",
    "    \n",
    "    # Top keywords\n",
    "    keyword_freq = Counter(words).most_common(top_n)\n",
    "    \n",
    "    trending = [{\"keyword\": word, \"count\": count} for word, count in keyword_freq]\n",
    "    \n",
    "    return {\n",
    "        \"period_days\": days,\n",
    "        \"trending_keywords\": trending\n",
    "    }\n",
    "\n",
    "@app.post(\"/regenerate-summary/{article_id}\")\n",
    "def regenerate_summary(article_id: str, num_sentences: int = 3):\n",
    "    \"\"\"T·∫°o l·∫°i t√≥m t·∫Øt v·ªõi thu·∫≠t to√°n m·ªõi\"\"\"\n",
    "    if articles_col is None:\n",
    "        raise HTTPException(503, \"Database kh√¥ng kh·∫£ d·ª•ng\")\n",
    "    \n",
    "    doc = articles_col.find_one({\"_id\": ObjectId(article_id)})\n",
    "    if doc is None:\n",
    "        raise HTTPException(404, \"Kh√¥ng t√¨m th·∫•y b√†i vi·∫øt\")\n",
    "    \n",
    "    # T·∫°o summary m·ªõi\n",
    "    new_summary = AdvancedSummarizer.summarize(\n",
    "        doc[\"content\"], \n",
    "        num_sentences=num_sentences,\n",
    "        method=\"textrank\"\n",
    "    )\n",
    "    \n",
    "    # C·∫≠p nh·∫≠t DB\n",
    "    articles_col.update_one(\n",
    "        {\"_id\": doc[\"_id\"]},\n",
    "        {\"$set\": {\"summary\": new_summary, \"summary_updated_at\": datetime.now()}}\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"id\": str(doc[\"_id\"]),\n",
    "        \"old_summary\": doc.get(\"summary\", \"\"),\n",
    "        \"new_summary\": new_summary\n",
    "    }\n",
    "\n",
    "@app.get(\"/export\")\n",
    "def export_articles(\n",
    "    format: str = Query(\"json\", description=\"json ho·∫∑c csv\"),\n",
    "    category: Optional[str] = None,\n",
    "    from_date: Optional[str] = None,\n",
    "    to_date: Optional[str] = None\n",
    "):\n",
    "    \"\"\"Export d·ªØ li·ªáu\"\"\"\n",
    "    if articles_col is None:\n",
    "        raise HTTPException(503, \"Database kh√¥ng kh·∫£ d·ª•ng\")\n",
    "    \n",
    "    query = {\"is_hcmc\": True}\n",
    "    \n",
    "    if category:\n",
    "        query[\"category\"] = category\n",
    "    \n",
    "    date_query = {}\n",
    "    if from_date:\n",
    "        date_query[\"$gte\"] = datetime.strptime(from_date, \"%Y-%m-%d\")\n",
    "    if to_date:\n",
    "        date_query[\"$lte\"] = datetime.strptime(to_date, \"%Y-%m-%d\")\n",
    "    if date_query:\n",
    "        query[\"published_date\"] = date_query\n",
    "    \n",
    "    docs = articles_col.find(query).sort(\"published_date\", -1)\n",
    "    \n",
    "    articles = []\n",
    "    for doc in docs:\n",
    "        articles.append({\n",
    "            \"id\": str(doc[\"_id\"]),\n",
    "            \"title\": doc[\"title\"],\n",
    "            \"source\": doc[\"source_name\"],\n",
    "            \"category\": doc.get(\"category\", \"general\"),\n",
    "            \"summary\": doc.get(\"summary\", \"\"),\n",
    "            \"published_date\": doc[\"published_date\"].isoformat(),\n",
    "            \"url\": doc[\"url\"]\n",
    "        })\n",
    "    \n",
    "    if format == \"csv\":\n",
    "        # Simple CSV format\n",
    "        csv_data = \"ID,Title,Source,Category,Date,URL\\n\"\n",
    "        for art in articles:\n",
    "            csv_data += f\"{art['id']},\\\"{art['title']}\\\",{art['source']},{art['category']},{art['published_date']},{art['url']}\\n\"\n",
    "        \n",
    "        return {\"format\": \"csv\", \"data\": csv_data, \"count\": len(articles)}\n",
    "    \n",
    "    return {\"format\": \"json\", \"articles\": articles, \"count\": len(articles)}\n",
    "\n",
    "# ==================== CH·∫†Y NHANH (SIMPLE MODE) ====================\n",
    "\n",
    "def quick_scrape(num_articles_per_source: int = 5):\n",
    "    \"\"\"Ch·∫°y scrape ƒë∆°n gi·∫£n - kh√¥ng c·∫ßn sys.argv\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üöÄ TP.HCM TRAFFIC NEWS INTELLIGENCE SYSTEM 2025\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"üìÖ Th·ªùi gian: {START_DATE.date()} ‚Üí {CURRENT_DATE.date()}\")\n",
    "    print(f\"üì∞ ƒêang scrape {num_articles_per_source} b√†i/ngu·ªìn...\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    results = scrape_all_sources_2025(max_per_source=num_articles_per_source)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"‚úÖ HO√ÄN TH√ÄNH!\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"üìä K·∫øt qu·∫£:\")\n",
    "    print(f\"   ‚Ä¢ T·ªïng URLs: {results['grand_total']['urls']}\")\n",
    "    print(f\"   ‚Ä¢ ƒê√£ scrape: {results['grand_total']['scraped']}\")\n",
    "    print(f\"   ‚Ä¢ ƒê√£ l∆∞u DB: {results['grand_total']['saved']}\")\n",
    "    print(f\"   ‚Ä¢ Tr√πng l·∫∑p: {results['grand_total']['duplicates']}\")\n",
    "    \n",
    "    for source_key, stats in results[\"sources\"].items():\n",
    "        name = NEWS_SOURCES[source_key][\"name\"]\n",
    "        print(f\"\\n   üîπ {name}: {stats['saved']} b√†i\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üí° Xem d·ªØ li·ªáu:\")\n",
    "    print(f\"   python {__file__} stats\")\n",
    "    print(f\"   ho·∫∑c ch·∫°y API: uvicorn {__file__.replace('.py', '')}:app --reload\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def show_stats_simple():\n",
    "    \"\"\"Hi·ªÉn th·ªã th·ªëng k√™ ƒë∆°n gi·∫£n\"\"\"\n",
    "    if articles_col is None:\n",
    "        print(\"‚ùå Kh√¥ng k·∫øt n·ªëi ƒë∆∞·ª£c MongoDB\")\n",
    "        return\n",
    "    \n",
    "    total = articles_col.count_documents({\"is_hcmc\": True})\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìä TH·ªêNG K√ä T·ªîNG QUAN\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"T·ªïng s·ªë b√†i vi·∫øt v·ªÅ TP.HCM: {total}\")\n",
    "    \n",
    "    # By category\n",
    "    print(f\"\\nüìÇ Theo danh m·ª•c:\")\n",
    "    for category in [\"projects\", \"infrastructure\", \"issues\", \"planning\", \"transport\", \"general\"]:\n",
    "        count = articles_col.count_documents({\"is_hcmc\": True, \"category\": category})\n",
    "        if count > 0:\n",
    "            cat_name = {\n",
    "                \"projects\": \"D·ª± √°n\",\n",
    "                \"infrastructure\": \"H·∫° t·∫ßng\", \n",
    "                \"issues\": \"V·∫•n ƒë·ªÅ\",\n",
    "                \"planning\": \"Quy ho·∫°ch\",\n",
    "                \"transport\": \"V·∫≠n t·∫£i\",\n",
    "                \"general\": \"Chung\"\n",
    "            }.get(category, category)\n",
    "            print(f\"   ‚Ä¢ {cat_name}: {count} b√†i\")\n",
    "    \n",
    "    # By source\n",
    "    print(f\"\\nüì∞ Theo ngu·ªìn:\")\n",
    "    for source_key, source in NEWS_SOURCES.items():\n",
    "        count = articles_col.count_documents({\"is_hcmc\": True, \"source\": source_key})\n",
    "        if count > 0:\n",
    "            print(f\"   ‚Ä¢ {source['name']}: {count} b√†i\")\n",
    "    \n",
    "    # Recent\n",
    "    recent = articles_col.count_documents({\n",
    "        \"is_hcmc\": True,\n",
    "        \"published_date\": {\"$gte\": datetime.now() - timedelta(days=7)}\n",
    "    })\n",
    "    print(f\"\\nüìÖ 7 ng√†y g·∫ßn ƒë√¢y: {recent} b√†i\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\\n\")\n",
    "\n",
    "# ==================== CLI MODE ====================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    \n",
    "    # Ki·ªÉm tra c√≥ argument kh√¥ng\n",
    "    if len(sys.argv) > 1:\n",
    "        command = sys.argv[1]\n",
    "        \n",
    "        if command == \"scrape\":\n",
    "            # Scrape all sources\n",
    "            max_articles = int(sys.argv[2]) if len(sys.argv) > 2 else 50\n",
    "            results = scrape_all_sources_2025(max_per_source=max_articles)\n",
    "            \n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"üìä K·∫æT QU·∫¢ T·ªîNG H·ª¢P\")\n",
    "            print(f\"{'='*70}\")\n",
    "            print(f\"T·ªïng URLs: {results['grand_total']['urls']}\")\n",
    "            print(f\"ƒê√£ scrape: {results['grand_total']['scraped']}\")\n",
    "            print(f\"ƒê√£ l∆∞u: {results['grand_total']['saved']}\")\n",
    "            print(f\"Tr√πng l·∫∑p: {results['grand_total']['duplicates']}\")\n",
    "            \n",
    "            print(f\"\\nüìà Chi ti·∫øt theo ngu·ªìn:\")\n",
    "            for source_key, stats in results[\"sources\"].items():\n",
    "                name = NEWS_SOURCES[source_key][\"name\"]\n",
    "                print(f\"\\n  üîπ {name}\")\n",
    "                print(f\"     URLs: {stats['total_urls']}\")\n",
    "                print(f\"     ƒê√£ l∆∞u: {stats['saved']}/{stats['scraped']}\")\n",
    "                print(f\"     Kh√¥ng ph·∫£i HCM: {stats.get('not_hcmc', 0)}\")\n",
    "                print(f\"     B√†i c≈©: {stats.get('old_articles', 0)}\")\n",
    "            \n",
    "            print(f\"\\n{'='*70}\")\n",
    "        \n",
    "        elif command == \"stats\":\n",
    "            # Show statistics\n",
    "            show_stats_simple()\n",
    "        \n",
    "        else:\n",
    "            print(f\"‚ùå L·ªánh kh√¥ng h·ª£p l·ªá: {command}\")\n",
    "            print(f\"S·ª≠ d·ª•ng: python {sys.argv[0]} [scrape|stats]\")\n",
    "    \n",
    "    else:\n",
    "        # Kh√¥ng c√≥ argument - ch·∫°y mode m·∫∑c ƒë·ªãnh\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"üöÄ TP.HCM TRAFFIC NEWS INTELLIGENCE SYSTEM 2025\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"üìÖ Th·ªùi gian: {START_DATE.date()} ‚Üí {CURRENT_DATE.date()}\")\n",
    "        print(f\"üì∞ Ngu·ªìn tin: {len(NEWS_SOURCES)} b√°o\")\n",
    "        print(f\"ü§ñ Thu·∫≠t to√°n: TextRank + TF-IDF\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        print(\"Ch·ªçn ch·∫ø ƒë·ªô:\")\n",
    "        print(\"  [1] Scrape nhanh (5 b√†i/ngu·ªìn)\")\n",
    "        print(\"  [2] Xem th·ªëng k√™\")\n",
    "        print(\"  [3] H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng\")\n",
    "        \n",
    "        try:\n",
    "            choice = input(\"\\nNh·∫≠p l·ª±a ch·ªçn (1/2/3): \").strip()\n",
    "            \n",
    "            if choice == \"1\":\n",
    "                quick_scrape(num_articles_per_source=5)\n",
    "            elif choice == \"2\":\n",
    "                show_stats_simple()\n",
    "            else:\n",
    "                print(f\"\\nüìñ H∆Ø·ªöNG D·∫™N S·ª¨ D·ª§NG:\")\n",
    "                print(f\"\\n  1Ô∏è‚É£  Scrape d·ªØ li·ªáu:\")\n",
    "                print(f\"     python {sys.argv[0]} scrape [s·ªë_b√†i]\")\n",
    "                print(f\"     V√≠ d·ª•: python {sys.argv[0]} scrape 50\")\n",
    "                \n",
    "                print(f\"\\n  2Ô∏è‚É£  Xem th·ªëng k√™:\")\n",
    "                print(f\"     python {sys.argv[0]} stats\")\n",
    "                \n",
    "                print(f\"\\n  3Ô∏è‚É£  Ch·∫°y API server:\")\n",
    "                print(f\"     uvicorn {sys.argv[0].replace('.py', '')}:app --reload\")\n",
    "                \n",
    "                print(f\"\\n  4Ô∏è‚É£  Test API:\")\n",
    "                print(f\"     curl http://localhost:8000/stats\")\n",
    "                print(f\"     curl http://localhost:8000/articles?category=projects\")\n",
    "                print(f\"     curl http://localhost:8000/search?q=metro\")\n",
    "                \n",
    "                print(f\"\\n{'='*70}\\n\")\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            print(f\"\\n\\nüëã T·∫°m bi·ªát!\\n\")\n",
    "\n",
    "# ==================== Th√™m v√†o cu·ªëi file ====================\n",
    "def interactive_mode():\n",
    "    \"\"\"Ch·∫°y interactive ƒë·ªÉ tr√°nh l·ªói ipykernel\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üöÄ TP.HCM TRAFFIC NEWS SCRAPER\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\n[1] Scrape 5 b√†i/ngu·ªìn\")\n",
    "    print(\"[2] Xem th·ªëng k√™\")\n",
    "    \n",
    "    choice = input(\"\\nCh·ªçn (1/2): \").strip()\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        quick_scrape(5)\n",
    "    elif choice == \"2\":\n",
    "        show_stats_simple()\n",
    "\n",
    "# Ch·∫°y interactive mode thay v√¨ d√πng sys.argv\n",
    "interactive_mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ecc959",
   "metadata": {},
   "source": [
    "GIAI ƒêO·∫†N 1: THU TH·∫¨P D·ªÆ LI·ªÜU TH√î (RAW DATA COLLECTION)\n",
    "=======================================================\n",
    "M·ª•c ti√™u: Scrape b√†i b√°o t·ª´ nhi·ªÅu ngu·ªìn v√† l∆∞u v√†o MongoDB collection \"raw_data\"\n",
    "Kh√¥ng l√†m s·∫°ch, kh√¥ng l·ªçc - ch·ªâ thu th·∫≠p th√¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25af4e9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '--f=c:\\\\Users\\\\ASUS\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-v3e88060ec6ce0abe0108c4929c27dbcc5f1d608e1.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 345\u001b[39m\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sys.argv) > \u001b[32m1\u001b[39m:\n\u001b[32m    344\u001b[39m     \u001b[38;5;66;03m# Ch·∫°y v·ªõi tham s·ªë: python 1_data_collection.py 100\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m     max_articles = \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43margv\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    347\u001b[39m     \u001b[38;5;66;03m# M·∫∑c ƒë·ªãnh: 50 b√†i/ngu·ªìn\u001b[39;00m\n\u001b[32m    348\u001b[39m     max_articles = \u001b[32m50\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: invalid literal for int() with base 10: '--f=c:\\\\Users\\\\ASUS\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-v3e88060ec6ce0abe0108c4929c27dbcc5f1d608e1.json'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "GIAI ƒêO·∫†N 1: THU TH·∫¨P D·ªÆ LI·ªÜU TH√î (RAW DATA COLLECTION)\n",
    "=======================================================\n",
    "M·ª•c ti√™u: Scrape b√†i b√°o t·ª´ nhi·ªÅu ngu·ªìn v√† l∆∞u v√†o MongoDB collection \"raw_data\"\n",
    "Kh√¥ng l√†m s·∫°ch, kh√¥ng l·ªçc - ch·ªâ thu th·∫≠p th√¥\n",
    "\"\"\"\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "import feedparser\n",
    "import time\n",
    "import hashlib\n",
    "\n",
    "# ==================== C·∫§U H√åNH ====================\n",
    "\n",
    "# C·∫•u h√¨nh MongoDB\n",
    "MONGODB_URI = \"mongodb://localhost:27017/\"\n",
    "DATABASE_NAME = \"hcmc_traffic_summarization\"\n",
    "RAW_COLLECTION = \"raw_data\"  # Collection cho d·ªØ li·ªáu th√¥\n",
    "\n",
    "# C·∫•u h√¨nh ngu·ªìn tin\n",
    "NEWS_SOURCES = {\n",
    "    \"laodong\": {\n",
    "        \"name\": \"B√°o Lao ƒê·ªông\",\n",
    "        \"base_url\": \"https://laodong.vn\",\n",
    "        \"rss_feeds\": [\"https://laodong.vn/rss/giao-thong.rss\"],\n",
    "        \"selectors\": {\n",
    "            \"title\": \"h1.detail__title, h1\",\n",
    "            \"content\": \"div.detail-content-body, div.detail__content\",\n",
    "            \"date\": \"span.time-update, time\"\n",
    "        }\n",
    "    },\n",
    "    \"vnexpress\": {\n",
    "        \"name\": \"VnExpress\",\n",
    "        \"base_url\": \"https://vnexpress.net\",\n",
    "        \"rss_feeds\": [\"https://vnexpress.net/rss/giao-thong.rss\"],\n",
    "        \"selectors\": {\n",
    "            \"title\": \"h1.title-detail\",\n",
    "            \"content\": \"article.fck_detail, div.fck_detail\",\n",
    "            \"date\": \"span.date\"\n",
    "        }\n",
    "    },\n",
    "    \"tuoitre\": {\n",
    "        \"name\": \"B√°o Tu·ªïi Tr·∫ª\",\n",
    "        \"base_url\": \"https://tuoitre.vn\",\n",
    "        \"rss_feeds\": [\"https://tuoitre.vn/rss/giao-thong.rss\"],\n",
    "        \"selectors\": {\n",
    "            \"title\": \"h1.detail-title\",\n",
    "            \"content\": \"div.detail-content, div#main-detail-content\",\n",
    "            \"date\": \"div.date-time\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# ==================== K·∫æT N·ªêI DATABASE ====================\n",
    "\n",
    "def connect_mongodb():\n",
    "    \"\"\"\n",
    "    K·∫øt n·ªëi MongoDB v√† t·∫°o collection raw_data\n",
    "    \n",
    "    Returns:\n",
    "        Collection object ho·∫∑c None n·∫øu l·ªói\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = MongoClient(MONGODB_URI, serverSelectionTimeoutMS=5000)\n",
    "        client.server_info()  # Test connection\n",
    "        \n",
    "        db = client[DATABASE_NAME]\n",
    "        raw_col = db[RAW_COLLECTION]\n",
    "        \n",
    "        # T·∫°o index ƒë·ªÉ tr√°nh tr√πng l·∫∑p URL\n",
    "        raw_col.create_index([(\"url\", 1)], unique=True)\n",
    "        raw_col.create_index([(\"scraped_at\", -1)])\n",
    "        raw_col.create_index([(\"source\", 1)])\n",
    "        \n",
    "        print(f\"‚úÖ ƒê√£ k·∫øt n·ªëi MongoDB: {DATABASE_NAME}.{RAW_COLLECTION}\")\n",
    "        return raw_col\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói k·∫øt n·ªëi MongoDB: {e}\")\n",
    "        return None\n",
    "\n",
    "# ==================== SCRAPING FUNCTIONS ====================\n",
    "\n",
    "def init_selenium_driver():\n",
    "    \"\"\"\n",
    "    Kh·ªüi t·∫°o Selenium WebDriver (Edge headless)\n",
    "    \n",
    "    Returns:\n",
    "        WebDriver object\n",
    "    \"\"\"\n",
    "    options = webdriver.EdgeOptions()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64)\")\n",
    "    \n",
    "    driver = webdriver.Edge(options=options)\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "def get_urls_from_rss(rss_url, max_items=50):\n",
    "    \"\"\"\n",
    "    L·∫•y danh s√°ch URLs t·ª´ RSS feed\n",
    "    \n",
    "    Args:\n",
    "        rss_url: URL c·ªßa RSS feed\n",
    "        max_items: S·ªë l∆∞·ª£ng b√†i t·ªëi ƒëa\n",
    "    \n",
    "    Returns:\n",
    "        List of URLs\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"  üì° ƒêang ƒë·ªçc RSS: {rss_url}\")\n",
    "        feed = feedparser.parse(rss_url)\n",
    "        \n",
    "        urls = []\n",
    "        for entry in feed.entries[:max_items]:\n",
    "            url = entry.get(\"link\", \"\")\n",
    "            if url:\n",
    "                urls.append(url)\n",
    "        \n",
    "        print(f\"  ‚úÖ T√¨m th·∫•y {len(urls)} URLs t·ª´ RSS\")\n",
    "        return urls\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå L·ªói ƒë·ªçc RSS: {e}\")\n",
    "        return []\n",
    "\n",
    "def scrape_article_raw(url, source_key, driver):\n",
    "    \"\"\"\n",
    "    Scrape 1 b√†i b√°o v√† tr·∫£ v·ªÅ d·ªØ li·ªáu TH√î (kh√¥ng l√†m s·∫°ch)\n",
    "    \n",
    "    Args:\n",
    "        url: URL b√†i b√°o\n",
    "        source_key: Key c·ªßa ngu·ªìn trong NEWS_SOURCES\n",
    "        driver: Selenium WebDriver\n",
    "    \n",
    "    Returns:\n",
    "        Dict ch·ª©a d·ªØ li·ªáu th√¥ ho·∫∑c None n·∫øu l·ªói\n",
    "    \"\"\"\n",
    "    source = NEWS_SOURCES[source_key]\n",
    "    \n",
    "    try:\n",
    "        driver.get(url)\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        \n",
    "        # L·∫•y title (raw - kh√¥ng l√†m s·∫°ch)\n",
    "        try:\n",
    "            title_elem = wait.until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, source[\"selectors\"][\"title\"]))\n",
    "            )\n",
    "            title = title_elem.text\n",
    "        except:\n",
    "            title = driver.title\n",
    "        \n",
    "        # L·∫•y content (raw - bao g·ªìm c·∫£ HTML)\n",
    "        try:\n",
    "            content_elem = driver.find_element(By.CSS_SELECTOR, source[\"selectors\"][\"content\"])\n",
    "            # L·∫•y c·∫£ innerHTML ƒë·ªÉ gi·ªØ c·∫•u tr√∫c\n",
    "            content_html = content_elem.get_attribute(\"innerHTML\")\n",
    "            content_text = content_elem.text\n",
    "        except:\n",
    "            content_html = driver.find_element(By.TAG_NAME, \"body\").get_attribute(\"innerHTML\")\n",
    "            content_text = driver.find_element(By.TAG_NAME, \"body\").text\n",
    "        \n",
    "        # L·∫•y ng√†y ƒëƒÉng (raw string)\n",
    "        date_raw = \"\"\n",
    "        if \"date\" in source[\"selectors\"]:\n",
    "            try:\n",
    "                date_elem = driver.find_element(By.CSS_SELECTOR, source[\"selectors\"][\"date\"])\n",
    "                date_raw = date_elem.text\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # T·∫°o hash ƒë·ªÉ ph√°t hi·ªán duplicate sau n√†y\n",
    "        content_hash = hashlib.md5(content_text.encode('utf-8')).hexdigest()\n",
    "        \n",
    "        return {\n",
    "            \"url\": url,\n",
    "            \"source\": source_key,\n",
    "            \"source_name\": source[\"name\"],\n",
    "            \"title\": title,  # RAW title\n",
    "            \"content_text\": content_text,  # RAW text\n",
    "            \"content_html\": content_html,  # RAW HTML\n",
    "            \"date_raw\": date_raw,  # RAW date string\n",
    "            \"content_hash\": content_hash,\n",
    "            \"scraped_at\": datetime.now(),\n",
    "            \"processing_status\": \"raw\"  # ƒê√°nh d·∫•u ch∆∞a x·ª≠ l√Ω\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå L·ªói scrape {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# ==================== MAIN SCRAPING PIPELINE ====================\n",
    "\n",
    "def scrape_source(source_key, raw_collection, max_articles=50):\n",
    "    \"\"\"\n",
    "    Scrape 1 ngu·ªìn tin v√† l∆∞u v√†o MongoDB\n",
    "    \n",
    "    Args:\n",
    "        source_key: Key c·ªßa ngu·ªìn trong NEWS_SOURCES\n",
    "        raw_collection: MongoDB collection object\n",
    "        max_articles: S·ªë l∆∞·ª£ng b√†i t·ªëi ƒëa\n",
    "    \n",
    "    Returns:\n",
    "        Dict ch·ª©a th·ªëng k√™\n",
    "    \"\"\"\n",
    "    source = NEWS_SOURCES[source_key]\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üîç ƒêang scrape: {source['name']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # B∆∞·ªõc 1: L·∫•y URLs t·ª´ RSS\n",
    "    all_urls = []\n",
    "    for rss_url in source.get(\"rss_feeds\", []):\n",
    "        urls = get_urls_from_rss(rss_url, max_items=max_articles)\n",
    "        all_urls.extend(urls)\n",
    "    \n",
    "    # Lo·∫°i b·ªè duplicate URLs\n",
    "    all_urls = list(set(all_urls))\n",
    "    print(f\"\\nüìä T·ªïng c·ªông: {len(all_urls)} URLs duy nh·∫•t\")\n",
    "    \n",
    "    # B∆∞·ªõc 2: Scrape t·ª´ng b√†i\n",
    "    driver = None\n",
    "    stats = {\n",
    "        \"total_urls\": len(all_urls),\n",
    "        \"scraped_success\": 0,\n",
    "        \"saved_to_db\": 0,\n",
    "        \"duplicates\": 0,\n",
    "        \"errors\": 0\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        driver = init_selenium_driver()\n",
    "        \n",
    "        for idx, url in enumerate(all_urls[:max_articles], 1):\n",
    "            print(f\"\\n[{idx}/{min(len(all_urls), max_articles)}] {url[:60]}...\")\n",
    "            \n",
    "            # Ki·ªÉm tra ƒë√£ t·ªìn t·∫°i trong DB ch∆∞a\n",
    "            if raw_collection.find_one({\"url\": url}):\n",
    "                print(f\"  ‚è≠Ô∏è  ƒê√£ t·ªìn t·∫°i trong DB\")\n",
    "                stats[\"duplicates\"] += 1\n",
    "                continue\n",
    "            \n",
    "            # Scrape b√†i b√°o\n",
    "            article_data = scrape_article_raw(url, source_key, driver)\n",
    "            \n",
    "            if article_data is None:\n",
    "                stats[\"errors\"] += 1\n",
    "                continue\n",
    "            \n",
    "            stats[\"scraped_success\"] += 1\n",
    "            \n",
    "            # L∆∞u v√†o MongoDB (raw data)\n",
    "            try:\n",
    "                raw_collection.insert_one(article_data)\n",
    "                print(f\"  ‚úÖ ƒê√£ l∆∞u raw data: {article_data['title'][:50]}...\")\n",
    "                stats[\"saved_to_db\"] += 1\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ùå L·ªói l∆∞u DB: {e}\")\n",
    "                stats[\"duplicates\"] += 1\n",
    "            \n",
    "            time.sleep(2)  # Tr√°nh spam server\n",
    "    \n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def scrape_all_sources(max_per_source=50):\n",
    "    \"\"\"\n",
    "    Scrape t·∫•t c·∫£ ngu·ªìn tin\n",
    "    \n",
    "    Args:\n",
    "        max_per_source: S·ªë b√†i t·ªëi ƒëa m·ªói ngu·ªìn\n",
    "    \n",
    "    Returns:\n",
    "        Dict ch·ª©a th·ªëng k√™ t·ªïng h·ª£p\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üöÄ GIAI ƒêO·∫†N 1: THU TH·∫¨P D·ªÆ LI·ªÜU TH√î\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # K·∫øt n·ªëi MongoDB\n",
    "    raw_collection = connect_mongodb()\n",
    "    if raw_collection is None:\n",
    "        print(\"‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi MongoDB. D·ª´ng scraping.\")\n",
    "        return None\n",
    "    \n",
    "    # Scrape t·ª´ng ngu·ªìn\n",
    "    total_stats = {\n",
    "        \"sources\": {},\n",
    "        \"grand_total\": {\n",
    "            \"scraped\": 0,\n",
    "            \"saved\": 0,\n",
    "            \"duplicates\": 0,\n",
    "            \"errors\": 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for source_key in NEWS_SOURCES.keys():\n",
    "        stats = scrape_source(source_key, raw_collection, max_per_source)\n",
    "        total_stats[\"sources\"][source_key] = stats\n",
    "        \n",
    "        total_stats[\"grand_total\"][\"scraped\"] += stats[\"scraped_success\"]\n",
    "        total_stats[\"grand_total\"][\"saved\"] += stats[\"saved_to_db\"]\n",
    "        total_stats[\"grand_total\"][\"duplicates\"] += stats[\"duplicates\"]\n",
    "        total_stats[\"grand_total\"][\"errors\"] += stats[\"errors\"]\n",
    "    \n",
    "    # In b√°o c√°o t·ªïng k·∫øt\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"‚úÖ HO√ÄN TH√ÄNH GIAI ƒêO·∫†N 1\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"üìä Th·ªëng k√™ t·ªïng h·ª£p:\")\n",
    "    print(f\"   ‚Ä¢ ƒê√£ scrape th√†nh c√¥ng: {total_stats['grand_total']['scraped']}\")\n",
    "    print(f\"   ‚Ä¢ ƒê√£ l∆∞u v√†o raw_data: {total_stats['grand_total']['saved']}\")\n",
    "    print(f\"   ‚Ä¢ Tr√πng l·∫∑p: {total_stats['grand_total']['duplicates']}\")\n",
    "    print(f\"   ‚Ä¢ L·ªói: {total_stats['grand_total']['errors']}\")\n",
    "    \n",
    "    print(f\"\\nüìà Chi ti·∫øt theo ngu·ªìn:\")\n",
    "    for source_key, stats in total_stats[\"sources\"].items():\n",
    "        name = NEWS_SOURCES[source_key][\"name\"]\n",
    "        print(f\"   ‚Ä¢ {name}: {stats['saved_to_db']} b√†i\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"‚û°Ô∏è  Ti·∫øp theo: Ch·∫°y 2_data_filtering.py ƒë·ªÉ l·ªçc & g√°n nh√£n\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return total_stats\n",
    "\n",
    "# ==================== CH·∫†Y CH∆Ø∆†NG TR√åNH ====================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    \n",
    "    if len(sys.argv) > 1:\n",
    "        # Ch·∫°y v·ªõi tham s·ªë: python 1_data_collection.py 100\n",
    "        max_articles = int(sys.argv[1])\n",
    "    else:\n",
    "        # M·∫∑c ƒë·ªãnh: 50 b√†i/ngu·ªìn\n",
    "        max_articles = 50\n",
    "    \n",
    "    print(f\"‚öôÔ∏è  C·∫•u h√¨nh: T·ªëi ƒëa {max_articles} b√†i/ngu·ªìn\\n\")\n",
    "    \n",
    "    # B·∫Øt ƒë·∫ßu scraping\n",
    "    results = scrape_all_sources(max_per_source=max_articles)\n",
    "    \n",
    "    if results:\n",
    "        print(f\"\\n‚úÖ D·ªØ li·ªáu th√¥ ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o MongoDB collection '{RAW_COLLECTION}'\")\n",
    "        print(f\"üìä T·ªïng s·ªë b√†i: {results['grand_total']['saved']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e9ea8e",
   "metadata": {},
   "source": [
    "GIAI ƒêO·∫†N 2: L·ªåC V√Ä G√ÅN NH√ÉN (DATA FILTERING & LABELING)\n",
    "=========================================================\n",
    "M·ª•c ti√™u: \n",
    "- L·ªçc ch·ªâ gi·ªØ b√†i vi·∫øt v·ªÅ giao th√¥ng TP.HCM nƒÉm 2025\n",
    "- G√°n nh√£n category t·ª± ƒë·ªông (projects/infrastructure/issues/planning/transport)\n",
    "- Ph√°t hi·ªán v√† lo·∫°i b·ªè duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04031e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GIAI ƒêO·∫†N 2: L·ªåC V√Ä G√ÅN NH√ÉN (DATA FILTERING & LABELING)\n",
    "=========================================================\n",
    "M·ª•c ti√™u: \n",
    "- L·ªçc ch·ªâ gi·ªØ b√†i vi·∫øt v·ªÅ giao th√¥ng TP.HCM nƒÉm 2025\n",
    "- G√°n nh√£n category t·ª± ƒë·ªông (projects/infrastructure/issues/planning/transport)\n",
    "- Ph√°t hi·ªán v√† lo·∫°i b·ªè duplicate\n",
    "\"\"\"\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# ==================== C·∫§U H√åNH ====================\n",
    "\n",
    "# MongoDB\n",
    "MONGODB_URI = \"mongodb://localhost:27017/\"\n",
    "DATABASE_NAME = \"hcmc_traffic_summarization\"\n",
    "RAW_COLLECTION = \"raw_data\"\n",
    "FILTERED_COLLECTION = \"filtered_data\"  # Collection sau khi l·ªçc\n",
    "\n",
    "# Th·ªùi gian l·ªçc\n",
    "START_DATE = datetime(2025, 1, 1)\n",
    "END_DATE = datetime(2025, 12, 31)\n",
    "\n",
    "# T·ª´ kh√≥a TP.HCM (ƒë·ªÉ x√°c ƒë·ªãnh b√†i vi·∫øt thu·ªôc TP.HCM)\n",
    "HCMC_KEYWORDS = [\n",
    "    \"tp.hcm\", \"tp hcm\", \"tphcm\", \"hcmc\",\n",
    "    \"h·ªì ch√≠ minh\", \"ho chi minh\",\n",
    "    \"s√†i g√≤n\", \"saigon\",\n",
    "    \"th√†nh ph·ªë h·ªì ch√≠ minh\",\n",
    "    # ƒê·ªãa danh c·ª• th·ªÉ\n",
    "    \"qu·∫≠n 1\", \"qu·∫≠n 2\", \"qu·∫≠n 3\", \"qu·∫≠n 4\", \"qu·∫≠n 5\",\n",
    "    \"b√¨nh th·∫°nh\", \"t√¢n b√¨nh\", \"ph√∫ nhu·∫≠n\", \"g√≤ v·∫•p\",\n",
    "    \"th·ªß ƒë·ª©c\", \"b√¨nh t√¢n\", \"t√¢n ph√∫\",\n",
    "    \"c·∫ßu s√†i g√≤n\", \"c·∫ßu ph√∫ m·ªπ\", \"c·∫ßu b√¨nh tri·ªáu\",\n",
    "    \"ƒë·∫°i l·ªô v√µ vƒÉn ki·ªát\", \"ƒë·∫°i l·ªô ƒë√¥ng t√¢y\"\n",
    "]\n",
    "\n",
    "# T·ª´ kh√≥a giao th√¥ng (ƒë·ªÉ x√°c ƒë·ªãnh b√†i vi·∫øt v·ªÅ giao th√¥ng)\n",
    "TRAFFIC_KEYWORDS = [\n",
    "    # C∆° s·ªü h·∫° t·∫ßng\n",
    "    \"giao th√¥ng\", \"metro\", \"t√†u ƒëi·ªán\", \"cao t·ªëc\", \"c·∫ßu\", \"ƒë∆∞·ªùng\",\n",
    "    \"v√†nh ƒëai\", \"brt\", \"ƒë∆∞·ªùng s·∫Øt\", \"ƒë∆∞·ªùng b·ªô\",\n",
    "    \n",
    "    # D·ª± √°n\n",
    "    \"d·ª± √°n\", \"kh·ªüi c√¥ng\", \"ho√†n th√†nh\", \"ƒë·∫ßu t∆∞\", \"x√¢y d·ª±ng\",\n",
    "    \"tri·ªÉn khai\", \"thi c√¥ng\",\n",
    "    \n",
    "    # V·∫•n ƒë·ªÅ\n",
    "    \"k·∫πt xe\", \"√πn t·∫Øc\", \"tai n·∫°n\", \"giao th√¥ng\", \"ng·∫≠p n∆∞·ªõc\",\n",
    "    \"·ªï g√†\", \"h∆∞ h·ªèng\",\n",
    "    \n",
    "    # Ph∆∞∆°ng ti·ªán\n",
    "    \"xe bu√Ωt\", \"xe bus\", \"taxi\", \"grab\", \"xe m√°y\", \"√¥ t√¥\",\n",
    "    \"v·∫≠n t·∫£i\", \"xe ƒëi·ªán\",\n",
    "    \n",
    "    # Quy ho·∫°ch\n",
    "    \"quy ho·∫°ch\", \"k·∫ø ho·∫°ch\", \"ph√™ duy·ªát\", \"ƒëi·ªÅu ch·ªânh\"\n",
    "]\n",
    "\n",
    "# Danh m·ª•c v√† t·ª´ kh√≥a ƒë·∫∑c tr∆∞ng\n",
    "CATEGORY_KEYWORDS = {\n",
    "    \"projects\": {\n",
    "        \"name\": \"D·ª± √°n giao th√¥ng\",\n",
    "        \"keywords\": [\n",
    "            \"metro\", \"t√†u ƒëi·ªán\", \"cao t·ªëc\", \"c·∫ßu\", \"kh·ªüi c√¥ng\",\n",
    "            \"ho√†n th√†nh\", \"d·ª± √°n\", \"ƒë·∫ßu t∆∞\", \"x√¢y d·ª±ng\", \"tri·ªÉn khai\",\n",
    "            \"thi c√¥ng\", \"ƒë∆∞·ªùng v√†nh ƒëai\", \"brt\"\n",
    "        ]\n",
    "    },\n",
    "    \"infrastructure\": {\n",
    "        \"name\": \"H·∫° t·∫ßng giao th√¥ng\",\n",
    "        \"keywords\": [\n",
    "            \"h·∫° t·∫ßng\", \"ƒë∆∞·ªùng b·ªô\", \"ƒë∆∞·ªùng s·∫Øt\", \"ƒë∆∞·ªùng th·ªßy\",\n",
    "            \"s√¢n bay\", \"c·∫£ng\", \"b·∫øn xe\", \"tr·∫°m\", \"n√∫t giao th√¥ng\",\n",
    "            \"c∆° s·ªü h·∫° t·∫ßng\", \"h·ªá th·ªëng\"\n",
    "        ]\n",
    "    },\n",
    "    \"issues\": {\n",
    "        \"name\": \"V·∫•n ƒë·ªÅ giao th√¥ng\",\n",
    "        \"keywords\": [\n",
    "            \"k·∫πt xe\", \"√πn t·∫Øc\", \"tai n·∫°n\", \"ng·∫≠p n∆∞·ªõc\", \"·ªï g√†\",\n",
    "            \"h∆∞ h·ªèng\", \"√°ch t·∫Øc\", \"t·∫Øc ƒë∆∞·ªùng\", \"va ch·∫°m\",\n",
    "            \"ƒë·ª•ng ƒë·ªô\", \"s·ª± c·ªë\"\n",
    "        ]\n",
    "    },\n",
    "    \"planning\": {\n",
    "        \"name\": \"Quy ho·∫°ch giao th√¥ng\",\n",
    "        \"keywords\": [\n",
    "            \"quy ho·∫°ch\", \"k·∫ø ho·∫°ch\", \"ph√™ duy·ªát\", \"ƒëi·ªÅu ch·ªânh\",\n",
    "            \"ƒë·ªÅ √°n\", \"ch√≠nh s√°ch\", \"ph√°p lu·∫≠t\", \"quy·∫øt ƒë·ªãnh\",\n",
    "            \"ch·ªâ ƒë·∫°o\", \"nghi√™n c·ª©u\"\n",
    "        ]\n",
    "    },\n",
    "    \"transport\": {\n",
    "        \"name\": \"Ph∆∞∆°ng ti·ªán v·∫≠n t·∫£i\",\n",
    "        \"keywords\": [\n",
    "            \"xe bu√Ωt\", \"xe bus\", \"taxi\", \"grab\", \"xe m√°y\", \"√¥ t√¥\",\n",
    "            \"v·∫≠n t·∫£i\", \"xe ƒëi·ªán\", \"xe c√¥ng c·ªông\", \"xe kh√°ch\",\n",
    "            \"tuy·∫øn xe\", \"ƒëi·ªÉm d·ª´ng\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# ==================== K·∫æT N·ªêI DATABASE ====================\n",
    "\n",
    "def connect_mongodb():\n",
    "    \"\"\"\n",
    "    K·∫øt n·ªëi MongoDB v√† t·∫°o collection filtered_data\n",
    "    \n",
    "    Returns:\n",
    "        Tuple (raw_collection, filtered_collection)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = MongoClient(MONGODB_URI, serverSelectionTimeoutMS=5000)\n",
    "        client.server_info()\n",
    "        \n",
    "        db = client[DATABASE_NAME]\n",
    "        raw_col = db[RAW_COLLECTION]\n",
    "        filtered_col = db[FILTERED_COLLECTION]\n",
    "        \n",
    "        # T·∫°o index\n",
    "        filtered_col.create_index([(\"url\", 1)], unique=True)\n",
    "        filtered_col.create_index([(\"published_date\", -1)])\n",
    "        filtered_col.create_index([(\"category\", 1)])\n",
    "        filtered_col.create_index([(\"is_hcmc\", 1)])\n",
    "        \n",
    "        print(f\"‚úÖ ƒê√£ k·∫øt n·ªëi MongoDB\")\n",
    "        print(f\"   Raw data: {raw_col.count_documents({})}\")\n",
    "        return raw_col, filtered_col\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói k·∫øt n·ªëi MongoDB: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# ==================== FILTERING FUNCTIONS ====================\n",
    "\n",
    "def is_hcmc_related(text):\n",
    "    \"\"\"\n",
    "    Ki·ªÉm tra b√†i vi·∫øt c√≥ li√™n quan ƒë·∫øn TP.HCM kh√¥ng\n",
    "    \n",
    "    Args:\n",
    "        text: N·ªôi dung c·∫ßn ki·ªÉm tra (title + content)\n",
    "    \n",
    "    Returns:\n",
    "        bool: True n·∫øu li√™n quan TP.HCM\n",
    "    \"\"\"\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # ƒê·∫øm s·ªë l·∫ßn xu·∫•t hi·ªán t·ª´ kh√≥a TP.HCM\n",
    "    hcmc_count = sum(1 for kw in HCMC_KEYWORDS if kw in text_lower)\n",
    "    \n",
    "    # Quy t·∫Øc: Ph·∫£i c√≥ √≠t nh·∫•t 2 t·ª´ kh√≥a TP.HCM HO·∫∂C 1 t·ª´ kh√≥a xu·∫•t hi·ªán nhi·ªÅu l·∫ßn\n",
    "    if hcmc_count >= 2:\n",
    "        return True\n",
    "    \n",
    "    # Ki·ªÉm tra t·ª´ kh√≥a ch√≠nh xu·∫•t hi·ªán nhi·ªÅu l·∫ßn\n",
    "    for kw in [\"tp.hcm\", \"tphcm\", \"h·ªì ch√≠ minh\", \"s√†i g√≤n\"]:\n",
    "        if text_lower.count(kw) >= 2:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def is_traffic_related(text):\n",
    "    \"\"\"\n",
    "    Ki·ªÉm tra b√†i vi·∫øt c√≥ li√™n quan ƒë·∫øn giao th√¥ng kh√¥ng\n",
    "    \n",
    "    Args:\n",
    "        text: N·ªôi dung c·∫ßn ki·ªÉm tra\n",
    "    \n",
    "    Returns:\n",
    "        bool: True n·∫øu li√™n quan giao th√¥ng\n",
    "    \"\"\"\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # ƒê·∫øm s·ªë l·∫ßn xu·∫•t hi·ªán t·ª´ kh√≥a giao th√¥ng\n",
    "    traffic_count = sum(1 for kw in TRAFFIC_KEYWORDS if kw in text_lower)\n",
    "    \n",
    "    # Quy t·∫Øc: Ph·∫£i c√≥ √≠t nh·∫•t 3 t·ª´ kh√≥a giao th√¥ng\n",
    "    return traffic_count >= 3\n",
    "\n",
    "def parse_date_from_raw(date_raw):\n",
    "    \"\"\"\n",
    "    Parse ng√†y th√°ng t·ª´ chu·ªói raw\n",
    "    \n",
    "    Args:\n",
    "        date_raw: Chu·ªói ng√†y th√°ng ch∆∞a x·ª≠ l√Ω\n",
    "    \n",
    "    Returns:\n",
    "        datetime object ho·∫∑c None\n",
    "    \"\"\"\n",
    "    if not date_raw:\n",
    "        return None\n",
    "    \n",
    "    # Danh s√°ch c√°c format ph·ªï bi·∫øn\n",
    "    formats = [\n",
    "        \"%d/%m/%Y %H:%M\",\n",
    "        \"%d/%m/%Y, %H:%M\",\n",
    "        \"%d-%m-%Y %H:%M\",\n",
    "        \"%d/%m/%Y\",\n",
    "        \"%d-%m-%Y\",\n",
    "        \"%Y-%m-%d\",\n",
    "    ]\n",
    "    \n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            return datetime.strptime(date_raw.strip(), fmt)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Th·ª≠ tr√≠ch xu·∫•t ng√†y t·ª´ chu·ªói b·∫±ng regex\n",
    "    match = re.search(r'(\\d{1,2})[/-](\\d{1,2})[/-](\\d{4})', date_raw)\n",
    "    if match:\n",
    "        day, month, year = match.groups()\n",
    "        try:\n",
    "            return datetime(int(year), int(month), int(day))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return None\n",
    "\n",
    "def categorize_article(title, content):\n",
    "    \"\"\"\n",
    "    Ph√¢n lo·∫°i b√†i vi·∫øt t·ª± ƒë·ªông theo category\n",
    "    \n",
    "    Args:\n",
    "        title: Ti√™u ƒë·ªÅ b√†i vi·∫øt\n",
    "        content: N·ªôi dung b√†i vi·∫øt\n",
    "    \n",
    "    Returns:\n",
    "        str: Category key (projects/infrastructure/issues/planning/transport/general)\n",
    "    \"\"\"\n",
    "    text = (title + \" \" + content).lower()\n",
    "    \n",
    "    # ƒê·∫øm s·ªë l·∫ßn xu·∫•t hi·ªán t·ª´ kh√≥a c·ªßa m·ªói category\n",
    "    scores = {}\n",
    "    for cat_key, cat_data in CATEGORY_KEYWORDS.items():\n",
    "        score = sum(1 for kw in cat_data[\"keywords\"] if kw in text)\n",
    "        scores[cat_key] = score\n",
    "    \n",
    "    # L·∫•y category c√≥ ƒëi·ªÉm cao nh·∫•t\n",
    "    if max(scores.values()) == 0:\n",
    "        return \"general\"\n",
    "    \n",
    "    return max(scores, key=scores.get)\n",
    "\n",
    "def detect_duplicate(content_hash, filtered_col):\n",
    "    \"\"\"\n",
    "    Ph√°t hi·ªán b√†i vi·∫øt tr√πng l·∫∑p d·ª±a v√†o content_hash\n",
    "    \n",
    "    Args:\n",
    "        content_hash: MD5 hash c·ªßa content\n",
    "        filtered_col: MongoDB collection\n",
    "    \n",
    "    Returns:\n",
    "        bool: True n·∫øu l√† duplicate\n",
    "    \"\"\"\n",
    "    existing = filtered_col.find_one({\"content_hash\": content_hash})\n",
    "    return existing is not None\n",
    "\n",
    "# ==================== MAIN FILTERING PIPELINE ====================\n",
    "\n",
    "def filter_article(raw_doc, filtered_col):\n",
    "    \"\"\"\n",
    "    L·ªçc v√† x·ª≠ l√Ω 1 b√†i vi·∫øt raw\n",
    "    \n",
    "    Args:\n",
    "        raw_doc: Document t·ª´ raw_data collection\n",
    "        filtered_col: Collection ƒë·ªÉ l∆∞u k·∫øt qu·∫£\n",
    "    \n",
    "    Returns:\n",
    "        Dict v·ªõi status v√† message\n",
    "    \"\"\"\n",
    "    url = raw_doc.get(\"url\", \"\")\n",
    "    title = raw_doc.get(\"title\", \"\")\n",
    "    content = raw_doc.get(\"content_text\", \"\")\n",
    "    \n",
    "    # B∆∞·ªõc 1: Ki·ªÉm tra duplicate\n",
    "    if detect_duplicate(raw_doc.get(\"content_hash\"), filtered_col):\n",
    "        return {\"status\": \"duplicate\", \"reason\": \"N·ªôi dung tr√πng l·∫∑p\"}\n",
    "    \n",
    "    # B∆∞·ªõc 2: Ki·ªÉm tra li√™n quan TP.HCM\n",
    "    if not is_hcmc_related(title + \" \" + content):\n",
    "        return {\"status\": \"not_hcmc\", \"reason\": \"Kh√¥ng li√™n quan TP.HCM\"}\n",
    "    \n",
    "    # B∆∞·ªõc 3: Ki·ªÉm tra li√™n quan giao th√¥ng\n",
    "    if not is_traffic_related(title + \" \" + content):\n",
    "        return {\"status\": \"not_traffic\", \"reason\": \"Kh√¥ng li√™n quan giao th√¥ng\"}\n",
    "    \n",
    "    # B∆∞·ªõc 4: Parse ng√†y ƒëƒÉng\n",
    "    published_date = parse_date_from_raw(raw_doc.get(\"date_raw\", \"\"))\n",
    "    \n",
    "    # N·∫øu kh√¥ng parse ƒë∆∞·ª£c ng√†y, d√πng scraped_at\n",
    "    if published_date is None:\n",
    "        published_date = raw_doc.get(\"scraped_at\", datetime.now())\n",
    "    \n",
    "    # B∆∞·ªõc 5: Ki·ªÉm tra nƒÉm 2025\n",
    "    if published_date.year != 2025:\n",
    "        return {\"status\": \"wrong_year\", \"reason\": f\"NƒÉm {published_date.year}, kh√¥ng ph·∫£i 2025\"}\n",
    "    \n",
    "    # B∆∞·ªõc 6: G√°n category\n",
    "    category = categorize_article(title, content)\n",
    "    \n",
    "    # T·∫°o document m·ªõi cho filtered_data\n",
    "    filtered_doc = {\n",
    "        \"url\": url,\n",
    "        \"source\": raw_doc.get(\"source\"),\n",
    "        \"source_name\": raw_doc.get(\"source_name\"),\n",
    "        \"title\": title,\n",
    "        \"content_text\": content,\n",
    "        \"content_html\": raw_doc.get(\"content_html\"),\n",
    "        \"content_hash\": raw_doc.get(\"content_hash\"),\n",
    "        \"published_date\": published_date,\n",
    "        \"category\": category,\n",
    "        \"category_name\": CATEGORY_KEYWORDS.get(category, {}).get(\"name\", \"Chung\"),\n",
    "        \"is_hcmc\": True,\n",
    "        \"is_traffic\": True,\n",
    "        \"scraped_at\": raw_doc.get(\"scraped_at\"),\n",
    "        \"filtered_at\": datetime.now(),\n",
    "        \"processing_status\": \"filtered\"  # ƒê√°nh d·∫•u ƒë√£ l·ªçc\n",
    "    }\n",
    "    \n",
    "    return {\"status\": \"pass\", \"doc\": filtered_doc, \"category\": category}\n",
    "\n",
    "def run_filtering(batch_size=100):\n",
    "    \"\"\"\n",
    "    Ch·∫°y qu√° tr√¨nh l·ªçc cho to√†n b·ªô raw_data\n",
    "    \n",
    "    Args:\n",
    "        batch_size: S·ªë l∆∞·ª£ng document x·ª≠ l√Ω m·ªói l·∫ßn\n",
    "    \n",
    "    Returns:\n",
    "        Dict ch·ª©a th·ªëng k√™\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üîç GIAI ƒêO·∫†N 2: L·ªåC V√Ä G√ÅN NH√ÉN\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # K·∫øt n·ªëi MongoDB\n",
    "    raw_col, filtered_col = connect_mongodb()\n",
    "    if raw_col is None or filtered_col is None:\n",
    "        print(\"‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi MongoDB\")\n",
    "        return None\n",
    "    \n",
    "    # L·∫•y t·∫•t c·∫£ raw documents ch∆∞a x·ª≠ l√Ω\n",
    "    total_raw = raw_col.count_documents({\"processing_status\": \"raw\"})\n",
    "    print(f\"üìä T·ªïng s·ªë b√†i raw ch∆∞a x·ª≠ l√Ω: {total_raw}\")\n",
    "    \n",
    "    stats = {\n",
    "        \"total_processed\": 0,\n",
    "        \"passed\": 0,\n",
    "        \"not_hcmc\": 0,\n",
    "        \"not_traffic\": 0,\n",
    "        \"wrong_year\": 0,\n",
    "        \"duplicates\": 0,\n",
    "        \"errors\": 0,\n",
    "        \"by_category\": Counter()\n",
    "    }\n",
    "    \n",
    "    # X·ª≠ l√Ω t·ª´ng batch\n",
    "    cursor = raw_col.find({\"processing_status\": \"raw\"}).batch_size(batch_size)\n",
    "    \n",
    "    for idx, raw_doc in enumerate(cursor, 1):\n",
    "        print(f\"\\r[{idx}/{total_raw}] ƒêang x·ª≠ l√Ω...\", end=\"\", flush=True)\n",
    "        \n",
    "        stats[\"total_processed\"] += 1\n",
    "        \n",
    "        try:\n",
    "            # L·ªçc b√†i vi·∫øt\n",
    "            result = filter_article(raw_doc, filtered_col)\n",
    "            \n",
    "            if result[\"status\"] == \"pass\":\n",
    "                # L∆∞u v√†o filtered_data\n",
    "                try:\n",
    "                    filtered_col.insert_one(result[\"doc\"])\n",
    "                    stats[\"passed\"] += 1\n",
    "                    stats[\"by_category\"][result[\"category\"]] += 1\n",
    "                    \n",
    "                    # C·∫≠p nh·∫≠t tr·∫°ng th√°i trong raw_data\n",
    "                    raw_col.update_one(\n",
    "                        {\"_id\": raw_doc[\"_id\"]},\n",
    "                        {\"$set\": {\"processing_status\": \"filtered\"}}\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"\\n  ‚ùå L·ªói l∆∞u DB: {e}\")\n",
    "                    stats[\"errors\"] += 1\n",
    "            \n",
    "            elif result[\"status\"] == \"duplicate\":\n",
    "                stats[\"duplicates\"] += 1\n",
    "            elif result[\"status\"] == \"not_hcmc\":\n",
    "                stats[\"not_hcmc\"] += 1\n",
    "            elif result[\"status\"] == \"not_traffic\":\n",
    "                stats[\"not_traffic\"] += 1\n",
    "            elif result[\"status\"] == \"wrong_year\":\n",
    "                stats[\"wrong_year\"] += 1\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"\\n  ‚ùå L·ªói x·ª≠ l√Ω: {e}\")\n",
    "            stats[\"errors\"] += 1\n",
    "    \n",
    "    print()  # Xu·ªëng d√≤ng sau progress bar\n",
    "    \n",
    "    # In b√°o c√°o\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"‚úÖ HO√ÄN TH√ÄNH GIAI ƒêO·∫†N 2\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"üìä Th·ªëng k√™:\")\n",
    "    print(f\"   ‚Ä¢ T·ªïng s·ªë ƒë√£ x·ª≠ l√Ω: {stats['total_processed']}\")\n",
    "    print(f\"   ‚Ä¢ ƒê√£ l·ªçc th√†nh c√¥ng: {stats['passed']} ({stats['passed']/stats['total_processed']*100:.1f}%)\")\n",
    "    print(f\"   ‚Ä¢ Kh√¥ng ph·∫£i TP.HCM: {stats['not_hcmc']}\")\n",
    "    print(f\"   ‚Ä¢ Kh√¥ng ph·∫£i giao th√¥ng: {stats['not_traffic']}\")\n",
    "    print(f\"   ‚Ä¢ Kh√¥ng ph·∫£i nƒÉm 2025: {stats['wrong_year']}\")\n",
    "    print(f\"   ‚Ä¢ Tr√πng l·∫∑p: {stats['duplicates']}\")\n",
    "    print(f\"   ‚Ä¢ L·ªói: {stats['errors']}\")\n",
    "    \n",
    "    print(f\"\\nüìÇ Ph√¢n lo·∫°i theo category:\")\n",
    "    for cat_key, count in stats[\"by_category\"].most_common():\n",
    "        cat_name = CATEGORY_KEYWORDS.get(cat_key, {}).get(\"name\", cat_key)\n",
    "        print(f\"   ‚Ä¢ {cat_name}: {count} b√†i\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"‚û°Ô∏è  Ti·∫øp theo: Ch·∫°y 3_data_cleaning.py ƒë·ªÉ l√†m s·∫°ch d·ªØ li·ªáu\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# ==================== CH·∫†Y CH∆Ø∆†NG TR√åNH ====================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ch·∫°y filtering\n",
    "    results = run_filtering(batch_size=100)\n",
    "    \n",
    "    if results:\n",
    "        print(f\"\\n‚úÖ D·ªØ li·ªáu ƒë√£ l·ªçc ƒë∆∞·ª£c l∆∞u v√†o collection '{FILTERED_COLLECTION}'\")\n",
    "        print(f\"üìä S·ªë b√†i ƒë·∫°t chu·∫©n: {results['passed']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2795e67b",
   "metadata": {},
   "source": [
    "GIAI ƒêO·∫†N 3: L√ÄM S·∫†CH D·ªÆ LI·ªÜU (DATA CLEANING)\n",
    "==============================================\n",
    "M·ª•c ti√™u:\n",
    "- Lo·∫°i b·ªè HTML tags, k√Ω t·ª± ƒë·∫∑c bi·ªát, qu·∫£ng c√°o\n",
    "- Chu·∫©n h√≥a unicode, d·∫•u c√¢u, kho·∫£ng tr·∫Øng\n",
    "- T√°ch c√¢u (sentence segmentation)\n",
    "- L∆∞u v√†o collection \"clean_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1273daf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GIAI ƒêO·∫†N 3: L√ÄM S·∫†CH D·ªÆ LI·ªÜU (DATA CLEANING)\n",
    "==============================================\n",
    "M·ª•c ti√™u:\n",
    "- Lo·∫°i b·ªè HTML tags, k√Ω t·ª± ƒë·∫∑c bi·ªát, qu·∫£ng c√°o\n",
    "- Chu·∫©n h√≥a unicode, d·∫•u c√¢u, kho·∫£ng tr·∫Øng\n",
    "- T√°ch c√¢u (sentence segmentation)\n",
    "- L∆∞u v√†o collection \"clean_data\"\n",
    "\"\"\"\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "\n",
    "# ==================== C·∫§U H√åNH ====================\n",
    "\n",
    "# MongoDB\n",
    "MONGODB_URI = \"mongodb://localhost:27017/\"\n",
    "DATABASE_NAME = \"hcmc_traffic_summarization\"\n",
    "FILTERED_COLLECTION = \"filtered_data\"\n",
    "CLEAN_COLLECTION = \"clean_data\"  # Collection sau khi l√†m s·∫°ch\n",
    "\n",
    "# C√°c m·∫´u qu·∫£ng c√°o/spam c·∫ßn lo·∫°i b·ªè\n",
    "SPAM_PATTERNS = [\n",
    "    r\"qu·∫£ng c√°o\",\n",
    "    r\"li√™n h·ªá:?\\s*\\d\",\n",
    "    r\"hotline:?\\s*\\d\",\n",
    "    r\"ƒëƒÉng k√Ω ngay\",\n",
    "    r\"click here\",\n",
    "    r\"xem th√™m t·∫°i\",\n",
    "    r\"theo\\s+\\w+\\.vn\",  # Ngu·ªìn tr√≠ch d·∫´n cu·ªëi b√†i\n",
    "]\n",
    "\n",
    "# C√°c t·ª´/c·ª•m t·ª´ kh√¥ng mang th√¥ng tin (stopwords m·ªü r·ªông)\n",
    "NOISE_PHRASES = [\n",
    "    \"theo ƒë√≥\", \"b√™n c·∫°nh ƒë√≥\", \"ngo√†i ra\",\n",
    "    \"nh∆∞ ƒë√£ bi·∫øt\", \"ƒë∆∞·ª£c bi·∫øt\", \"c√≥ th·ªÉ n√≥i\",\n",
    "    \"c≈©ng theo\", \"ƒë·ªìng th·ªùi\", \"trong khi ƒë√≥\"\n",
    "]\n",
    "\n",
    "# ==================== K·∫æT N·ªêI DATABASE ====================\n",
    "\n",
    "def connect_mongodb():\n",
    "    \"\"\"\n",
    "    K·∫øt n·ªëi MongoDB v√† t·∫°o collection clean_data\n",
    "    \n",
    "    Returns:\n",
    "        Tuple (filtered_collection, clean_collection)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = MongoClient(MONGODB_URI, serverSelectionTimeoutMS=5000)\n",
    "        client.server_info()\n",
    "        \n",
    "        db = client[DATABASE_NAME]\n",
    "        filtered_col = db[FILTERED_COLLECTION]\n",
    "        clean_col = db[CLEAN_COLLECTION]\n",
    "        \n",
    "        # T·∫°o index\n",
    "        clean_col.create_index([(\"url\", 1)], unique=True)\n",
    "        clean_col.create_index([(\"published_date\", -1)])\n",
    "        clean_col.create_index([(\"category\", 1)])\n",
    "        \n",
    "        print(f\"‚úÖ ƒê√£ k·∫øt n·ªëi MongoDB\")\n",
    "        print(f\"   Filtered data: {filtered_col.count_documents({})}\")\n",
    "        return filtered_col, clean_col\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói k·∫øt n·ªëi MongoDB: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# ==================== TEXT CLEANING FUNCTIONS ====================\n",
    "\n",
    "def remove_html(html_text):\n",
    "    \"\"\"\n",
    "    Lo·∫°i b·ªè HTML tags\n",
    "    \n",
    "    Args:\n",
    "        html_text: Chu·ªói ch·ª©a HTML\n",
    "    \n",
    "    Returns:\n",
    "        str: Text thu·∫ßn t√∫y\n",
    "    \"\"\"\n",
    "    if not html_text:\n",
    "        return \"\"\n",
    "    \n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    \n",
    "    # Lo·∫°i b·ªè script v√† style tags\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.decompose()\n",
    "    \n",
    "    # L·∫•y text\n",
    "    text = soup.get_text()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def normalize_unicode(text):\n",
    "    \"\"\"\n",
    "    Chu·∫©n h√≥a Unicode (NFC normalization)\n",
    "    V√≠ d·ª•: \"caf√©\" c√≥ th·ªÉ ƒë∆∞·ª£c encode theo nhi·ªÅu c√°ch\n",
    "    \n",
    "    Args:\n",
    "        text: Chu·ªói c·∫ßn chu·∫©n h√≥a\n",
    "    \n",
    "    Returns:\n",
    "        str: Chu·ªói ƒë√£ chu·∫©n h√≥a\n",
    "    \"\"\"\n",
    "    return unicodedata.normalize('NFC', text)\n",
    "\n",
    "def remove_special_chars(text):\n",
    "    \"\"\"\n",
    "    Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát kh√¥ng c·∫ßn thi·∫øt\n",
    "    \n",
    "    Args:\n",
    "        text: Chu·ªói c·∫ßn x·ª≠ l√Ω\n",
    "    \n",
    "    Returns:\n",
    "        str: Chu·ªói ƒë√£ l√†m s·∫°ch\n",
    "    \"\"\"\n",
    "    # Gi·ªØ l·∫°i: ch·ªØ c√°i, s·ªë, d·∫•u c√¢u c∆° b·∫£n, kho·∫£ng tr·∫Øng\n",
    "    text = re.sub(r'[^\\w\\s.,!?;:()\\-‚Äì‚Äî\\'\\\"¬∞%/]', ' ', text)\n",
    "    \n",
    "    # Lo·∫°i b·ªè emoji\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_spam_content(text):\n",
    "    \"\"\"\n",
    "    Lo·∫°i b·ªè n·ªôi dung qu·∫£ng c√°o/spam\n",
    "    \n",
    "    Args:\n",
    "        text: Chu·ªói c·∫ßn x·ª≠ l√Ω\n",
    "    \n",
    "    Returns:\n",
    "        str: Chu·ªói ƒë√£ lo·∫°i b·ªè spam\n",
    "    \"\"\"\n",
    "    for pattern in SPAM_PATTERNS:\n",
    "        text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def normalize_whitespace(text):\n",
    "    \"\"\"\n",
    "    Chu·∫©n h√≥a kho·∫£ng tr·∫Øng\n",
    "    - Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a\n",
    "    - Lo·∫°i b·ªè d√≤ng tr·ªëng\n",
    "    \n",
    "    Args:\n",
    "        text: Chu·ªói c·∫ßn x·ª≠ l√Ω\n",
    "    \n",
    "    Returns:\n",
    "        str: Chu·ªói ƒë√£ chu·∫©n h√≥a\n",
    "    \"\"\"\n",
    "    # Thay th·∫ø nhi·ªÅu kho·∫£ng tr·∫Øng b·∫±ng 1\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng ƒë·∫ßu/cu·ªëi m·ªói d√≤ng\n",
    "    lines = [line.strip() for line in text.split('\\n')]\n",
    "    \n",
    "    # Lo·∫°i b·ªè d√≤ng tr·ªëng\n",
    "    lines = [line for line in lines if line]\n",
    "    \n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "def normalize_punctuation(text):\n",
    "    \"\"\"\n",
    "    Chu·∫©n h√≥a d·∫•u c√¢u\n",
    "    \n",
    "    Args:\n",
    "        text: Chu·ªói c·∫ßn x·ª≠ l√Ω\n",
    "    \n",
    "    Returns:\n",
    "        str: Chu·ªói ƒë√£ chu·∫©n h√≥a d·∫•u c√¢u\n",
    "    \"\"\"\n",
    "    # Chu·∫©n h√≥a d·∫•u ngo·∫∑c k√©p\n",
    "    text = text.replace('\"', '\"').replace('\"', '\"')\n",
    "    text = text.replace(\"'\", \"'\").replace(\"'\", \"'\")\n",
    "    \n",
    "    # Chu·∫©n h√≥a d·∫•u g·∫°ch ngang\n",
    "    text = text.replace('‚Äì', '-').replace('‚Äî', '-')\n",
    "    \n",
    "    # Th√™m kho·∫£ng tr·∫Øng sau d·∫•u c√¢u n·∫øu thi·∫øu\n",
    "    text = re.sub(r'([.,!?;:])([^\\s])', r'\\1 \\2', text)\n",
    "    \n",
    "    # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng tr∆∞·ªõc d·∫•u c√¢u\n",
    "    text = re.sub(r'\\s+([.,!?;:])', r'\\1', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def segment_sentences(text):\n",
    "    \"\"\"\n",
    "    T√°ch c√¢u trong vƒÉn b·∫£n ti·∫øng Vi·ªát\n",
    "    \n",
    "    Args:\n",
    "        text: Chu·ªói vƒÉn b·∫£n\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: Danh s√°ch c√°c c√¢u\n",
    "    \"\"\"\n",
    "    # T√°ch theo d·∫•u c√¢u k·∫øt th√∫c\n",
    "    sentences = re.split(r'[.!?]\\s+', text)\n",
    "    \n",
    "    # L√†m s·∫°ch v√† l·ªçc c√¢u\n",
    "    cleaned_sentences = []\n",
    "    for sent in sentences:\n",
    "        sent = sent.strip()\n",
    "        \n",
    "        # Ch·ªâ gi·ªØ c√¢u c√≥ ƒë·ªô d√†i h·ª£p l√Ω (20-500 k√Ω t·ª±)\n",
    "        if 20 <= len(sent) <= 500:\n",
    "            # Lo·∫°i b·ªè c√¢u ch·ªâ ch·ª©a s·ªë ho·∫∑c k√Ω t·ª± ƒë·∫∑c bi·ªát\n",
    "            if re.search(r'[a-zA-Z√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµƒë]', sent):\n",
    "                cleaned_sentences.append(sent)\n",
    "    \n",
    "    return cleaned_sentences\n",
    "\n",
    "def clean_title(title):\n",
    "    \"\"\"\n",
    "    L√†m s·∫°ch ti√™u ƒë·ªÅ\n",
    "    \n",
    "    Args:\n",
    "        title: Ti√™u ƒë·ªÅ c·∫ßn l√†m s·∫°ch\n",
    "    \n",
    "    Returns:\n",
    "        str: Ti√™u ƒë·ªÅ ƒë√£ l√†m s·∫°ch\n",
    "    \"\"\"\n",
    "    # Lo·∫°i b·ªè emoji v√† k√Ω t·ª± ƒë·∫∑c bi·ªát\n",
    "    title = remove_special_chars(title)\n",
    "    \n",
    "    # Chu·∫©n h√≥a\n",
    "    title = normalize_unicode(title)\n",
    "    title = normalize_whitespace(title)\n",
    "    title = normalize_punctuation(title)\n",
    "    \n",
    "    # Lo·∫°i b·ªè c√°c t·ª´ nh∆∞ [Video], [Infographic]\n",
    "    title = re.sub(r'\\[[\\w\\s]+\\]', '', title)\n",
    "    \n",
    "    return title.strip()\n",
    "\n",
    "def clean_content(content_text, content_html):\n",
    "    \"\"\"\n",
    "    L√†m s·∫°ch n·ªôi dung b√†i vi·∫øt\n",
    "    \n",
    "    Args:\n",
    "        content_text: N·ªôi dung d·∫°ng text thu·∫ßn\n",
    "        content_html: N·ªôi dung d·∫°ng HTML\n",
    "    \n",
    "    Returns:\n",
    "        Dict ch·ª©a n·ªôi dung ƒë√£ l√†m s·∫°ch v√† sentences\n",
    "    \"\"\"\n",
    "    # ∆Øu ti√™n x·ª≠ l√Ω t·ª´ HTML ƒë·ªÉ c√≥ k·∫øt qu·∫£ t·ªët h∆°n\n",
    "    if content_html:\n",
    "        text = remove_html(content_html)\n",
    "    else:\n",
    "        text = content_text\n",
    "    \n",
    "    # Pipeline l√†m s·∫°ch\n",
    "    text = normalize_unicode(text)\n",
    "    text = remove_spam_content(text)\n",
    "    text = remove_special_chars(text)\n",
    "    text = normalize_whitespace(text)\n",
    "    text = normalize_punctuation(text)\n",
    "    \n",
    "    # T√°ch c√¢u\n",
    "    sentences = segment_sentences(text)\n",
    "    \n",
    "    # G·ªôp l·∫°i th√†nh vƒÉn b·∫£n li·ªÅn m·∫°ch\n",
    "    clean_text = '. '.join(sentences) + '.'\n",
    "    \n",
    "    return {\n",
    "        \"content_clean\": clean_text,\n",
    "        \"sentences\": sentences,\n",
    "        \"num_sentences\": len(sentences),\n",
    "        \"num_words\": len(clean_text.split())\n",
    "    }\n",
    "\n",
    "# ==================== MAIN CLEANING PIPELINE ====================\n",
    "\n",
    "def clean_article(filtered_doc):\n",
    "    \"\"\"\n",
    "    L√†m s·∫°ch 1 b√†i vi·∫øt\n",
    "    \n",
    "    Args:\n",
    "        filtered_doc: Document t·ª´ filtered_data collection\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Document ƒë√£ l√†m s·∫°ch\n",
    "    \"\"\"\n",
    "    # L√†m s·∫°ch title\n",
    "    clean_title_text = clean_title(filtered_doc.get(\"title\", \"\"))\n",
    "    \n",
    "    # L√†m s·∫°ch content\n",
    "    content_result = clean_content(\n",
    "        filtered_doc.get(\"content_text\", \"\"),\n",
    "        filtered_doc.get(\"content_html\", \"\")\n",
    "    )\n",
    "    \n",
    "    # T·∫°o document m·ªõi\n",
    "    clean_doc = {\n",
    "        \"url\": filtered_doc.get(\"url\"),\n",
    "        \"source\": filtered_doc.get(\"source\"),\n",
    "        \"source_name\": filtered_doc.get(\"source_name\"),\n",
    "        \n",
    "        # D·ªØ li·ªáu ƒë√£ l√†m s·∫°ch\n",
    "        \"title_clean\": clean_title_text,\n",
    "        \"content_clean\": content_result[\"content_clean\"],\n",
    "        \"sentences\": content_result[\"sentences\"],\n",
    "        \n",
    "        # Metadata\n",
    "        \"num_sentences\": content_result[\"num_sentences\"],\n",
    "        \"num_words\": content_result[\"num_words\"],\n",
    "        \"content_hash\": filtered_doc.get(\"content_hash\"),\n",
    "        \n",
    "        # Ph√¢n lo·∫°i\n",
    "        \"published_date\": filtered_doc.get(\"published_date\"),\n",
    "        \"category\": filtered_doc.get(\"category\"),\n",
    "        \"category_name\": filtered_doc.get(\"category_name\"),\n",
    "        \"is_hcmc\": filtered_doc.get(\"is_hcmc\"),\n",
    "        \"is_traffic\": filtered_doc.get(\"is_traffic\"),\n",
    "        \n",
    "        # Timestamps\n",
    "        \"scraped_at\": filtered_doc.get(\"scraped_at\"),\n",
    "        \"filtered_at\": filtered_doc.get(\"filtered_at\"),\n",
    "        \"cleaned_at\": datetime.now(),\n",
    "        \"processing_status\": \"cleaned\"  # ƒê√°nh d·∫•u ƒë√£ l√†m s·∫°ch\n",
    "    }\n",
    "    \n",
    "    return clean_doc\n",
    "\n",
    "def run_cleaning(batch_size=100):\n",
    "    \"\"\"\n",
    "    Ch·∫°y qu√° tr√¨nh l√†m s·∫°ch cho to√†n b·ªô filtered_data\n",
    "    \n",
    "    Args:\n",
    "        batch_size: S·ªë l∆∞·ª£ng document x·ª≠ l√Ω m·ªói l·∫ßn\n",
    "    \n",
    "    Returns:\n",
    "        Dict ch·ª©a th·ªëng k√™\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üßπ GIAI ƒêO·∫†N 3: L√ÄM S·∫†CH D·ªÆ LI·ªÜU\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # K·∫øt n·ªëi MongoDB\n",
    "    filtered_col, clean_col = connect_mongodb()\n",
    "    if filtered_col is None or clean_col is None:\n",
    "        print(\"‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi MongoDB\")\n",
    "        return None\n",
    "    \n",
    "    # L·∫•y t·∫•t c·∫£ filtered documents ch∆∞a l√†m s·∫°ch\n",
    "    total_filtered = filtered_col.count_documents({\"processing_status\": \"filtered\"})\n",
    "    print(f\"üìä T·ªïng s·ªë b√†i c·∫ßn l√†m s·∫°ch: {total_filtered}\")\n",
    "    \n",
    "    stats = {\n",
    "        \"total_processed\": 0,\n",
    "        \"cleaned\": 0,\n",
    "        \"errors\": 0,\n",
    "        \"total_sentences\": 0,\n",
    "        \"total_words\": 0\n",
    "    }\n",
    "    \n",
    "    # X·ª≠ l√Ω t·ª´ng batch\n",
    "    cursor = filtered_col.find({\"processing_status\": \"filtered\"}).batch_size(batch_size)\n",
    "    \n",
    "    for idx, filtered_doc in enumerate(cursor, 1):\n",
    "        print(f\"\\r[{idx}/{total_filtered}] ƒêang x·ª≠ l√Ω...\", end=\"\", flush=True)\n",
    "        \n",
    "        stats[\"total_processed\"] += 1\n",
    "        \n",
    "        try:\n",
    "            # L√†m s·∫°ch b√†i vi·∫øt\n",
    "            clean_doc = clean_article(filtered_doc)\n",
    "            \n",
    "            # L∆∞u v√†o clean_data\n",
    "            try:\n",
    "                clean_col.insert_one(clean_doc)\n",
    "                stats[\"cleaned\"] += 1\n",
    "                stats[\"total_sentences\"] += clean_doc[\"num_sentences\"]\n",
    "                stats[\"total_words\"] += clean_doc[\"num_words\"]\n",
    "                \n",
    "                # C·∫≠p nh·∫≠t tr·∫°ng th√°i trong filtered_data\n",
    "                filtered_col.update_one(\n",
    "                    {\"_id\": filtered_doc[\"_id\"]},\n",
    "                    {\"$set\": {\"processing_status\": \"cleaned\"}}\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"\\n  ‚ùå L·ªói l∆∞u DB: {e}\")\n",
    "                stats[\"errors\"] += 1\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"\\n  ‚ùå L·ªói x·ª≠ l√Ω: {e}\")\n",
    "            stats[\"errors\"] += 1\n",
    "    \n",
    "    print()  # Xu·ªëng d√≤ng sau progress bar\n",
    "    \n",
    "    # In b√°o c√°o\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"‚úÖ HO√ÄN TH√ÄNH GIAI ƒêO·∫†N 3\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"üìä Th·ªëng k√™:\")\n",
    "    print(f\"   ‚Ä¢ T·ªïng s·ªë ƒë√£ x·ª≠ l√Ω: {stats['total_processed']}\")\n",
    "    print(f\"   ‚Ä¢ ƒê√£ l√†m s·∫°ch th√†nh c√¥ng: {stats['cleaned']}\")\n",
    "    print(f\"   ‚Ä¢ L·ªói: {stats['errors']}\")\n",
    "    print(f\"   ‚Ä¢ T·ªïng s·ªë c√¢u: {stats['total_sentences']}\")\n",
    "    print(f\"   ‚Ä¢ T·ªïng s·ªë t·ª´: {stats['total_words']}\")\n",
    "    \n",
    "    if stats['cleaned'] > 0:\n",
    "        avg_sentences = stats['total_sentences'] / stats['cleaned']\n",
    "        avg_words = stats['total_words'] / stats['cleaned']\n",
    "        print(f\"   ‚Ä¢ Trung b√¨nh: {avg_sentences:.1f} c√¢u/b√†i, {avg_words:.0f} t·ª´/b√†i\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"‚û°Ô∏è  Ti·∫øp theo: Ch·∫°y 4_model_training.py ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# ==================== CH·∫†Y CH∆Ø∆†NG TR√åNH ====================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ch·∫°y cleaning\n",
    "    results = run_cleaning(batch_size=100)\n",
    "    \n",
    "    if results:\n",
    "        print(f\"\\n‚úÖ D·ªØ li·ªáu ƒë√£ l√†m s·∫°ch ƒë∆∞·ª£c l∆∞u v√†o collection '{CLEAN_COLLECTION}'\")\n",
    "        print(f\"üìä S·ªë b√†i ƒë√£ x·ª≠ l√Ω: {results['cleaned']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a24335",
   "metadata": {},
   "source": [
    "GIAI ƒêO·∫†N 4: HU·∫§N LUY·ªÜN M√î H√åNH (MODEL TRAINING)\n",
    "=================================================\n",
    "M·ª•c ti√™u:\n",
    "- S·ª≠ d·ª•ng ViT5 (Vietnamese T5) cho t√≥m t·∫Øt abstractive\n",
    "- Fine-tune tr√™n d·ªØ li·ªáu giao th√¥ng TP.HCM\n",
    "- Chia train/val/test: 70/15/15\n",
    "- ƒê√°nh gi√° b·∫±ng ROUGE, BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caad2207",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GIAI ƒêO·∫†N 4: HU·∫§N LUY·ªÜN M√î H√åNH (MODEL TRAINING)\n",
    "=================================================\n",
    "M·ª•c ti√™u:\n",
    "- S·ª≠ d·ª•ng ViT5 (Vietnamese T5) cho t√≥m t·∫Øt abstractive\n",
    "- Fine-tune tr√™n d·ªØ li·ªáu giao th√¥ng TP.HCM\n",
    "- Chia train/val/test: 70/15/15\n",
    "- ƒê√°nh gi√° b·∫±ng ROUGE, BLEU\n",
    "\"\"\"\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ƒê√°nh gi√° metrics\n",
    "try:\n",
    "    from rouge_score import rouge_scorer\n",
    "    ROUGE_AVAILABLE = True\n",
    "except:\n",
    "    ROUGE_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  Ch∆∞a c√†i rouge_score. Ch·∫°y: pip install rouge-score\")\n",
    "\n",
    "# ==================== C·∫§U H√åNH ====================\n",
    "\n",
    "# MongoDB\n",
    "MONGODB_URI = \"mongodb://localhost:27017/\"\n",
    "DATABASE_NAME = \"hcmc_traffic_summarization\"\n",
    "CLEAN_COLLECTION = \"clean_data\"\n",
    "\n",
    "# M√¥ h√¨nh\n",
    "MODEL_NAME = \"VietAI/vit5-base\"  # M√¥ h√¨nh ViT5 pre-trained\n",
    "OUTPUT_DIR = \"./models/vit5_hcmc_traffic\"  # Th∆∞ m·ª•c l∆∞u model\n",
    "\n",
    "# Training hyperparameters\n",
    "MAX_INPUT_LENGTH = 512  # ƒê·ªô d√†i t·ªëi ƒëa c·ªßa input (b√†i b√°o)\n",
    "MAX_TARGET_LENGTH = 128  # ƒê·ªô d√†i t·ªëi ƒëa c·ªßa summary\n",
    "BATCH_SIZE = 4  # Gi·∫£m n·∫øu thi·∫øu RAM/VRAM\n",
    "LEARNING_RATE = 5e-5\n",
    "NUM_EPOCHS = 5\n",
    "WARMUP_STEPS = 500\n",
    "\n",
    "# Data split\n",
    "TRAIN_RATIO = 0.70\n",
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.15\n",
    "\n",
    "# ==================== K·∫æT N·ªêI DATABASE ====================\n",
    "\n",
    "def connect_mongodb():\n",
    "    \"\"\"\n",
    "    K·∫øt n·ªëi MongoDB v√† l·∫•y clean_data\n",
    "    \n",
    "    Returns:\n",
    "        Collection object ho·∫∑c None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = MongoClient(MONGODB_URI, serverSelectionTimeoutMS=5000)\n",
    "        client.server_info()\n",
    "        \n",
    "        db = client[DATABASE_NAME]\n",
    "        clean_col = db[CLEAN_COLLECTION]\n",
    "        \n",
    "        total = clean_col.count_documents({})\n",
    "        print(f\"‚úÖ ƒê√£ k·∫øt n·ªëi MongoDB\")\n",
    "        print(f\"   Clean data: {total} b√†i\")\n",
    "        return clean_col\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói k·∫øt n·ªëi MongoDB: {e}\")\n",
    "        return None\n",
    "\n",
    "# ==================== CHU·∫®N B·ªä D·ªÆ LI·ªÜU ====================\n",
    "\n",
    "def load_data_from_mongodb(clean_col):\n",
    "    \"\"\"\n",
    "    Load d·ªØ li·ªáu t·ª´ MongoDB v√† chu·∫©n b·ªã cho training\n",
    "    \n",
    "    Args:\n",
    "        clean_col: MongoDB collection\n",
    "    \n",
    "    Returns:\n",
    "        List of dicts v·ªõi format {content, summary}\n",
    "    \"\"\"\n",
    "    print(f\"\\nüì• ƒêang load d·ªØ li·ªáu t·ª´ MongoDB...\")\n",
    "    \n",
    "    data = []\n",
    "    cursor = clean_col.find({\n",
    "        \"processing_status\": \"cleaned\",\n",
    "        \"num_sentences\": {\"$gte\": 5}  # Ch·ªâ l·∫•y b√†i c√≥ √≠t nh·∫•t 5 c√¢u\n",
    "    })\n",
    "    \n",
    "    for doc in cursor:\n",
    "        # Input: N·ªôi dung b√†i b√°o ƒë√£ l√†m s·∫°ch\n",
    "        content = doc.get(\"content_clean\", \"\")\n",
    "        \n",
    "        # Target: T·∫°o summary t·ª´ 3 c√¢u ƒë·∫ßu ti√™n (pseudo-label)\n",
    "        # L∆ØU √ù: ƒê√¢y l√† ph∆∞∆°ng ph√°p t·∫°m th·ªùi. T·ªët nh·∫•t l√† c√≥ human-labeled summaries\n",
    "        sentences = doc.get(\"sentences\", [])\n",
    "        if len(sentences) >= 5:\n",
    "            # L·∫•y 3 c√¢u ƒë·∫ßu l√†m summary (extractive)\n",
    "            summary = \". \".join(sentences[:3]) + \".\"\n",
    "            \n",
    "            data.append({\n",
    "                \"content\": content,\n",
    "                \"summary\": summary,\n",
    "                \"category\": doc.get(\"category\", \"general\"),\n",
    "                \"url\": doc.get(\"url\", \"\")\n",
    "            })\n",
    "    \n",
    "    print(f\"‚úÖ ƒê√£ load {len(data)} b√†i c√≥ ƒë·ªß ƒëi·ªÅu ki·ªán\")\n",
    "    return data\n",
    "\n",
    "def split_dataset(data):\n",
    "    \"\"\"\n",
    "    Chia dataset th√†nh train/val/test\n",
    "    \n",
    "    Args:\n",
    "        data: List of dicts\n",
    "    \n",
    "    Returns:\n",
    "        Tuple (train_data, val_data, test_data)\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîÄ ƒêang chia dataset...\")\n",
    "    \n",
    "    # Chia train + temp (val + test)\n",
    "    train_data, temp_data = train_test_split(\n",
    "        data, \n",
    "        test_size=(VAL_RATIO + TEST_RATIO),\n",
    "        random_state=42,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Chia val v√† test\n",
    "    val_ratio_adjusted = VAL_RATIO / (VAL_RATIO + TEST_RATIO)\n",
    "    val_data, test_data = train_test_split(\n",
    "        temp_data,\n",
    "        test_size=(1 - val_ratio_adjusted),\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ ƒê√£ chia dataset:\")\n",
    "    print(f\"   ‚Ä¢ Train: {len(train_data)} b√†i ({len(train_data)/len(data)*100:.1f}%)\")\n",
    "    print(f\"   ‚Ä¢ Val: {len(val_data)} b√†i ({len(val_data)/len(data)*100:.1f}%)\")\n",
    "    print(f\"   ‚Ä¢ Test: {len(test_data)} b√†i ({len(test_data)/len(data)*100:.1f}%)\")\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "def prepare_dataset(data, tokenizer):\n",
    "    \"\"\"\n",
    "    Chu·∫©n b·ªã dataset cho Hugging Face Trainer\n",
    "    \n",
    "    Args:\n",
    "        data: List of dicts\n",
    "        tokenizer: Tokenizer object\n",
    "    \n",
    "    Returns:\n",
    "        Dataset object\n",
    "    \"\"\"\n",
    "    # Chuy·ªÉn sang format c·ªßa Hugging Face\n",
    "    dataset_dict = {\n",
    "        \"content\": [item[\"content\"] for item in data],\n",
    "        \"summary\": [item[\"summary\"] for item in data]\n",
    "    }\n",
    "    \n",
    "    dataset = Dataset.from_dict(dataset_dict)\n",
    "    \n",
    "    # Tokenize\n",
    "    def tokenize_function(examples):\n",
    "        # Tokenize input (content)\n",
    "        model_inputs = tokenizer(\n",
    "            examples[\"content\"],\n",
    "            max_length=MAX_INPUT_LENGTH,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "        \n",
    "        # Tokenize target (summary)\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(\n",
    "                examples[\"summary\"],\n",
    "                max_length=MAX_TARGET_LENGTH,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\"\n",
    "            )\n",
    "        \n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "    \n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "    \n",
    "    return tokenized_dataset\n",
    "\n",
    "# ==================== METRICS ====================\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    T√≠nh to√°n metrics cho evaluation\n",
    "    \n",
    "    Args:\n",
    "        eval_pred: Tuple (predictions, labels)\n",
    "    \n",
    "    Returns:\n",
    "        Dict ch·ª©a c√°c metrics\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Decode predictions v√† labels\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 trong labels (padding tokens)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # T√≠nh ROUGE scores\n",
    "    if ROUGE_AVAILABLE:\n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=False)\n",
    "        \n",
    "        rouge1_scores = []\n",
    "        rouge2_scores = []\n",
    "        rougeL_scores = []\n",
    "        \n",
    "        for pred, label in zip(decoded_preds, decoded_labels):\n",
    "            scores = scorer.score(label, pred)\n",
    "            rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "            rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "            rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "        \n",
    "        return {\n",
    "            \"rouge1\": np.mean(rouge1_scores),\n",
    "            \"rouge2\": np.mean(rouge2_scores),\n",
    "            \"rougeL\": np.mean(rougeL_scores)\n",
    "        }\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "# ==================== MAIN TRAINING PIPELINE ====================\n",
    "\n",
    "def train_model():\n",
    "    \"\"\"\n",
    "    Hu·∫•n luy·ªán m√¥ h√¨nh ViT5 cho t√≥m t·∫Øt\n",
    "    \n",
    "    Returns:\n",
    "        Trainer object\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ü§ñ GIAI ƒêO·∫†N 4: HU·∫§N LUY·ªÜN M√î H√åNH\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # B∆∞·ªõc 1: Load d·ªØ li·ªáu t·ª´ MongoDB\n",
    "    clean_col = connect_mongodb()\n",
    "    if clean_col is None:\n",
    "        print(\"‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi MongoDB\")\n",
    "        return None\n",
    "    \n",
    "    data = load_data_from_mongodb(clean_col)\n",
    "    \n",
    "    if len(data) < 100:\n",
    "        print(f\"‚ö†Ô∏è  C·∫£nh b√°o: Ch·ªâ c√≥ {len(data)} b√†i. C·∫ßn √≠t nh·∫•t 100 b√†i ƒë·ªÉ train t·ªët.\")\n",
    "        response = input(\"Ti·∫øp t·ª•c? (y/n): \")\n",
    "        if response.lower() != 'y':\n",
    "            return None\n",
    "    \n",
    "    # B∆∞·ªõc 2: Chia dataset\n",
    "    train_data, val_data, test_data = split_dataset(data)\n",
    "    \n",
    "    # L∆∞u test set ƒë·ªÉ evaluation sau\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    with open(f\"{OUTPUT_DIR}/test_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(test_data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"‚úÖ ƒê√£ l∆∞u test set: {OUTPUT_DIR}/test_data.json\")\n",
    "    \n",
    "    # B∆∞·ªõc 3: Load tokenizer v√† model\n",
    "    print(f\"\\nüì¶ ƒêang load m√¥ h√¨nh {MODEL_NAME}...\")\n",
    "    \n",
    "    global tokenizer  # ƒê·ªÉ d√πng trong compute_metrics\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    print(f\"‚úÖ ƒê√£ load m√¥ h√¨nh\")\n",
    "    \n",
    "    # Ki·ªÉm tra GPU\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"üíª Device: {device}\")\n",
    "    \n",
    "    if device == \"cpu\":\n",
    "        print(\"‚ö†Ô∏è  C·∫£nh b√°o: ƒêang d√πng CPU. Training s·∫Ω r·∫•t ch·∫≠m. Khuy·∫øn ngh·ªã d√πng GPU.\")\n",
    "    \n",
    "    # B∆∞·ªõc 4: Chu·∫©n b·ªã datasets\n",
    "    print(f\"\\nüî® ƒêang tokenize datasets...\")\n",
    "    train_dataset = prepare_dataset(train_data, tokenizer)\n",
    "    val_dataset = prepare_dataset(val_data, tokenizer)\n",
    "    print(f\"‚úÖ ƒê√£ tokenize xong\")\n",
    "    \n",
    "    # B∆∞·ªõc 5: Training arguments\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        evaluation_strategy=\"epoch\",  # Evaluate m·ªói epoch\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        warmup_steps=WARMUP_STEPS,\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=2,  # Ch·ªâ gi·ªØ 2 checkpoint t·ªët nh·∫•t\n",
    "        predict_with_generate=True,  # Generate text khi evaluate\n",
    "        fp16=True if device == \"cuda\" else False,  # Mixed precision training\n",
    "        logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    "        logging_steps=50,\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"rougeL\" if ROUGE_AVAILABLE else \"loss\",\n",
    "        greater_is_better=True if ROUGE_AVAILABLE else False,\n",
    "    )\n",
    "    \n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "    \n",
    "    # B∆∞·ªõc 6: Kh·ªüi t·∫°o Trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics if ROUGE_AVAILABLE else None\n",
    "    )\n",
    "    \n",
    "    # B∆∞·ªõc 7: B·∫Øt ƒë·∫ßu training\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üöÄ B·∫ÆT ƒê·∫¶U TRAINING\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    print(f\"C·∫•u h√¨nh:\")\n",
    "    print(f\"   ‚Ä¢ Epochs: {NUM_EPOCHS}\")\n",
    "    print(f\"   ‚Ä¢ Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"   ‚Ä¢ Learning rate: {LEARNING_RATE}\")\n",
    "    print(f\"   ‚Ä¢ Device: {device}\")\n",
    "    print(f\"\\n‚è±Ô∏è  Th·ªùi gian d·ª± ki·∫øn: {len(train_data) * NUM_EPOCHS / BATCH_SIZE / 60:.1f} ph√∫t\")\n",
    "    print(f\"\\nƒêang training...\\n\")\n",
    "    \n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    # B∆∞·ªõc 8: L∆∞u model\n",
    "    print(f\"\\nüíæ ƒêang l∆∞u model...\")\n",
    "    trainer.save_model(OUTPUT_DIR)\n",
    "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "    \n",
    "    # L∆∞u training metrics\n",
    "    metrics = train_result.metrics\n",
    "    with open(f\"{OUTPUT_DIR}/train_metrics.json\", \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"‚úÖ HO√ÄN TH√ÄNH TRAINING\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"üìä Training metrics:\")\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"   ‚Ä¢ {key}: {value:.4f}\")\n",
    "    \n",
    "    print(f\"\\nüíæ Model ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {OUTPUT_DIR}\")\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"‚û°Ô∏è  Ti·∫øp theo: Ch·∫°y 5_model_evaluation.py ƒë·ªÉ ƒë√°nh gi√° chi ti·∫øt\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return trainer\n",
    "\n",
    "# ==================== CH·∫†Y CH∆Ø∆†NG TR√åNH ====================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è  L∆ØU √ù QUAN TR·ªåNG:\")\n",
    "    print(f\"   ‚Ä¢ Training m√¥ h√¨nh c·∫ßn GPU m·∫°nh (√≠t nh·∫•t 8GB VRAM)\")\n",
    "    print(f\"   ‚Ä¢ Th·ªùi gian: 1-3 gi·ªù t√πy s·ªë l∆∞·ª£ng b√†i v√† GPU\")\n",
    "    print(f\"   ‚Ä¢ C·∫ßn c√†i ƒë·∫∑t: pip install transformers datasets torch rouge-score\")\n",
    "    print(f\"   ‚Ä¢ Hi·ªán t·∫°i ƒëang d√πng pseudo-labels (3 c√¢u ƒë·∫ßu). T·ªët nh·∫•t c√≥ human-labeled data\\n\")\n",
    "    \n",
    "    # H·ªèi x√°c nh·∫≠n\n",
    "    if len(sys.argv) > 1 and sys.argv[1] == \"--auto\":\n",
    "        # Auto mode: kh√¥ng h·ªèi\n",
    "        pass\n",
    "    else:\n",
    "        response = input(\"B·∫°n c√≥ mu·ªën ti·∫øp t·ª•c training? (y/n): \")\n",
    "        if response.lower() != 'y':\n",
    "            print(\"‚ùå ƒê√£ h·ªßy training\")\n",
    "            sys.exit(0)\n",
    "    \n",
    "    # B·∫Øt ƒë·∫ßu training\n",
    "    trainer = train_model()\n",
    "    \n",
    "    if trainer:\n",
    "        print(f\"\\n‚úÖ Training th√†nh c√¥ng!\")\n",
    "        print(f\"üìÅ Model ƒë∆∞·ª£c l∆∞u t·∫°i: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf43f07e",
   "metadata": {},
   "source": [
    "GIAI ƒêO·∫†N 5: ƒê√ÅNH GI√Å M√î H√åNH (MODEL EVALUATION)\n",
    "=================================================\n",
    "M·ª•c ti√™u:\n",
    "- ƒê√°nh gi√° m√¥ h√¨nh tr√™n test set\n",
    "- T√≠nh ROUGE, BLEU scores\n",
    "- Ki·ªÉm tra overfitting\n",
    "- Ph√¢n t√≠ch l·ªói"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfe8402",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GIAI ƒêO·∫†N 5: ƒê√ÅNH GI√Å M√î H√åNH (MODEL EVALUATION)\n",
    "=================================================\n",
    "M·ª•c ti√™u:\n",
    "- ƒê√°nh gi√° m√¥ h√¨nh tr√™n test set\n",
    "- T√≠nh ROUGE, BLEU scores\n",
    "- Ki·ªÉm tra overfitting\n",
    "- Ph√¢n t√≠ch l·ªói\n",
    "\"\"\"\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Metrics\n",
    "try:\n",
    "    from rouge_score import rouge_scorer\n",
    "    ROUGE_AVAILABLE = True\n",
    "except:\n",
    "    ROUGE_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  C·∫ßn c√†i: pip install rouge-score\")\n",
    "\n",
    "try:\n",
    "    from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "    BLEU_AVAILABLE = True\n",
    "except:\n",
    "    BLEU_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  C·∫ßn c√†i: pip install nltk\")\n",
    "\n",
    "# ==================== C·∫§U H√åNH ====================\n",
    "\n",
    "MODEL_DIR = \"./models/vit5_hcmc_traffic\"  # Th∆∞ m·ª•c ch·ª©a model ƒë√£ train\n",
    "TEST_DATA_PATH = f\"{MODEL_DIR}/test_data.json\"  # Test set\n",
    "\n",
    "# Generation parameters\n",
    "MAX_LENGTH = 128\n",
    "MIN_LENGTH = 30\n",
    "NUM_BEAMS = 4  # Beam search\n",
    "NO_REPEAT_NGRAM_SIZE = 3  # Tr√°nh l·∫∑p n-gram\n",
    "\n",
    "# ==================== LOAD MODEL ====================\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"\n",
    "    Load m√¥ h√¨nh ƒë√£ fine-tune\n",
    "    \n",
    "    Returns:\n",
    "        Tuple (model, tokenizer, device)\n",
    "    \"\"\"\n",
    "    print(f\"\\nüì¶ ƒêang load m√¥ h√¨nh t·ª´ {MODEL_DIR}...\")\n",
    "    \n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_DIR)\n",
    "        \n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        model = model.to(device)\n",
    "        model.eval()  # Evaluation mode\n",
    "        \n",
    "        print(f\"‚úÖ ƒê√£ load m√¥ h√¨nh\")\n",
    "        print(f\"üíª Device: {device}\")\n",
    "        \n",
    "        return model, tokenizer, device\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói load model: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# ==================== GENERATE SUMMARY ====================\n",
    "\n",
    "def generate_summary(content, model, tokenizer, device):\n",
    "    \"\"\"\n",
    "    T·∫°o summary t·ª´ content\n",
    "    \n",
    "    Args:\n",
    "        content: N·ªôi dung b√†i b√°o\n",
    "        model: Model object\n",
    "        tokenizer: Tokenizer object\n",
    "        device: Device (cuda/cpu)\n",
    "    \n",
    "    Returns:\n",
    "        str: Summary ƒë√£ generate\n",
    "    \"\"\"\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(\n",
    "        content,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_length=MAX_LENGTH,\n",
    "            min_length=MIN_LENGTH,\n",
    "            num_beams=NUM_BEAMS,\n",
    "            no_repeat_ngram_size=NO_REPEAT_NGRAM_SIZE,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# ==================== METRICS CALCULATION ====================\n",
    "\n",
    "def calculate_rouge(predictions, references):\n",
    "    \"\"\"\n",
    "    T√≠nh ROUGE scores\n",
    "    \n",
    "    Args:\n",
    "        predictions: List of generated summaries\n",
    "        references: List of reference summaries\n",
    "    \n",
    "    Returns:\n",
    "        Dict ch·ª©a ROUGE scores\n",
    "    \"\"\"\n",
    "    if not ROUGE_AVAILABLE:\n",
    "        return {}\n",
    "    \n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=False)\n",
    "    \n",
    "    rouge1_scores = []\n",
    "    rouge2_scores = []\n",
    "    rougeL_scores = []\n",
    "    \n",
    "    for pred, ref in zip(predictions, references):\n",
    "        scores = scorer.score(ref, pred)\n",
    "        rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "        rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "        rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "    \n",
    "    return {\n",
    "        \"rouge1\": {\n",
    "            \"mean\": np.mean(rouge1_scores),\n",
    "            \"std\": np.std(rouge1_scores),\n",
    "            \"min\": np.min(rouge1_scores),\n",
    "            \"max\": np.max(rouge1_scores)\n",
    "        },\n",
    "        \"rouge2\": {\n",
    "            \"mean\": np.mean(rouge2_scores),\n",
    "            \"std\": np.std(rouge2_scores),\n",
    "            \"min\": np.min(rouge2_scores),\n",
    "            \"max\": np.max(rouge2_scores)\n",
    "        },\n",
    "        \"rougeL\": {\n",
    "            \"mean\": np.mean(rougeL_scores),\n",
    "            \"std\": np.std(rougeL_scores),\n",
    "            \"min\": np.min(rougeL_scores),\n",
    "            \"max\": np.max(rougeL_scores)\n",
    "        }\n",
    "    }\n",
    "\n",
    "def calculate_bleu(predictions, references):\n",
    "    \"\"\"\n",
    "    T√≠nh BLEU scores\n",
    "    \n",
    "    Args:\n",
    "        predictions: List of generated summaries\n",
    "        references: List of reference summaries\n",
    "    \n",
    "    Returns:\n",
    "        Dict ch·ª©a BLEU scores\n",
    "    \"\"\"\n",
    "    if not BLEU_AVAILABLE:\n",
    "        return {}\n",
    "    \n",
    "    smoothing = SmoothingFunction().method1\n",
    "    bleu_scores = []\n",
    "    \n",
    "    for pred, ref in zip(predictions, references):\n",
    "        pred_tokens = pred.split()\n",
    "        ref_tokens = [ref.split()]  # BLEU expects list of references\n",
    "        \n",
    "        try:\n",
    "            score = sentence_bleu(ref_tokens, pred_tokens, smoothing_function=smoothing)\n",
    "            bleu_scores.append(score)\n",
    "        except:\n",
    "            bleu_scores.append(0.0)\n",
    "    \n",
    "    return {\n",
    "        \"bleu\": {\n",
    "            \"mean\": np.mean(bleu_scores),\n",
    "            \"std\": np.std(bleu_scores),\n",
    "            \"min\": np.min(bleu_scores),\n",
    "            \"max\": np.max(bleu_scores)\n",
    "        }\n",
    "    }\n",
    "\n",
    "def calculate_length_stats(predictions, references):\n",
    "    \"\"\"\n",
    "    Th·ªëng k√™ ƒë·ªô d√†i summaries\n",
    "    \n",
    "    Args:\n",
    "        predictions: List of generated summaries\n",
    "        references: List of reference summaries\n",
    "    \n",
    "    Returns:\n",
    "        Dict ch·ª©a th·ªëng k√™ ƒë·ªô d√†i\n",
    "    \"\"\"\n",
    "    pred_lengths = [len(p.split()) for p in predictions]\n",
    "    ref_lengths = [len(r.split()) for r in references]\n",
    "    \n",
    "    return {\n",
    "        \"prediction_length\": {\n",
    "            \"mean\": np.mean(pred_lengths),\n",
    "            \"std\": np.std(pred_lengths),\n",
    "            \"min\": np.min(pred_lengths),\n",
    "            \"max\": np.max(pred_lengths)\n",
    "        },\n",
    "        \"reference_length\": {\n",
    "            \"mean\": np.mean(ref_lengths),\n",
    "            \"std\": np.std(ref_lengths)\n",
    "        }\n",
    "    }\n",
    "\n",
    "# ==================== OVERFITTING CHECK ====================\n",
    "\n",
    "def check_overfitting():\n",
    "    \"\"\"\n",
    "    Ki·ªÉm tra overfitting b·∫±ng c√°ch so s√°nh train v√† val loss\n",
    "    \n",
    "    Returns:\n",
    "        Dict ch·ª©a ph√¢n t√≠ch overfitting\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(f\"{MODEL_DIR}/train_metrics.json\", \"r\") as f:\n",
    "            train_metrics = json.load(f)\n",
    "        \n",
    "        # L·∫•y train v√† eval loss t·ª´ trainer_state.json\n",
    "        with open(f\"{MODEL_DIR}/trainer_state.json\", \"r\") as f:\n",
    "            trainer_state = json.load(f)\n",
    "        \n",
    "        log_history = trainer_state.get(\"log_history\", [])\n",
    "        \n",
    "        train_losses = []\n",
    "        eval_losses = []\n",
    "        \n",
    "        for log in log_history:\n",
    "            if \"loss\" in log:\n",
    "                train_losses.append(log[\"loss\"])\n",
    "            if \"eval_loss\" in log:\n",
    "                eval_losses.append(log[\"eval_loss\"])\n",
    "        \n",
    "        # Ph√¢n t√≠ch overfitting\n",
    "        if len(train_losses) > 0 and len(eval_losses) > 0:\n",
    "            final_train_loss = train_losses[-1]\n",
    "            final_eval_loss = eval_losses[-1]\n",
    "            \n",
    "            gap = final_eval_loss - final_train_loss\n",
    "            gap_percent = (gap / final_train_loss) * 100\n",
    "            \n",
    "            if gap_percent > 20:\n",
    "                status = \"HIGH_OVERFITTING\"\n",
    "                message = \"M√¥ h√¨nh b·ªã overfit nghi√™m tr·ªçng. C·∫ßn gi·∫£m model complexity ho·∫∑c tƒÉng data.\"\n",
    "            elif gap_percent > 10:\n",
    "                status = \"MODERATE_OVERFITTING\"\n",
    "                message = \"M√¥ h√¨nh c√≥ d·∫•u hi·ªáu overfit nh·∫π. C√≥ th·ªÉ ch·∫•p nh·∫≠n ƒë∆∞·ª£c.\"\n",
    "            else:\n",
    "                status = \"GOOD\"\n",
    "                message = \"M√¥ h√¨nh h·ªçc t·ªët, kh√¥ng b·ªã overfit.\"\n",
    "            \n",
    "            return {\n",
    "                \"status\": status,\n",
    "                \"message\": message,\n",
    "                \"final_train_loss\": final_train_loss,\n",
    "                \"final_eval_loss\": final_eval_loss,\n",
    "                \"gap\": gap,\n",
    "                \"gap_percent\": gap_percent,\n",
    "                \"train_losses\": train_losses,\n",
    "                \"eval_losses\": eval_losses\n",
    "            }\n",
    "        \n",
    "        return {\"status\": \"UNKNOWN\", \"message\": \"Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ ph√¢n t√≠ch\"}\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\"status\": \"ERROR\", \"message\": str(e)}\n",
    "\n",
    "# ==================== MAIN EVALUATION ====================\n",
    "\n",
    "def run_evaluation():\n",
    "    \"\"\"\n",
    "    Ch·∫°y evaluation tr√™n test set\n",
    "    \n",
    "    Returns:\n",
    "        Dict ch·ª©a k·∫øt qu·∫£ ƒë√°nh gi√°\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìä GIAI ƒêO·∫†N 5: ƒê√ÅNH GI√Å M√î H√åNH\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # B∆∞·ªõc 1: Load model\n",
    "    model, tokenizer, device = load_model()\n",
    "    if model is None:\n",
    "        print(\"‚ùå Kh√¥ng th·ªÉ load model. H√£y ch·∫°y 4_model_training.py tr∆∞·ªõc.\")\n",
    "        return None\n",
    "    \n",
    "    # B∆∞·ªõc 2: Load test data\n",
    "    print(f\"\\nüì• ƒêang load test data t·ª´ {TEST_DATA_PATH}...\")\n",
    "    try:\n",
    "        with open(TEST_DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "            test_data = json.load(f)\n",
    "        print(f\"‚úÖ ƒê√£ load {len(test_data)} b√†i test\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói load test data: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # B∆∞·ªõc 3: Generate summaries\n",
    "    print(f\"\\nü§ñ ƒêang generate summaries...\")\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    for idx, item in enumerate(test_data, 1):\n",
    "        print(f\"\\r[{idx}/{len(test_data)}] ƒêang generate...\", end=\"\", flush=True)\n",
    "        \n",
    "        content = item[\"content\"]\n",
    "        reference = item[\"summary\"]\n",
    "        \n",
    "        # Generate summary\n",
    "        prediction = generate_summary(content, model, tokenizer, device)\n",
    "        \n",
    "        predictions.append(prediction)\n",
    "        references.append(reference)\n",
    "    \n",
    "    print()  # Xu·ªëng d√≤ng sau progress bar\n",
    "    \n",
    "    # B∆∞·ªõc 4: T√≠nh metrics\n",
    "    print(f\"\\nüìè ƒêang t√≠nh metrics...\")\n",
    "    \n",
    "    rouge_scores = calculate_rouge(predictions, references)\n",
    "    bleu_scores = calculate_bleu(predictions, references)\n",
    "    length_stats = calculate_length_stats(predictions, references)\n",
    "    \n",
    "    # B∆∞·ªõc 5: Ki·ªÉm tra overfitting\n",
    "    print(f\"\\nüîç ƒêang ki·ªÉm tra overfitting...\")\n",
    "    overfitting_analysis = check_overfitting()\n",
    "    \n",
    "    # B∆∞·ªõc 6: T·ªïng h·ª£p k·∫øt qu·∫£\n",
    "    results = {\n",
    "        \"test_size\": len(test_data),\n",
    "        \"rouge_scores\": rouge_scores,\n",
    "        \"bleu_scores\": bleu_scores,\n",
    "        \"length_stats\": length_stats,\n",
    "        \"overfitting_analysis\": overfitting_analysis,\n",
    "        \"evaluated_at\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    # L∆∞u k·∫øt qu·∫£\n",
    "    results_path = f\"{MODEL_DIR}/evaluation_results.json\"\n",
    "    with open(results_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # L∆∞u v√≠ d·ª• predictions\n",
    "    examples_path = f\"{MODEL_DIR}/evaluation_examples.json\"\n",
    "    examples = []\n",
    "    for i in range(min(10, len(test_data))):  # L∆∞u 10 v√≠ d·ª•\n",
    "        examples.append({\n",
    "            \"content\": test_data[i][\"content\"][:200] + \"...\",  # Ch·ªâ l∆∞u 200 k√Ω t·ª± ƒë·∫ßu\n",
    "            \"reference\": references[i],\n",
    "            \"prediction\": predictions[i],\n",
    "            \"url\": test_data[i].get(\"url\", \"\")\n",
    "        })\n",
    "    \n",
    "    with open(examples_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(examples, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # In b√°o c√°o\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"‚úÖ HO√ÄN TH√ÄNH ƒê√ÅNH GI√Å\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    print(f\"\\nüìä K·∫æT QU·∫¢ ƒê√ÅNH GI√Å:\")\n",
    "    print(f\"\\nüéØ ROUGE Scores:\")\n",
    "    if rouge_scores:\n",
    "        for metric, values in rouge_scores.items():\n",
    "            print(f\"   ‚Ä¢ {metric.upper()}: {values['mean']:.4f} ¬± {values['std']:.4f}\")\n",
    "            print(f\"     (min: {values['min']:.4f}, max: {values['max']:.4f})\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  Kh√¥ng t√≠nh ƒë∆∞·ª£c ROUGE scores\")\n",
    "    \n",
    "    print(f\"\\nüìù BLEU Scores:\")\n",
    "    if bleu_scores:\n",
    "        print(f\"   ‚Ä¢ BLEU: {bleu_scores['bleu']['mean']:.4f} ¬± {bleu_scores['bleu']['std']:.4f}\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  Kh√¥ng t√≠nh ƒë∆∞·ª£c BLEU scores\")\n",
    "    \n",
    "    print(f\"\\nüìè ƒê·ªô d√†i Summary:\")\n",
    "    print(f\"   ‚Ä¢ Generated: {length_stats['prediction_length']['mean']:.1f} t·ª´ \"\n",
    "          f\"(min: {length_stats['prediction_length']['min']}, \"\n",
    "          f\"max: {length_stats['prediction_length']['max']})\")\n",
    "    print(f\"   ‚Ä¢ Reference: {length_stats['reference_length']['mean']:.1f} t·ª´\")\n",
    "    \n",
    "    print(f\"\\nüîç Ph√¢n t√≠ch Overfitting:\")\n",
    "    print(f\"   ‚Ä¢ Status: {overfitting_analysis.get('status', 'UNKNOWN')}\")\n",
    "    print(f\"   ‚Ä¢ {overfitting_analysis.get('message', 'N/A')}\")\n",
    "    if 'gap_percent' in overfitting_analysis:\n",
    "        print(f\"   ‚Ä¢ Train/Val gap: {overfitting_analysis['gap_percent']:.2f}%\")\n",
    "    \n",
    "    # ƒê√°nh gi√° t·ªïng th·ªÉ\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üí° ƒê√ÅNH GI√Å T·ªîNG QUAN:\")\n",
    "    \n",
    "    if rouge_scores:\n",
    "        rougeL_mean = rouge_scores['rougeL']['mean']\n",
    "        \n",
    "        if rougeL_mean >= 0.4:\n",
    "            quality = \"T·ªêT\"\n",
    "            emoji = \"üü¢\"\n",
    "            comment = \"M√¥ h√¨nh ho·∫°t ƒë·ªông t·ªët!\"\n",
    "        elif rougeL_mean >= 0.3:\n",
    "            quality = \"KH√Å\"\n",
    "            emoji = \"üü°\"\n",
    "            comment = \"M√¥ h√¨nh ho·∫°t ƒë·ªông ·ªïn. C√≥ th·ªÉ c·∫£i thi·ªán th√™m.\"\n",
    "        else:\n",
    "            quality = \"C·∫¶N C·∫¢I THI·ªÜN\"\n",
    "            emoji = \"üî¥\"\n",
    "            comment = \"M√¥ h√¨nh c·∫ßn fine-tune th√™m ho·∫∑c tƒÉng data.\"\n",
    "        \n",
    "        print(f\"{emoji} Ch·∫•t l∆∞·ª£ng: {quality}\")\n",
    "        print(f\"   {comment}\")\n",
    "    \n",
    "    print(f\"\\nüìÅ K·∫øt qu·∫£ ƒë√£ l∆∞u:\")\n",
    "    print(f\"   ‚Ä¢ Chi ti·∫øt: {results_path}\")\n",
    "    print(f\"   ‚Ä¢ V√≠ d·ª•: {examples_path}\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"‚û°Ô∏è  Ti·∫øp theo: Ch·∫°y 6_generate_summaries.py ƒë·ªÉ t√≥m t·∫Øt to√†n b·ªô d·ªØ li·ªáu\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ==================== V·∫º BI·ªÇU ƒê·ªí ====================\n",
    "\n",
    "def plot_training_curves():\n",
    "    \"\"\"\n",
    "    V·∫Ω bi·ªÉu ƒë·ªì train/val loss theo epoch\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìà ƒêang v·∫Ω bi·ªÉu ƒë·ªì training curves...\")\n",
    "    \n",
    "    try:\n",
    "        overfitting_analysis = check_overfitting()\n",
    "        \n",
    "        if overfitting_analysis['status'] not in ['UNKNOWN', 'ERROR']:\n",
    "            train_losses = overfitting_analysis['train_losses']\n",
    "            eval_losses = overfitting_analysis['eval_losses']\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            \n",
    "            # Train loss\n",
    "            plt.plot(range(1, len(train_losses) + 1), train_losses, \n",
    "                    label='Train Loss', marker='o', linewidth=2)\n",
    "            \n",
    "            # Eval loss\n",
    "            eval_epochs = np.linspace(1, len(train_losses), len(eval_losses))\n",
    "            plt.plot(eval_epochs, eval_losses, \n",
    "                    label='Validation Loss', marker='s', linewidth=2)\n",
    "            \n",
    "            plt.xlabel('Epoch', fontsize=12)\n",
    "            plt.ylabel('Loss', fontsize=12)\n",
    "            plt.title('Training v√† Validation Loss theo Epoch', fontsize=14, fontweight='bold')\n",
    "            plt.legend(fontsize=11)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # L∆∞u bi·ªÉu ƒë·ªì\n",
    "            plot_path = f\"{MODEL_DIR}/training_curves.png\"\n",
    "            plt.savefig(plot_path, dpi=300)\n",
    "            print(f\"‚úÖ ƒê√£ l∆∞u bi·ªÉu ƒë·ªì: {plot_path}\")\n",
    "            \n",
    "            plt.close()\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói v·∫Ω bi·ªÉu ƒë·ªì: {e}\")\n",
    "\n",
    "# ==================== CH·∫†Y CH∆Ø∆†NG TR√åNH ====================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    \n",
    "    # Ch·∫°y evaluation\n",
    "    results = run_evaluation()\n",
    "    \n",
    "    if results:\n",
    "        # V·∫Ω bi·ªÉu ƒë·ªì (optional)\n",
    "        try:\n",
    "            plot_training_curves()\n",
    "        except:\n",
    "            print(\"‚ö†Ô∏è  Kh√¥ng th·ªÉ v·∫Ω bi·ªÉu ƒë·ªì (c·∫ßn matplotlib)\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ Evaluation ho√†n t·∫•t!\")\n",
    "        print(f\"\\nüí° ƒê·ªÉ xem chi ti·∫øt:\")\n",
    "        print(f\"   cat {MODEL_DIR}/evaluation_results.json\")\n",
    "        print(f\"   cat {MODEL_DIR}/evaluation_examples.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea503a64",
   "metadata": {},
   "source": [
    "GIAI ƒêO·∫†N 6: T·∫†O T√ìM T·∫ÆT CHO TO√ÄN B·ªò D·ªÆ LI·ªÜU (GENERATE SUMMARIES)\n",
    "===================================================================\n",
    "M·ª•c ti√™u:\n",
    "- S·ª≠ d·ª•ng m√¥ h√¨nh ƒë√£ fine-tune ƒë·ªÉ t√≥m t·∫Øt t·∫•t c·∫£ b√†i trong clean_data\n",
    "- Th√™m ph√¢n t√≠ch ng·ªØ c·∫£nh giao th√¥ng (keywords extraction)\n",
    "- L∆∞u v√†o collection \"final_output\" v·ªõi tr∆∞·ªùng \"summary\" v√† \"analysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10064dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GIAI ƒêO·∫†N 6: T·∫†O T√ìM T·∫ÆT CHO TO√ÄN B·ªò D·ªÆ LI·ªÜU (GENERATE SUMMARIES)\n",
    "===================================================================\n",
    "M·ª•c ti√™u:\n",
    "- S·ª≠ d·ª•ng m√¥ h√¨nh ƒë√£ fine-tune ƒë·ªÉ t√≥m t·∫Øt t·∫•t c·∫£ b√†i trong clean_data\n",
    "- Th√™m ph√¢n t√≠ch ng·ªØ c·∫£nh giao th√¥ng (keywords extraction)\n",
    "- L∆∞u v√†o collection \"final_output\" v·ªõi tr∆∞·ªùng \"summary\" v√† \"analysis\"\n",
    "\"\"\"\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# ==================== C·∫§U H√åNH ====================\n",
    "\n",
    "# MongoDB\n",
    "MONGODB_URI = \"mongodb://localhost:27017/\"\n",
    "DATABASE_NAME = \"hcmc_traffic_summarization\"\n",
    "CLEAN_COLLECTION = \"clean_data\"\n",
    "FINAL_COLLECTION = \"final_output\"  # Collection ch·ª©a k·∫øt qu·∫£ cu·ªëi c√πng\n",
    "\n",
    "# Model\n",
    "MODEL_DIR = \"./models/vit5_hcmc_traffic\"\n",
    "\n",
    "# Generation parameters\n",
    "MAX_LENGTH = 128\n",
    "MIN_LENGTH = 30\n",
    "NUM_BEAMS = 4\n",
    "BATCH_SIZE = 8  # X·ª≠ l√Ω theo batch ƒë·ªÉ nhanh h∆°n\n",
    "\n",
    "# Keywords giao th√¥ng ƒë·ªÉ ph√¢n t√≠ch\n",
    "TRAFFIC_ENTITIES = {\n",
    "    \"infrastructure\": [\n",
    "        \"metro\", \"t√†u ƒëi·ªán\", \"cao t·ªëc\", \"c·∫ßu\", \"ƒë∆∞·ªùng v√†nh ƒëai\", \n",
    "        \"brt\", \"ƒë∆∞·ªùng s·∫Øt\", \"n√∫t giao th√¥ng\", \"c·∫£ng\", \"s√¢n bay\"\n",
    "    ],\n",
    "    \"locations\": [\n",
    "        \"qu·∫≠n 1\", \"qu·∫≠n 2\", \"qu·∫≠n 3\", \"th·ªß ƒë·ª©c\", \"b√¨nh th·∫°nh\",\n",
    "        \"t√¢n b√¨nh\", \"g√≤ v·∫•p\", \"b√¨nh t√¢n\", \"c·∫ßu s√†i g√≤n\"\n",
    "    ],\n",
    "    \"issues\": [\n",
    "        \"k·∫πt xe\", \"√πn t·∫Øc\", \"tai n·∫°n\", \"ng·∫≠p n∆∞·ªõc\", \"·ªï g√†\", \"h∆∞ h·ªèng\"\n",
    "    ],\n",
    "    \"projects\": [\n",
    "        \"d·ª± √°n\", \"kh·ªüi c√¥ng\", \"ho√†n th√†nh\", \"tri·ªÉn khai\", \"ƒë·∫ßu t∆∞\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# ==================== K·∫æT N·ªêI DATABASE ====================\n",
    "\n",
    "def connect_mongodb():\n",
    "    \"\"\"\n",
    "    K·∫øt n·ªëi MongoDB v√† t·∫°o collection final_output\n",
    "    \n",
    "    Returns:\n",
    "        Tuple (clean_collection, final_collection)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = MongoClient(MONGODB_URI, serverSelectionTimeoutMS=5000)\n",
    "        client.server_info()\n",
    "        \n",
    "        db = client[DATABASE_NAME]\n",
    "        clean_col = db[CLEAN_COLLECTION]\n",
    "        final_col = db[FINAL_COLLECTION]\n",
    "        \n",
    "        # T·∫°o index\n",
    "        final_col.create_index([(\"url\", 1)], unique=True)\n",
    "        final_col.create_index([(\"published_date\", -1)])\n",
    "        final_col.create_index([(\"category\", 1)])\n",
    "        \n",
    "        # Full-text search index\n",
    "        final_col.create_index([\n",
    "            (\"title_clean\", \"text\"),\n",
    "            (\"summary\", \"text\"),\n",
    "            (\"analysis\", \"text\")\n",
    "        ])\n",
    "        \n",
    "        print(f\"‚úÖ ƒê√£ k·∫øt n·ªëi MongoDB\")\n",
    "        print(f\"   Clean data: {clean_col.count_documents({})}\")\n",
    "        return clean_col, final_col\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói k·∫øt n·ªëi MongoDB: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# ==================== LOAD MODEL ====================\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"\n",
    "    Load m√¥ h√¨nh ƒë√£ fine-tune\n",
    "    \n",
    "    Returns:\n",
    "        Tuple (model, tokenizer, device)\n",
    "    \"\"\"\n",
    "    print(f\"\\nüì¶ ƒêang load m√¥ h√¨nh t·ª´ {MODEL_DIR}...\")\n",
    "    \n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_DIR)\n",
    "        \n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        print(f\"‚úÖ ƒê√£ load m√¥ h√¨nh\")\n",
    "        print(f\"üíª Device: {device}\")\n",
    "        \n",
    "        return model, tokenizer, device\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói load model: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# ==================== TEXT ANALYSIS ====================\n",
    "\n",
    "def extract_keywords(text, top_n=10):\n",
    "    \"\"\"\n",
    "    Tr√≠ch xu·∫•t keywords t·ª´ vƒÉn b·∫£n\n",
    "    \n",
    "    Args:\n",
    "        text: VƒÉn b·∫£n c·∫ßn tr√≠ch xu·∫•t\n",
    "        top_n: S·ªë l∆∞·ª£ng keywords\n",
    "    \n",
    "    Returns:\n",
    "        List of keywords\n",
    "    \"\"\"\n",
    "    # T√°ch t·ª´\n",
    "    words = re.findall(r'\\w+', text.lower())\n",
    "    \n",
    "    # Stopwords ti·∫øng Vi·ªát\n",
    "    stopwords = {\n",
    "        'v√†', 'c·ªßa', 'c√≥', 'ƒë∆∞·ª£c', 'l√†', 'trong', 'v·ªõi', 'cho', 'c√°c',\n",
    "        'm·ªôt', 'n√†y', 'ƒë√£', 't·ª´', 'nh·ªØng', 'ƒë·ªÉ', 'ng∆∞·ªùi', 'kh√¥ng', 'nh∆∞',\n",
    "        'v·ªÅ', 'theo', 'nƒÉm', 't·∫°i', 'ƒë·∫øn', 'khi', 'ng√†y', 'tr√™n', 'sau',\n",
    "        'v√†o', 'th√¨', 's·∫Ω', 'ra', 'ƒëang', 'n√™n', 'b·ªã', 'hay', 'nh∆∞ng'\n",
    "    }\n",
    "    \n",
    "    # L·ªçc stopwords v√† t·ª´ ng·∫Øn\n",
    "    words = [w for w in words if w not in stopwords and len(w) > 3]\n",
    "    \n",
    "    # ƒê·∫øm t·∫ßn su·∫•t\n",
    "    word_freq = Counter(words)\n",
    "    \n",
    "    # L·∫•y top keywords\n",
    "    keywords = [word for word, _ in word_freq.most_common(top_n)]\n",
    "    \n",
    "    return keywords\n",
    "\n",
    "def analyze_traffic_context(text):\n",
    "    \"\"\"\n",
    "    Ph√¢n t√≠ch ng·ªØ c·∫£nh giao th√¥ng trong b√†i vi·∫øt\n",
    "    \n",
    "    Args:\n",
    "        text: VƒÉn b·∫£n c·∫ßn ph√¢n t√≠ch\n",
    "    \n",
    "    Returns:\n",
    "        Dict ch·ª©a ph√¢n t√≠ch\n",
    "    \"\"\"\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    analysis = {\n",
    "        \"mentioned_infrastructure\": [],\n",
    "        \"mentioned_locations\": [],\n",
    "        \"mentioned_issues\": [],\n",
    "        \"mentioned_projects\": [],\n",
    "        \"main_topic\": \"\"\n",
    "    }\n",
    "    \n",
    "    # Ph√°t hi·ªán c√°c entities giao th√¥ng\n",
    "    for category, keywords in TRAFFIC_ENTITIES.items():\n",
    "        mentioned = [kw for kw in keywords if kw in text_lower]\n",
    "        \n",
    "        if category == \"infrastructure\":\n",
    "            analysis[\"mentioned_infrastructure\"] = mentioned\n",
    "        elif category == \"locations\":\n",
    "            analysis[\"mentioned_locations\"] = mentioned\n",
    "        elif category == \"issues\":\n",
    "            analysis[\"mentioned_issues\"] = mentioned\n",
    "        elif category == \"projects\":\n",
    "            analysis[\"mentioned_projects\"] = mentioned\n",
    "    \n",
    "    # X√°c ƒë·ªãnh ch·ªß ƒë·ªÅ ch√≠nh\n",
    "    if analysis[\"mentioned_projects\"]:\n",
    "        analysis[\"main_topic\"] = \"D·ª± √°n giao th√¥ng\"\n",
    "    elif analysis[\"mentioned_issues\"]:\n",
    "        analysis[\"main_topic\"] = \"V·∫•n ƒë·ªÅ giao th√¥ng\"\n",
    "    elif analysis[\"mentioned_infrastructure\"]:\n",
    "        analysis[\"main_topic\"] = \"H·∫° t·∫ßng giao th√¥ng\"\n",
    "    else:\n",
    "        analysis[\"main_topic\"] = \"Giao th√¥ng chung\"\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "def generate_analysis_text(analysis, keywords):\n",
    "    \"\"\"\n",
    "    T·∫°o vƒÉn b·∫£n ph√¢n t√≠ch t·ª´ d·ªØ li·ªáu analysis\n",
    "    \n",
    "    Args:\n",
    "        analysis: Dict t·ª´ analyze_traffic_context\n",
    "        keywords: List of keywords\n",
    "    \n",
    "    Returns:\n",
    "        str: VƒÉn b·∫£n ph√¢n t√≠ch\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    \n",
    "    # Ch·ªß ƒë·ªÅ ch√≠nh\n",
    "    parts.append(f\"Ch·ªß ƒë·ªÅ: {analysis['main_topic']}.\")\n",
    "    \n",
    "    # H·∫° t·∫ßng ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p\n",
    "    if analysis['mentioned_infrastructure']:\n",
    "        infra = \", \".join(analysis['mentioned_infrastructure'][:3])\n",
    "        parts.append(f\"H·∫° t·∫ßng: {infra}.\")\n",
    "    \n",
    "    # ƒê·ªãa ƒëi·ªÉm\n",
    "    if analysis['mentioned_locations']:\n",
    "        locs = \", \".join(analysis['mentioned_locations'][:3])\n",
    "        parts.append(f\"Khu v·ª±c: {locs}.\")\n",
    "    \n",
    "    # V·∫•n ƒë·ªÅ\n",
    "    if analysis['mentioned_issues']:\n",
    "        issues = \", \".join(analysis['mentioned_issues'][:3])\n",
    "        parts.append(f\"V·∫•n ƒë·ªÅ: {issues}.\")\n",
    "    \n",
    "    # Keywords\n",
    "    if keywords:\n",
    "        kw = \", \".join(keywords[:5])\n",
    "        parts.append(f\"T·ª´ kh√≥a: {kw}.\")\n",
    "    \n",
    "    return \" \".join(parts)\n",
    "\n",
    "# ==================== GENERATE SUMMARY ====================\n",
    "\n",
    "def generate_summary(content, model, tokenizer, device):\n",
    "    \"\"\"\n",
    "    T·∫°o summary t·ª´ content\n",
    "    \n",
    "    Args:\n",
    "        content: N·ªôi dung b√†i b√°o\n",
    "        model: Model object\n",
    "        tokenizer: Tokenizer object\n",
    "        device: Device (cuda/cpu)\n",
    "    \n",
    "    Returns:\n",
    "        str: Summary ƒë√£ generate\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        content,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_length=MAX_LENGTH,\n",
    "            min_length=MIN_LENGTH,\n",
    "            num_beams=NUM_BEAMS,\n",
    "            no_repeat_ngram_size=3,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# ==================== MAIN PIPELINE ====================\n",
    "\n",
    "def process_article(clean_doc, model, tokenizer, device):\n",
    "    \"\"\"\n",
    "    X·ª≠ l√Ω 1 b√†i vi·∫øt: t√≥m t·∫Øt + ph√¢n t√≠ch\n",
    "    \n",
    "    Args:\n",
    "        clean_doc: Document t·ª´ clean_data\n",
    "        model, tokenizer, device: Model components\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Document cho final_output\n",
    "    \"\"\"\n",
    "    content = clean_doc.get(\"content_clean\", \"\")\n",
    "    title = clean_doc.get(\"title_clean\", \"\")\n",
    "    \n",
    "    # B∆∞·ªõc 1: Generate summary b·∫±ng model\n",
    "    summary = generate_summary(content, model, tokenizer, device)\n",
    "    \n",
    "    # B∆∞·ªõc 2: Tr√≠ch xu·∫•t keywords\n",
    "    keywords = extract_keywords(content, top_n=10)\n",
    "    \n",
    "    # B∆∞·ªõc 3: Ph√¢n t√≠ch ng·ªØ c·∫£nh giao th√¥ng\n",
    "    analysis_data = analyze_traffic_context(content)\n",
    "    \n",
    "    # B∆∞·ªõc 4: T·∫°o vƒÉn b·∫£n ph√¢n t√≠ch\n",
    "    analysis_text = generate_analysis_text(analysis_data, keywords)\n",
    "    \n",
    "    # T·∫°o final document\n",
    "    final_doc = {\n",
    "        \"url\": clean_doc.get(\"url\"),\n",
    "        \"source\": clean_doc.get(\"source\"),\n",
    "        \"source_name\": clean_doc.get(\"source_name\"),\n",
    "        \n",
    "        # D·ªØ li·ªáu g·ªëc\n",
    "        \"title_clean\": title,\n",
    "        \"content_clean\": content,  # Gi·ªØ l·∫°i content g·ªëc\n",
    "        \n",
    "        # K·∫øt qu·∫£ ch√≠nh: SUMMARY v√† ANALYSIS\n",
    "        \"summary\": summary,  # T√≥m t·∫Øt b·∫±ng AI\n",
    "        \"analysis\": analysis_text,  # Ph√¢n t√≠ch ng·ªØ c·∫£nh\n",
    "        \n",
    "        # Metadata b·ªï sung\n",
    "        \"keywords\": keywords,\n",
    "        \"traffic_entities\": analysis_data,\n",
    "        \n",
    "        # Ph√¢n lo·∫°i\n",
    "        \"published_date\": clean_doc.get(\"published_date\"),\n",
    "        \"category\": clean_doc.get(\"category\"),\n",
    "        \"category_name\": clean_doc.get(\"category_name\"),\n",
    "        \n",
    "        # Th·ªëng k√™\n",
    "        \"num_sentences\": clean_doc.get(\"num_sentences\"),\n",
    "        \"num_words\": clean_doc.get(\"num_words\"),\n",
    "        \"summary_length\": len(summary.split()),\n",
    "        \n",
    "        # Timestamps\n",
    "        \"scraped_at\": clean_doc.get(\"scraped_at\"),\n",
    "        \"cleaned_at\": clean_doc.get(\"cleaned_at\"),\n",
    "        \"summarized_at\": datetime.now(),\n",
    "        \"processing_status\": \"completed\"  # ƒê√°nh d·∫•u ho√†n th√†nh\n",
    "    }\n",
    "    \n",
    "    return final_doc\n",
    "\n",
    "def run_generation(batch_size=BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Ch·∫°y t√≥m t·∫Øt cho to√†n b·ªô clean_data\n",
    "    \n",
    "    Args:\n",
    "        batch_size: S·ªë l∆∞·ª£ng b√†i x·ª≠ l√Ω m·ªói l·∫ßn\n",
    "    \n",
    "    Returns:\n",
    "        Dict ch·ª©a th·ªëng k√™\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üöÄ GIAI ƒêO·∫†N 6: T·∫†O T√ìM T·∫ÆT CHO TO√ÄN B·ªò D·ªÆ LI·ªÜU\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # B∆∞·ªõc 1: K·∫øt n·ªëi MongoDB\n",
    "    clean_col, final_col = connect_mongodb()\n",
    "    if clean_col is None or final_col is None:\n",
    "        print(\"‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi MongoDB\")\n",
    "        return None\n",
    "    \n",
    "    # B∆∞·ªõc 2: Load model\n",
    "    model, tokenizer, device = load_model()\n",
    "    if model is None:\n",
    "        print(\"‚ùå Kh√¥ng th·ªÉ load model. H√£y ch·∫°y 4_model_training.py tr∆∞·ªõc.\")\n",
    "        return None\n",
    "    \n",
    "    # B∆∞·ªõc 3: L·∫•y d·ªØ li·ªáu c·∫ßn x·ª≠ l√Ω\n",
    "    total_clean = clean_col.count_documents({\"processing_status\": \"cleaned\"})\n",
    "    print(f\"\\nüìä T·ªïng s·ªë b√†i c·∫ßn t√≥m t·∫Øt: {total_clean}\")\n",
    "    \n",
    "    if total_clean == 0:\n",
    "        print(\"‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ x·ª≠ l√Ω. H√£y ch·∫°y 3_data_cleaning.py tr∆∞·ªõc.\")\n",
    "        return None\n",
    "    \n",
    "    stats = {\n",
    "        \"total_processed\": 0,\n",
    "        \"summarized\": 0,\n",
    "        \"errors\": 0,\n",
    "        \"total_summary_length\": 0\n",
    "    }\n",
    "    \n",
    "    # B∆∞·ªõc 4: X·ª≠ l√Ω t·ª´ng b√†i\n",
    "    cursor = clean_col.find({\"processing_status\": \"cleaned\"})\n",
    "    \n",
    "    for idx, clean_doc in enumerate(cursor, 1):\n",
    "        print(f\"\\r[{idx}/{total_clean}] ƒêang x·ª≠ l√Ω...\", end=\"\", flush=True)\n",
    "        \n",
    "        stats[\"total_processed\"] += 1\n",
    "        \n",
    "        try:\n",
    "            # Ki·ªÉm tra ƒë√£ t·ªìn t·∫°i trong final_output ch∆∞a\n",
    "            url = clean_doc.get(\"url\")\n",
    "            if final_col.find_one({\"url\": url}):\n",
    "                continue  # B·ªè qua n·∫øu ƒë√£ x·ª≠ l√Ω\n",
    "            \n",
    "            # Process: t√≥m t·∫Øt + ph√¢n t√≠ch\n",
    "            final_doc = process_article(clean_doc, model, tokenizer, device)\n",
    "            \n",
    "            # L∆∞u v√†o final_output\n",
    "            final_col.insert_one(final_doc)\n",
    "            \n",
    "            stats[\"summarized\"] += 1\n",
    "            stats[\"total_summary_length\"] += final_doc[\"summary_length\"]\n",
    "            \n",
    "            # C·∫≠p nh·∫≠t tr·∫°ng th√°i trong clean_data\n",
    "            clean_col.update_one(\n",
    "                {\"_id\": clean_doc[\"_id\"]},\n",
    "                {\"$set\": {\"processing_status\": \"summarized\"}}\n",
    "            )\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"\\n  ‚ùå L·ªói x·ª≠ l√Ω: {e}\")\n",
    "            stats[\"errors\"] += 1\n",
    "    \n",
    "    print()  # Xu·ªëng d√≤ng sau progress bar\n",
    "    \n",
    "    # In b√°o c√°o\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"‚úÖ HO√ÄN TH√ÄNH GIAI ƒêO·∫†N 6\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"üìä Th·ªëng k√™:\")\n",
    "    print(f\"   ‚Ä¢ T·ªïng s·ªë ƒë√£ x·ª≠ l√Ω: {stats['total_processed']}\")\n",
    "    print(f\"   ‚Ä¢ ƒê√£ t√≥m t·∫Øt th√†nh c√¥ng: {stats['summarized']}\")\n",
    "    print(f\"   ‚Ä¢ L·ªói: {stats['errors']}\")\n",
    "    \n",
    "    if stats['summarized'] > 0:\n",
    "        avg_summary_length = stats['total_summary_length'] / stats['summarized']\n",
    "        print(f\"   ‚Ä¢ ƒê·ªô d√†i summary trung b√¨nh: {avg_summary_length:.1f} t·ª´\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üéâ HO√ÄN TH√ÄNH T·∫§T C·∫¢ C√ÅC GIAI ƒêO·∫†N!\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\nüìÅ D·ªØ li·ªáu cu·ªëi c√πng ƒë√£ l∆∞u trong collection: '{FINAL_COLLECTION}'\")\n",
    "    print(f\"\\nüí° C·∫•u tr√∫c document trong final_output:\")\n",
    "    print(f\"   ‚Ä¢ title_clean: Ti√™u ƒë·ªÅ ƒë√£ l√†m s·∫°ch\")\n",
    "    print(f\"   ‚Ä¢ content_clean: N·ªôi dung ƒë·∫ßy ƒë·ªß ƒë√£ l√†m s·∫°ch\")\n",
    "    print(f\"   ‚Ä¢ summary: T√≥m t·∫Øt b·∫±ng AI (ViT5)\")\n",
    "    print(f\"   ‚Ä¢ analysis: Ph√¢n t√≠ch ng·ªØ c·∫£nh giao th√¥ng\")\n",
    "    print(f\"   ‚Ä¢ keywords: T·ª´ kh√≥a ch√≠nh\")\n",
    "    print(f\"   ‚Ä¢ traffic_entities: C√°c th·ª±c th·ªÉ giao th√¥ng ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p\")\n",
    "    print(f\"   ‚Ä¢ category: Danh m·ª•c b√†i vi·∫øt\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\\n\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# ==================== CH·∫†Y CH∆Ø∆†NG TR√åNH ====================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    \n",
    "    print(f\"\\nüìù Giai ƒëo·∫°n n√†y s·∫Ω:\")\n",
    "    print(f\"   1. Load m√¥ h√¨nh ViT5 ƒë√£ fine-tune\")\n",
    "    print(f\"   2. T√≥m t·∫Øt t·ª´ng b√†i b√°o trong clean_data\")\n",
    "    print(f\"   3. Ph√¢n t√≠ch ng·ªØ c·∫£nh giao th√¥ng\")\n",
    "    print(f\"   4. L∆∞u k·∫øt qu·∫£ v√†o final_output collection\")\n",
    "    print(f\"\\n‚è±Ô∏è  Th·ªùi gian d·ª± ki·∫øn: 5-30 ph√∫t t√πy s·ªë l∆∞·ª£ng b√†i\\n\")\n",
    "    \n",
    "    # Ch·∫°y generation\n",
    "    results = run_generation()\n",
    "    \n",
    "    if results:\n",
    "        print(f\"\\n‚úÖ T√≥m t·∫Øt ho√†n t·∫•t!\")\n",
    "        print(f\"üìä ƒê√£ x·ª≠ l√Ω: {results['summarized']} b√†i\")\n",
    "        print(f\"\\nüí° ƒê·ªÉ xem k·∫øt qu·∫£, b·∫°n c√≥ th·ªÉ:\")\n",
    "        print(f\"   ‚Ä¢ K·∫øt n·ªëi MongoDB v√† query collection '{FINAL_COLLECTION}'\")\n",
    "        print(f\"   ‚Ä¢ Ch·∫°y API server ƒë·ªÉ truy v·∫•n qua HTTP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77dbcb0",
   "metadata": {},
   "source": [
    "# H·ªÜ TH·ªêNG T√ìM T·∫ÆT B√ÅO GIAO TH√îNG TP.HCM 2025\n",
    "\n",
    "## üìã T·ªïng quan\n",
    "\n",
    "H·ªá th·ªëng t·ª± ƒë·ªông thu th·∫≠p, l√†m s·∫°ch, v√† t√≥m t·∫Øt tin t·ª©c giao th√¥ng TP.HCM nƒÉm 2025 s·ª≠ d·ª•ng m√¥ h√¨nh **ViT5** (Vietnamese T5).\n",
    "\n",
    "### üéØ M·ª•c ti√™u\n",
    "- Thu th·∫≠p d·ªØ li·ªáu t·ª´ nhi·ªÅu ngu·ªìn b√°o ti·∫øng Vi·ªát\n",
    "- L·ªçc ch·ªâ gi·ªØ b√†i v·ªÅ giao th√¥ng TP.HCM nƒÉm 2025\n",
    "- L√†m s·∫°ch v√† chu·∫©n h√≥a d·ªØ li·ªáu\n",
    "- Fine-tune m√¥ h√¨nh ViT5 cho t√≥m t·∫Øt ti·∫øng Vi·ªát\n",
    "- T·∫°o summary v√† ph√¢n t√≠ch ng·ªØ c·∫£nh giao th√¥ng\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è Ki·∫øn tr√∫c h·ªá th·ªëng\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ 1. DATA COLLECTION (1_data_collection.py)                  ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ ‚Ä¢ Scrape t·ª´ RSS feeds + pagination                          ‚îÇ\n",
    "‚îÇ ‚Ä¢ L∆∞u v√†o MongoDB: raw_data (d·ªØ li·ªáu th√¥)                  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                            ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ 2. DATA FILTERING (2_data_filtering.py)                    ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ ‚Ä¢ L·ªçc: ch·ªâ gi·ªØ b√†i giao th√¥ng TP.HCM 2025                  ‚îÇ\n",
    "‚îÇ ‚Ä¢ G√°n nh√£n category t·ª± ƒë·ªông                                 ‚îÇ\n",
    "‚îÇ ‚Ä¢ L∆∞u v√†o MongoDB: filtered_data                            ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                            ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ 3. DATA CLEANING (3_data_cleaning.py)                      ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ ‚Ä¢ Lo·∫°i HTML, k√Ω t·ª± ƒë·∫∑c bi·ªát, spam                          ‚îÇ\n",
    "‚îÇ ‚Ä¢ Chu·∫©n h√≥a unicode, d·∫•u c√¢u, kho·∫£ng tr·∫Øng                 ‚îÇ\n",
    "‚îÇ ‚Ä¢ T√°ch c√¢u (sentence segmentation)                          ‚îÇ\n",
    "‚îÇ ‚Ä¢ L∆∞u v√†o MongoDB: clean_data                               ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                            ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ 4. MODEL TRAINING (4_model_training.py)                    ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ ‚Ä¢ Load ViT5 pre-trained                                     ‚îÇ\n",
    "‚îÇ ‚Ä¢ Fine-tune tr√™n data giao th√¥ng TP.HCM                    ‚îÇ\n",
    "‚îÇ ‚Ä¢ Split: 70% train, 15% val, 15% test                      ‚îÇ\n",
    "‚îÇ ‚Ä¢ L∆∞u model: ./models/vit5_hcmc_traffic                    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                            ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ 5. MODEL EVALUATION (5_model_evaluation.py)                ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ ‚Ä¢ ƒê√°nh gi√° tr√™n test set                                    ‚îÇ\n",
    "‚îÇ ‚Ä¢ Metrics: ROUGE-1, ROUGE-2, ROUGE-L, BLEU                 ‚îÇ\n",
    "‚îÇ ‚Ä¢ Ki·ªÉm tra overfitting                                      ‚îÇ\n",
    "‚îÇ ‚Ä¢ L∆∞u b√°o c√°o: evaluation_results.json                      ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                            ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ 6. GENERATE SUMMARIES (6_generate_summaries.py)            ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ ‚Ä¢ T√≥m t·∫Øt to√†n b·ªô clean_data b·∫±ng ViT5                     ‚îÇ\n",
    "‚îÇ ‚Ä¢ Ph√¢n t√≠ch ng·ªØ c·∫£nh giao th√¥ng                            ‚îÇ\n",
    "‚îÇ ‚Ä¢ Tr√≠ch xu·∫•t keywords                                       ‚îÇ\n",
    "‚îÇ ‚Ä¢ L∆∞u v√†o MongoDB: final_output (K·∫æT QU·∫¢ CU·ªêI C√ôNG)       ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîß C√†i ƒë·∫∑t\n",
    "\n",
    "### 1. Y√™u c·∫ßu h·ªá th·ªëng\n",
    "- **Python**: 3.8 ho·∫∑c m·ªõi h∆°n\n",
    "- **MongoDB**: 4.0 ho·∫∑c m·ªõi h∆°n\n",
    "- **GPU**: Khuy·∫øn ngh·ªã c√≥ GPU v·ªõi 8GB+ VRAM (training s·∫Ω ch·∫≠m n·∫øu d√πng CPU)\n",
    "- **RAM**: T·ªëi thi·ªÉu 8GB\n",
    "- **Disk**: √çt nh·∫•t 5GB tr·ªëng\n",
    "\n",
    "### 2. C√†i ƒë·∫∑t dependencies\n",
    "\n",
    "```bash\n",
    "# T·∫°o virtual environment (khuy·∫øn ngh·ªã)\n",
    "python -m venv venv\n",
    "source venv/bin/activate  # Linux/Mac\n",
    "# ho·∫∑c\n",
    "venv\\Scripts\\activate  # Windows\n",
    "\n",
    "# C√†i ƒë·∫∑t packages\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "**requirements.txt:**\n",
    "```\n",
    "# Web scraping\n",
    "selenium==4.15.0\n",
    "beautifulsoup4==4.12.0\n",
    "feedparser==6.0.10\n",
    "\n",
    "# MongoDB\n",
    "pymongo==4.6.0\n",
    "\n",
    "# Machine Learning\n",
    "torch==2.1.0\n",
    "transformers==4.35.0\n",
    "datasets==2.15.0\n",
    "\n",
    "# Evaluation\n",
    "rouge-score==0.1.2\n",
    "nltk==3.8.1\n",
    "\n",
    "# Utilities\n",
    "numpy==1.24.0\n",
    "scikit-learn==1.3.0\n",
    "\n",
    "# Visualization\n",
    "matplotlib==3.8.0\n",
    "\n",
    "# API (optional)\n",
    "fastapi==0.104.0\n",
    "uvicorn==0.24.0\n",
    "```\n",
    "\n",
    "### 3. C√†i ƒë·∫∑t Edge WebDriver\n",
    "\n",
    "```bash\n",
    "# Download Edge WebDriver t∆∞∆°ng ·ª©ng v·ªõi phi√™n b·∫£n Edge c·ªßa b·∫°n\n",
    "# ƒê·∫∑t v√†o PATH ho·∫∑c c√πng th∆∞ m·ª•c v·ªõi code\n",
    "```\n",
    "\n",
    "### 4. Kh·ªüi ƒë·ªông MongoDB\n",
    "\n",
    "```bash\n",
    "# Linux/Mac\n",
    "sudo systemctl start mongod\n",
    "\n",
    "# Windows: MongoDB s·∫Ω t·ª± ch·∫°y nh∆∞ service\n",
    "# Ho·∫∑c ch·∫°y th·ªß c√¥ng:\n",
    "mongod --dbpath C:\\data\\db\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng\n",
    "\n",
    "### Quy tr√¨nh ƒë·∫ßy ƒë·ªß (6 b∆∞·ªõc)\n",
    "\n",
    "#### **B∆∞·ªõc 1: Thu th·∫≠p d·ªØ li·ªáu th√¥**\n",
    "```bash\n",
    "python 1_data_collection.py 50\n",
    "# 50 = s·ªë b√†i t·ªëi ƒëa m·ªói ngu·ªìn\n",
    "```\n",
    "**Output:** MongoDB collection `raw_data`\n",
    "\n",
    "---\n",
    "\n",
    "#### **B∆∞·ªõc 2: L·ªçc v√† g√°n nh√£n**\n",
    "```bash\n",
    "python 2_data_filtering.py\n",
    "```\n",
    "**Output:** MongoDB collection `filtered_data` (ch·ªâ b√†i v·ªÅ giao th√¥ng TP.HCM 2025)\n",
    "\n",
    "---\n",
    "\n",
    "#### **B∆∞·ªõc 3: L√†m s·∫°ch d·ªØ li·ªáu**\n",
    "```bash\n",
    "python 3_data_cleaning.py\n",
    "```\n",
    "**Output:** MongoDB collection `clean_data`\n",
    "\n",
    "---\n",
    "\n",
    "#### **B∆∞·ªõc 4: Hu·∫•n luy·ªán m√¥ h√¨nh** ‚ö†Ô∏è **C·∫¶N GPU**\n",
    "```bash\n",
    "python 4_model_training.py --auto\n",
    "# B·ªè --auto n·∫øu mu·ªën x√°c nh·∫≠n tr∆∞·ªõc khi train\n",
    "```\n",
    "**Output:** \n",
    "- Model: `./models/vit5_hcmc_traffic/`\n",
    "- Test set: `./models/vit5_hcmc_traffic/test_data.json`\n",
    "\n",
    "‚è±Ô∏è **Th·ªùi gian:** 1-3 gi·ªù t√πy GPU v√† s·ªë l∆∞·ª£ng d·ªØ li·ªáu\n",
    "\n",
    "---\n",
    "\n",
    "#### **B∆∞·ªõc 5: ƒê√°nh gi√° m√¥ h√¨nh**\n",
    "```bash\n",
    "python 5_model_evaluation.py\n",
    "```\n",
    "**Output:**\n",
    "- `./models/vit5_hcmc_traffic/evaluation_results.json`\n",
    "- `./models/vit5_hcmc_traffic/evaluation_examples.json`\n",
    "- `./models/vit5_hcmc_traffic/training_curves.png`\n",
    "\n",
    "---\n",
    "\n",
    "#### **B∆∞·ªõc 6: T·∫°o t√≥m t·∫Øt cho to√†n b·ªô d·ªØ li·ªáu** üéâ\n",
    "```bash\n",
    "python 6_generate_summaries.py\n",
    "```\n",
    "**Output:** MongoDB collection `final_output` v·ªõi:\n",
    "- `summary`: T√≥m t·∫Øt b·∫±ng ViT5\n",
    "- `analysis`: Ph√¢n t√≠ch ng·ªØ c·∫£nh giao th√¥ng\n",
    "- `keywords`: T·ª´ kh√≥a ch√≠nh\n",
    "- `traffic_entities`: C√°c th·ª±c th·ªÉ giao th√¥ng\n",
    "\n",
    "‚è±Ô∏è **Th·ªùi gian:** 5-30 ph√∫t\n",
    "\n",
    "---\n",
    "\n",
    "## üìä C·∫•u tr√∫c MongoDB Collections\n",
    "\n",
    "### 1. **raw_data** (D·ªØ li·ªáu th√¥)\n",
    "```json\n",
    "{\n",
    "  \"url\": \"https://...\",\n",
    "  \"source\": \"laodong\",\n",
    "  \"title\": \"Ti√™u ƒë·ªÅ g·ªëc...\",\n",
    "  \"content_text\": \"N·ªôi dung g·ªëc...\",\n",
    "  \"content_html\": \"<html>...\",\n",
    "  \"date_raw\": \"11/10/2025 14:30\",\n",
    "  \"content_hash\": \"abc123...\",\n",
    "  \"scraped_at\": \"2025-10-11T14:30:00\",\n",
    "  \"processing_status\": \"raw\"\n",
    "}\n",
    "```\n",
    "\n",
    "### 2. **filtered_data** (ƒê√£ l·ªçc)\n",
    "```json\n",
    "{\n",
    "  \"url\": \"https://...\",\n",
    "  \"title\": \"Ti√™u ƒë·ªÅ...\",\n",
    "  \"content_text\": \"N·ªôi dung...\",\n",
    "  \"published_date\": \"2025-10-11T00:00:00\",\n",
    "  \"category\": \"projects\",\n",
    "  \"category_name\": \"D·ª± √°n giao th√¥ng\",\n",
    "  \"is_hcmc\": true,\n",
    "  \"is_traffic\": true,\n",
    "  \"processing_status\": \"filtered\"\n",
    "}\n",
    "```\n",
    "\n",
    "### 3. **clean_data** (ƒê√£ l√†m s·∫°ch)\n",
    "```json\n",
    "{\n",
    "  \"url\": \"https://...\",\n",
    "  \"title_clean\": \"Ti√™u ƒë·ªÅ ƒë√£ l√†m s·∫°ch\",\n",
    "  \"content_clean\": \"N·ªôi dung ƒë√£ l√†m s·∫°ch...\",\n",
    "  \"sentences\": [\"C√¢u 1.\", \"C√¢u 2.\", ...],\n",
    "  \"num_sentences\": 25,\n",
    "  \"num_words\": 450,\n",
    "  \"category\": \"projects\",\n",
    "  \"published_date\": \"2025-10-11T00:00:00\",\n",
    "  \"processing_status\": \"cleaned\"\n",
    "}\n",
    "```\n",
    "\n",
    "### 4. **final_output** (K·∫æT QU·∫¢ CU·ªêI C√ôNG) ‚≠ê\n",
    "```json\n",
    "{\n",
    "  \"url\": \"https://...\",\n",
    "  \"title_clean\": \"Ti√™u ƒë·ªÅ\",\n",
    "  \"content_clean\": \"N·ªôi dung ƒë·∫ßy ƒë·ªß...\",\n",
    "  \n",
    "  \"summary\": \"T√≥m t·∫Øt b·∫±ng ViT5...\",\n",
    "  \"analysis\": \"Ch·ªß ƒë·ªÅ: D·ª± √°n giao th√¥ng. H·∫° t·∫ßng: metro...\",\n",
    "  \n",
    "  \"keywords\": [\"metro\", \"th·ªß ƒë·ª©c\", \"d·ª± √°n\", ...],\n",
    "  \"traffic_entities\": {\n",
    "    \"mentioned_infrastructure\": [\"metro\", \"c·∫ßu\"],\n",
    "    \"mentioned_locations\": [\"th·ªß ƒë·ª©c\", \"qu·∫≠n 2\"],\n",
    "    \"mentioned_issues\": [],\n",
    "    \"mentioned_projects\": [\"d·ª± √°n\", \"kh·ªüi c√¥ng\"],\n",
    "    \"main_topic\": \"D·ª± √°n giao th√¥ng\"\n",
    "  },\n",
    "  \n",
    "  \"category\": \"projects\",\n",
    "  \"published_date\": \"2025-10-11T00:00:00\",\n",
    "  \"summary_length\": 45,\n",
    "  \"summarized_at\": \"2025-10-12T10:00:00\",\n",
    "  \"processing_status\": \"completed\"\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìà ƒê√°nh gi√° m√¥ h√¨nh\n",
    "\n",
    "### Metrics s·ª≠ d·ª•ng\n",
    "\n",
    "| Metric | √ù nghƒ©a | ƒêi·ªÉm t·ªët |\n",
    "|--------|---------|----------|\n",
    "| **ROUGE-1** | ƒê·ªô tr√πng 1-gram (t·ª´ ƒë∆°n) | > 0.35 |\n",
    "| **ROUGE-2** | ƒê·ªô tr√πng 2-gram (c·ª•m 2 t·ª´) | > 0.15 |\n",
    "| **ROUGE-L** | Chu·ªói con chung d√†i nh·∫•t | > 0.30 |\n",
    "| **BLEU** | ƒê·ªô ch√≠nh x√°c n-gram | > 0.20 |\n",
    "\n",
    "### ƒê√°nh gi√° Overfitting\n",
    "\n",
    "- **Good**: Train/Val gap < 10%\n",
    "- **Moderate**: Gap 10-20%\n",
    "- **High**: Gap > 20% ‚Üí C·∫ßn gi·∫£m model complexity ho·∫∑c tƒÉng data\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è L∆∞u √Ω quan tr·ªçng\n",
    "\n",
    "### 1. **V·ªÅ d·ªØ li·ªáu hu·∫•n luy·ªán**\n",
    "- Hi·ªán t·∫°i code s·ª≠ d·ª•ng **pseudo-labels**: l·∫•y 3 c√¢u ƒë·∫ßu ti√™n l√†m summary\n",
    "- ƒê√¢y l√† ph∆∞∆°ng ph√°p t·∫°m th·ªùi, ch∆∞a t·ªëi ∆∞u\n",
    "- **Khuy·∫øn ngh·ªã**: C√≥ human-labeled summaries ƒë·ªÉ train t·ªët h∆°n\n",
    "\n",
    "### 2. **V·ªÅ GPU**\n",
    "- Training ViT5 **y√™u c·∫ßu GPU** (√≠t nh·∫•t 8GB VRAM)\n",
    "- N·∫øu d√πng CPU: r·∫•t ch·∫≠m (10-20x)\n",
    "- Gi·∫£i ph√°p:\n",
    "  - D√πng Google Colab (free GPU)\n",
    "  - Gi·∫£m `BATCH_SIZE` trong code\n",
    "  - D√πng m√¥ h√¨nh nh·ªè h∆°n (ViT5-small thay v√¨ ViT5-base)\n",
    "\n",
    "### 3. **V·ªÅ s·ªë l∆∞·ª£ng d·ªØ li·ªáu**\n",
    "- **T·ªëi thi·ªÉu**: 100 b√†i ƒë·ªÉ train\n",
    "- **Khuy·∫øn ngh·ªã**: 500-1000 b√†i\n",
    "- **T·ªët nh·∫•t**: 2000+ b√†i\n",
    "\n",
    "### 4. **V·ªÅ th·ªùi gian**\n",
    "| B∆∞·ªõc | Th·ªùi gian | Y√™u c·∫ßu |\n",
    "|------|-----------|---------|\n",
    "| 1. Collection | 10-30 ph√∫t | Internet |\n",
    "| 2. Filtering | 1-5 ph√∫t | CPU |\n",
    "| 3. Cleaning | 1-5 ph√∫t | CPU |\n",
    "| 4. Training | 1-3 gi·ªù | **GPU** |\n",
    "| 5. Evaluation | 5-15 ph√∫t | GPU/CPU |\n",
    "| 6. Generation | 5-30 ph√∫t | GPU/CPU |\n",
    "\n",
    "---\n",
    "\n",
    "## üêõ X·ª≠ l√Ω l·ªói th∆∞·ªùng g·∫∑p\n",
    "\n",
    "### L·ªói 1: \"Can't connect to MongoDB\"\n",
    "```bash\n",
    "# Ki·ªÉm tra MongoDB ƒë√£ ch·∫°y ch∆∞a\n",
    "sudo systemctl status mongod\n",
    "\n",
    "# Kh·ªüi ƒë·ªông MongoDB\n",
    "sudo systemctl start mongod\n",
    "```\n",
    "\n",
    "### L·ªói 2: \"Model not found\"\n",
    "‚Üí B·∫°n ch∆∞a ch·∫°y b∆∞·ªõc 4 (training). Ch·∫°y `python 4_model_training.py` tr∆∞·ªõc.\n",
    "\n",
    "### L·ªói 3: \"CUDA out of memory\"\n",
    "‚Üí Gi·∫£m `BATCH_SIZE` trong file training (v√≠ d·ª•: t·ª´ 8 ‚Üí 4 ‚Üí 2)\n",
    "\n",
    "### L·ªói 4: \"No data in collection\"\n",
    "‚Üí Ki·ªÉm tra l·∫°i c√°c b∆∞·ªõc tr∆∞·ªõc ƒë√£ ch·∫°y th√†nh c√¥ng ch∆∞a.\n",
    "\n",
    "---\n",
    "\n",
    "## üìö T√†i li·ªáu tham kh·∫£o\n",
    "\n",
    "- **ViT5 Model**: https://huggingface.co/VietAI/vit5-base\n",
    "- **PhoBERT**: https://github.com/VinAIResearch/PhoBERT\n",
    "- **Transformers**: https://huggingface.co/docs/transformers\n",
    "- **ROUGE Metric**: https://github.com/google-research/google-research/tree/master/rouge\n",
    "\n",
    "---\n",
    "\n",
    "## ü§ù ƒê√≥ng g√≥p\n",
    "\n",
    "N·∫øu b·∫°n mu·ªën c·∫£i thi·ªán h·ªá th·ªëng:\n",
    "1. Th√™m ngu·ªìn tin m·ªõi v√†o `NEWS_SOURCES`\n",
    "2. C·∫£i thi·ªán thu·∫≠t to√°n ph√¢n lo·∫°i\n",
    "3. Th√™m metrics ƒë√°nh gi√° kh√°c (BERTScore, METEOR)\n",
    "4. Fine-tune v·ªõi human-labeled data\n",
    "\n",
    "---\n",
    "\n",
    "## üìù License\n",
    "\n",
    "MIT License - T·ª± do s·ª≠ d·ª•ng cho m·ª•c ƒë√≠ch h·ªçc t·∫≠p v√† nghi√™n c·ª©u.\n",
    "\n",
    "---\n",
    "\n",
    "## üìß Li√™n h·ªá\n",
    "\n",
    "N·∫øu c√≥ th·∫Øc m·∫Øc, vui l√≤ng t·∫°o issue tr√™n GitHub ho·∫∑c li√™n h·ªá qua email.\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Ch√∫c b·∫°n th√†nh c√¥ng v·ªõi ƒë·ªÅ t√†i t√≥m t·∫Øt b√°o giao th√¥ng TP.HCM 2025!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8d6ee1",
   "metadata": {},
   "source": [
    "API ƒê·ªÇ TRUY V·∫§N K·∫æT QU·∫¢ T√ìM T·∫ÆT\n",
    "================================\n",
    "FastAPI server ƒë·ªÉ query d·ªØ li·ªáu t·ª´ final_output collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f48d080",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "API ƒê·ªÇ TRUY V·∫§N K·∫æT QU·∫¢ T√ìM T·∫ÆT\n",
    "================================\n",
    "FastAPI server ƒë·ªÉ query d·ªØ li·ªáu t·ª´ final_output collection\n",
    "\"\"\"\n",
    "\n",
    "from fastapi import FastAPI, HTTPException, Query\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pymongo import MongoClient\n",
    "from bson import ObjectId\n",
    "from datetime import datetime\n",
    "from typing import Optional, List\n",
    "import json\n",
    "\n",
    "# ==================== C·∫§U H√åNH ====================\n",
    "\n",
    "MONGODB_URI = \"mongodb://localhost:27017/\"\n",
    "DATABASE_NAME = \"hcmc_traffic_summarization\"\n",
    "FINAL_COLLECTION = \"final_output\"\n",
    "\n",
    "# ==================== K·∫æT N·ªêI DATABASE ====================\n",
    "\n",
    "try:\n",
    "    client = MongoClient(MONGODB_URI, serverSelectionTimeoutMS=5000)\n",
    "    client.server_info()\n",
    "    db = client[DATABASE_NAME]\n",
    "    final_col = db[FINAL_COLLECTION]\n",
    "    print(f\"‚úÖ ƒê√£ k·∫øt n·ªëi MongoDB: {FINAL_COLLECTION}\")\n",
    "    print(f\"   T·ªïng s·ªë b√†i: {final_col.count_documents({})}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå L·ªói MongoDB: {e}\")\n",
    "    final_col = None\n",
    "\n",
    "# ==================== FASTAPI APP ====================\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"TP.HCM Traffic News Summarization API\",\n",
    "    description=\"API ƒë·ªÉ truy v·∫•n tin t·ª©c giao th√¥ng TP.HCM ƒë√£ ƒë∆∞·ª£c t√≥m t·∫Øt b·∫±ng ViT5\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# CORS middleware (cho ph√©p truy c·∫≠p t·ª´ frontend)\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# ==================== HELPER FUNCTIONS ====================\n",
    "\n",
    "def serialize_doc(doc):\n",
    "    \"\"\"\n",
    "    Chuy·ªÉn MongoDB document sang JSON-serializable dict\n",
    "    \"\"\"\n",
    "    if doc is None:\n",
    "        return None\n",
    "    \n",
    "    doc[\"id\"] = str(doc[\"_id\"])\n",
    "    del doc[\"_id\"]\n",
    "    \n",
    "    # Convert datetime to ISO string\n",
    "    for key in [\"published_date\", \"scraped_at\", \"cleaned_at\", \"summarized_at\"]:\n",
    "        if key in doc and isinstance(doc[key], datetime):\n",
    "            doc[key] = doc[key].isoformat()\n",
    "    \n",
    "    return doc\n",
    "\n",
    "# ==================== API ENDPOINTS ====================\n",
    "\n",
    "@app.get(\"/\")\n",
    "def root():\n",
    "    \"\"\"\n",
    "    Th√¥ng tin API\n",
    "    \"\"\"\n",
    "    if final_col is None:\n",
    "        raise HTTPException(status_code=503, detail=\"Database kh√¥ng kh·∫£ d·ª•ng\")\n",
    "    \n",
    "    total = final_col.count_documents({})\n",
    "    \n",
    "    return {\n",
    "        \"name\": \"TP.HCM Traffic News Summarization API\",\n",
    "        \"version\": \"1.0.0\",\n",
    "        \"description\": \"API ƒë·ªÉ truy v·∫•n tin t·ª©c giao th√¥ng TP.HCM ƒë√£ t√≥m t·∫Øt\",\n",
    "        \"total_articles\": total,\n",
    "        \"endpoints\": {\n",
    "            \"GET /\": \"API info\",\n",
    "            \"GET /articles\": \"Danh s√°ch b√†i vi·∫øt (c√≥ filter)\",\n",
    "            \"GET /articles/{id}\": \"Chi ti·∫øt 1 b√†i vi·∫øt\",\n",
    "            \"GET /search\": \"T√¨m ki·∫øm full-text\",\n",
    "            \"GET /stats\": \"Th·ªëng k√™ t·ªïng quan\",\n",
    "            \"GET /categories\": \"Danh s√°ch categories\",\n",
    "            \"GET /trending\": \"T·ª´ kh√≥a trending\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "@app.get(\"/articles\")\n",
    "def list_articles(\n",
    "    category: Optional[str] = None,\n",
    "    source: Optional[str] = None,\n",
    "    from_date: Optional[str] = Query(None, description=\"YYYY-MM-DD\"),\n",
    "    to_date: Optional[str] = Query(None, description=\"YYYY-MM-DD\"),\n",
    "    limit: int = Query(20, le=100),\n",
    "    skip: int = Query(0, ge=0)\n",
    "):\n",
    "    \"\"\"\n",
    "    L·∫•y danh s√°ch b√†i vi·∫øt v·ªõi filter\n",
    "    \n",
    "    - **category**: L·ªçc theo danh m·ª•c (projects/infrastructure/issues/planning/transport)\n",
    "    - **source**: L·ªçc theo ngu·ªìn (laodong/vnexpress/tuoitre)\n",
    "    - **from_date**: L·ªçc t·ª´ ng√†y (YYYY-MM-DD)\n",
    "    - **to_date**: L·ªçc ƒë·∫øn ng√†y (YYYY-MM-DD)\n",
    "    - **limit**: S·ªë l∆∞·ª£ng k·∫øt qu·∫£ (t·ªëi ƒëa 100)\n",
    "    - **skip**: B·ªè qua N k·∫øt qu·∫£ ƒë·∫ßu (cho pagination)\n",
    "    \"\"\"\n",
    "    if final_col is None:\n",
    "        raise HTTPException(status_code=503, detail=\"Database kh√¥ng kh·∫£ d·ª•ng\")\n",
    "    \n",
    "    # Build query\n",
    "    query = {\"processing_status\": \"completed\"}\n",
    "    \n",
    "    if category:\n",
    "        query[\"category\"] = category\n",
    "    \n",
    "    if source:\n",
    "        query[\"source\"] = source\n",
    "    \n",
    "    # Date filter\n",
    "    date_query = {}\n",
    "    if from_date:\n",
    "        try:\n",
    "            date_query[\"$gte\"] = datetime.strptime(from_date, \"%Y-%m-%d\")\n",
    "        except:\n",
    "            raise HTTPException(status_code=400, detail=\"Invalid from_date format\")\n",
    "    \n",
    "    if to_date:\n",
    "        try:\n",
    "            date_query[\"$lte\"] = datetime.strptime(to_date, \"%Y-%m-%d\")\n",
    "        except:\n",
    "            raise HTTPException(status_code=400, detail=\"Invalid to_date format\")\n",
    "    \n",
    "    if date_query:\n",
    "        query[\"published_date\"] = date_query\n",
    "    \n",
    "    # Execute query\n",
    "    cursor = final_col.find(query).sort(\"published_date\", -1).skip(skip).limit(limit)\n",
    "    \n",
    "    articles = []\n",
    "    for doc in cursor:\n",
    "        articles.append({\n",
    "            \"id\": str(doc[\"_id\"]),\n",
    "            \"title\": doc.get(\"title_clean\", \"\"),\n",
    "            \"summary\": doc.get(\"summary\", \"\"),\n",
    "            \"analysis\": doc.get(\"analysis\", \"\"),\n",
    "            \"category\": doc.get(\"category\", \"\"),\n",
    "            \"category_name\": doc.get(\"category_name\", \"\"),\n",
    "            \"source\": doc.get(\"source_name\", \"\"),\n",
    "            \"published_date\": doc.get(\"published_date\", datetime.now()).isoformat(),\n",
    "            \"keywords\": doc.get(\"keywords\", [])[:5],  # Ch·ªâ l·∫•y 5 keywords\n",
    "            \"url\": doc.get(\"url\", \"\")\n",
    "        })\n",
    "    \n",
    "    total = final_col.count_documents(query)\n",
    "    \n",
    "    return {\n",
    "        \"articles\": articles,\n",
    "        \"count\": len(articles),\n",
    "        \"total\": total,\n",
    "        \"skip\": skip,\n",
    "        \"limit\": limit,\n",
    "        \"has_more\": (skip + len(articles)) < total\n",
    "    }\n",
    "\n",
    "@app.get(\"/articles/{article_id}\")\n",
    "def get_article(article_id: str):\n",
    "    \"\"\"\n",
    "    L·∫•y chi ti·∫øt ƒë·∫ßy ƒë·ªß c·ªßa 1 b√†i vi·∫øt\n",
    "    \"\"\"\n",
    "    if final_col is None:\n",
    "        raise HTTPException(status_code=503, detail=\"Database kh√¥ng kh·∫£ d·ª•ng\")\n",
    "    \n",
    "    try:\n",
    "        doc = final_col.find_one({\"_id\": ObjectId(article_id)})\n",
    "    except:\n",
    "        raise HTTPException(status_code=400, detail=\"Invalid article ID\")\n",
    "    \n",
    "    if doc is None:\n",
    "        raise HTTPException(status_code=404, detail=\"Article not found\")\n",
    "    \n",
    "    return serialize_doc(doc)\n",
    "\n",
    "@app.get(\"/search\")\n",
    "def search_articles(\n",
    "    q: str = Query(..., min_length=2, description=\"T·ª´ kh√≥a t√¨m ki·∫øm\"),\n",
    "    limit: int = Query(20, le=100)\n",
    "):\n",
    "    \"\"\"\n",
    "    T√¨m ki·∫øm full-text trong title, summary, analysis\n",
    "    \"\"\"\n",
    "    if final_col is None:\n",
    "        raise HTTPException(status_code=503, detail=\"Database kh√¥ng kh·∫£ d·ª•ng\")\n",
    "    \n",
    "    # Full-text search\n",
    "    cursor = final_col.find(\n",
    "        {\"$text\": {\"$search\": q}},\n",
    "        {\"score\": {\"$meta\": \"textScore\"}}\n",
    "    ).sort([(\"score\", {\"$meta\": \"textScore\"})]).limit(limit)\n",
    "    \n",
    "    results = []\n",
    "    for doc in cursor:\n",
    "        results.append({\n",
    "            \"id\": str(doc[\"_id\"]),\n",
    "            \"title\": doc.get(\"title_clean\", \"\"),\n",
    "            \"summary\": doc.get(\"summary\", \"\"),\n",
    "            \"category\": doc.get(\"category\", \"\"),\n",
    "            \"source\": doc.get(\"source_name\", \"\"),\n",
    "            \"published_date\": doc.get(\"published_date\", datetime.now()).isoformat(),\n",
    "            \"relevance_score\": doc.get(\"score\", 0),\n",
    "            \"url\": doc.get(\"url\", \"\")\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        \"query\": q,\n",
    "        \"results\": results,\n",
    "        \"count\": len(results)\n",
    "    }\n",
    "\n",
    "@app.get(\"/stats\")\n",
    "def get_statistics():\n",
    "    \"\"\"\n",
    "    Th·ªëng k√™ t·ªïng quan\n",
    "    \"\"\"\n",
    "    if final_col is None:\n",
    "        raise HTTPException(status_code=503, detail=\"Database kh√¥ng kh·∫£ d·ª•ng\")\n",
    "    \n",
    "    total = final_col.count_documents({\"processing_status\": \"completed\"})\n",
    "    \n",
    "    # By category\n",
    "    pipeline_category = [\n",
    "        {\"$match\": {\"processing_status\": \"completed\"}},\n",
    "        {\"$group\": {\"_id\": \"$category\", \"count\": {\"$sum\": 1}}},\n",
    "        {\"$sort\": {\"count\": -1}}\n",
    "    ]\n",
    "    by_category = {\n",
    "        doc[\"_id\"]: doc[\"count\"] \n",
    "        for doc in final_col.aggregate(pipeline_category)\n",
    "    }\n",
    "    \n",
    "    # By source\n",
    "    pipeline_source = [\n",
    "        {\"$match\": {\"processing_status\": \"completed\"}},\n",
    "        {\"$group\": {\"_id\": \"$source_name\", \"count\": {\"$sum\": 1}}},\n",
    "        {\"$sort\": {\"count\": -1}}\n",
    "    ]\n",
    "    by_source = {\n",
    "        doc[\"_id\"]: doc[\"count\"] \n",
    "        for doc in final_col.aggregate(pipeline_source)\n",
    "    }\n",
    "    \n",
    "    # By month\n",
    "    pipeline_monthly = [\n",
    "        {\"$match\": {\"processing_status\": \"completed\"}},\n",
    "        {\"$group\": {\n",
    "            \"_id\": {\n",
    "                \"year\": {\"$year\": \"$published_date\"},\n",
    "                \"month\": {\"$month\": \"$published_date\"}\n",
    "            },\n",
    "            \"count\": {\"$sum\": 1}\n",
    "        }},\n",
    "        {\"$sort\": {\"_id.year\": 1, \"_id.month\": 1}}\n",
    "    ]\n",
    "    monthly_data = []\n",
    "    for doc in final_col.aggregate(pipeline_monthly):\n",
    "        monthly_data.append({\n",
    "            \"month\": f\"{doc['_id']['year']}-{doc['_id']['month']:02d}\",\n",
    "            \"count\": doc[\"count\"]\n",
    "        })\n",
    "    \n",
    "    # Average summary length\n",
    "    pipeline_avg = [\n",
    "        {\"$match\": {\"processing_status\": \"completed\"}},\n",
    "        {\"$group\": {\n",
    "            \"_id\": None,\n",
    "            \"avg_summary_length\": {\"$avg\": \"$summary_length\"},\n",
    "            \"avg_num_sentences\": {\"$avg\": \"$num_sentences\"},\n",
    "            \"avg_num_words\": {\"$avg\": \"$num_words\"}\n",
    "        }}\n",
    "    ]\n",
    "    avg_stats = list(final_col.aggregate(pipeline_avg))\n",
    "    avg_data = avg_stats[0] if avg_stats else {}\n",
    "    \n",
    "    return {\n",
    "        \"total_articles\": total,\n",
    "        \"by_category\": by_category,\n",
    "        \"by_source\": by_source,\n",
    "        \"monthly_trend\": monthly_data,\n",
    "        \"averages\": {\n",
    "            \"summary_length\": round(avg_data.get(\"avg_summary_length\", 0), 1),\n",
    "            \"sentences_per_article\": round(avg_data.get(\"avg_num_sentences\", 0), 1),\n",
    "            \"words_per_article\": round(avg_data.get(\"avg_num_words\", 0), 1)\n",
    "        }\n",
    "    }\n",
    "\n",
    "@app.get(\"/categories\")\n",
    "def list_categories():\n",
    "    \"\"\"\n",
    "    Danh s√°ch c√°c categories v√† s·ªë l∆∞·ª£ng b√†i\n",
    "    \"\"\"\n",
    "    if final_col is None:\n",
    "        raise HTTPException(status_code=503, detail=\"Database kh√¥ng kh·∫£ d·ª•ng\")\n",
    "    \n",
    "    categories = [\n",
    "        {\n",
    "            \"key\": \"projects\",\n",
    "            \"name\": \"D·ª± √°n giao th√¥ng\",\n",
    "            \"count\": final_col.count_documents({\"category\": \"projects\"})\n",
    "        },\n",
    "        {\n",
    "            \"key\": \"infrastructure\",\n",
    "            \"name\": \"H·∫° t·∫ßng giao th√¥ng\",\n",
    "            \"count\": final_col.count_documents({\"category\": \"infrastructure\"})\n",
    "        },\n",
    "        {\n",
    "            \"key\": \"issues\",\n",
    "            \"name\": \"V·∫•n ƒë·ªÅ giao th√¥ng\",\n",
    "            \"count\": final_col.count_documents({\"category\": \"issues\"})\n",
    "        },\n",
    "        {\n",
    "            \"key\": \"planning\",\n",
    "            \"name\": \"Quy ho·∫°ch giao th√¥ng\",\n",
    "            \"count\": final_col.count_documents({\"category\": \"planning\"})\n",
    "        },\n",
    "        {\n",
    "            \"key\": \"transport\",\n",
    "            \"name\": \"Ph∆∞∆°ng ti·ªán v·∫≠n t·∫£i\",\n",
    "            \"count\": final_col.count_documents({\"category\": \"transport\"})\n",
    "        },\n",
    "        {\n",
    "            \"key\": \"general\",\n",
    "            \"name\": \"Chung\",\n",
    "            \"count\": final_col.count_documents({\"category\": \"general\"})\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"categories\": categories,\n",
    "        \"total\": sum(c[\"count\"] for c in categories)\n",
    "    }\n",
    "\n",
    "@app.get(\"/trending\")\n",
    "def get_trending_keywords(top_n: int = Query(20, le=50)):\n",
    "    \"\"\"\n",
    "    Top keywords trending (xu·∫•t hi·ªán nhi·ªÅu nh·∫•t)\n",
    "    \"\"\"\n",
    "    if final_col is None:\n",
    "        raise HTTPException(status_code=503, detail=\"Database kh√¥ng kh·∫£ d·ª•ng\")\n",
    "    \n",
    "    # Aggregate keywords\n",
    "    pipeline = [\n",
    "        {\"$match\": {\"processing_status\": \"completed\"}},\n",
    "        {\"$unwind\": \"$keywords\"},\n",
    "        {\"$group\": {\"_id\": \"$keywords\", \"count\": {\"$sum\": 1}}},\n",
    "        {\"$sort\": {\"count\": -1}},\n",
    "        {\"$limit\": top_n}\n",
    "    ]\n",
    "    \n",
    "    trending = []\n",
    "    for doc in final_col.aggregate(pipeline):\n",
    "        trending.append({\n",
    "            \"keyword\": doc[\"_id\"],\n",
    "            \"count\": doc[\"count\"]\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        \"trending_keywords\": trending,\n",
    "        \"count\": len(trending)\n",
    "    }\n",
    "\n",
    "@app.get(\"/sources\")\n",
    "def list_sources():\n",
    "    \"\"\"\n",
    "    Danh s√°ch c√°c ngu·ªìn tin\n",
    "    \"\"\"\n",
    "    if final_col is None:\n",
    "        raise HTTPException(status_code=503, detail=\"Database kh√¥ng kh·∫£ d·ª•ng\")\n",
    "    \n",
    "    pipeline = [\n",
    "        {\"$match\": {\"processing_status\": \"completed\"}},\n",
    "        {\"$group\": {\n",
    "            \"_id\": {\n",
    "                \"source\": \"$source\",\n",
    "                \"source_name\": \"$source_name\"\n",
    "            },\n",
    "            \"count\": {\"$sum\": 1}\n",
    "        }},\n",
    "        {\"$sort\": {\"count\": -1}}\n",
    "    ]\n",
    "    \n",
    "    sources = []\n",
    "    for doc in final_col.aggregate(pipeline):\n",
    "        sources.append({\n",
    "            \"key\": doc[\"_id\"][\"source\"],\n",
    "            \"name\": doc[\"_id\"][\"source_name\"],\n",
    "            \"count\": doc[\"count\"]\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        \"sources\": sources,\n",
    "        \"total\": sum(s[\"count\"] for s in sources)\n",
    "    }\n",
    "\n",
    "@app.get(\"/export\")\n",
    "def export_data(\n",
    "    format: str = Query(\"json\", regex=\"^(json|csv)$\"),\n",
    "    category: Optional[str] = None,\n",
    "    limit: int = Query(1000, le=10000)\n",
    "):\n",
    "    \"\"\"\n",
    "    Export d·ªØ li·ªáu ra JSON ho·∫∑c CSV\n",
    "    \"\"\"\n",
    "    if final_col is None:\n",
    "        raise HTTPException(status_code=503, detail=\"Database kh√¥ng kh·∫£ d·ª•ng\")\n",
    "    \n",
    "    query = {\"processing_status\": \"completed\"}\n",
    "    if category:\n",
    "        query[\"category\"] = category\n",
    "    \n",
    "    cursor = final_col.find(query).limit(limit)\n",
    "    \n",
    "    articles = []\n",
    "    for doc in cursor:\n",
    "        articles.append({\n",
    "            \"id\": str(doc[\"_id\"]),\n",
    "            \"title\": doc.get(\"title_clean\", \"\"),\n",
    "            \"summary\": doc.get(\"summary\", \"\"),\n",
    "            \"analysis\": doc.get(\"analysis\", \"\"),\n",
    "            \"category\": doc.get(\"category\", \"\"),\n",
    "            \"source\": doc.get(\"source_name\", \"\"),\n",
    "            \"published_date\": doc.get(\"published_date\", datetime.now()).isoformat(),\n",
    "            \"url\": doc.get(\"url\", \"\")\n",
    "        })\n",
    "    \n",
    "    if format == \"csv\":\n",
    "        # Simple CSV\n",
    "        csv_lines = [\"ID,Title,Summary,Category,Source,Date,URL\"]\n",
    "        for art in articles:\n",
    "            csv_lines.append(\n",
    "                f\"{art['id']},\"\n",
    "                f\"\\\"{art['title']}\\\",\"\n",
    "                f\"\\\"{art['summary']}\\\",\"\n",
    "                f\"{art['category']},\"\n",
    "                f\"{art['source']},\"\n",
    "                f\"{art['published_date']},\"\n",
    "                f\"{art['url']}\"\n",
    "            )\n",
    "        \n",
    "        return {\n",
    "            \"format\": \"csv\",\n",
    "            \"data\": \"\\n\".join(csv_lines),\n",
    "            \"count\": len(articles)\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        \"format\": \"json\",\n",
    "        \"articles\": articles,\n",
    "        \"count\": len(articles)\n",
    "    }\n",
    "\n",
    "# ==================== CH·∫†Y SERVER ====================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üöÄ KH·ªûI ƒê·ªòNG API SERVER\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\nüì° Server s·∫Ω ch·∫°y t·∫°i: http://localhost:8000\")\n",
    "    print(f\"\\nüìñ Swagger UI (API docs): http://localhost:8000/docs\")\n",
    "    print(f\"üìñ ReDoc: http://localhost:8000/redoc\")\n",
    "    print(f\"\\nüí° V√≠ d·ª• s·ª≠ d·ª•ng:\")\n",
    "    print(f\"   curl http://localhost:8000/stats\")\n",
    "    print(f\"   curl http://localhost:8000/articles?category=projects&limit=10\")\n",
    "    print(f\"   curl http://localhost:8000/search?q=metro\")\n",
    "    print(f\"\\n{'='*70}\\n\")\n",
    "    \n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
