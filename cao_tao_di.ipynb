{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778ce290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Không tìm thấy nội dung bài báo.\n",
      "Không có nội dung để hiển thị.\n",
      "None\n",
      "<html>\n",
      " <body>\n",
      "  <script>\n",
      "   document.cookie=\"D1N=4492fe22b0f8939a6e2cac59d196c3e2\"+\"; expires=Fri, 31 Dec 2099 23:59:59 GMT; path=/\";window.location.reload(true);\n",
      "  </script>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "from transformers import pipeline\n",
    "from fastapi import FastAPI\n",
    "from bson import ObjectId\n",
    "url = \"https://laodong.vn/xa-hoi/chuan-bi-khoi-cong-loat-du-an-giao-thong-lon-1586625.ldo\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "content_div = soup.select_one(\"div.detail-content, div.article-content\")\n",
    "  # hoặc thử \"detail-content\", \"content-detail\"\n",
    "if content_div:\n",
    "    article_text = content_div.get_text(strip=True)\n",
    "    print(article_text)\n",
    "else:\n",
    "    article_text = None\n",
    "    print(\"Không tìm thấy nội dung bài báo.\")\n",
    "if article_text:\n",
    "    print(\"Đã lấy được nội dung.\")\n",
    "else:\n",
    "    print(\"Không có nội dung để hiển thị.\")\n",
    "print(article_text)\n",
    "print(soup.prettify())\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"traffic_news\"]\n",
    "collection = db[\"articles\"]\n",
    "document = {\n",
    "    \"url\": url,\n",
    "    \"title\": \"Chuẩn bị khởi công loạt dự án giao thông lớn\",\n",
    "    \"content\": article_text,\n",
    "    \"date\": datetime.now()\n",
    "}\n",
    "collection.insert_one(document)\n",
    "summarizer = pipeline(\"summarization\", model=\"VietAI/vit5-base-vietnews-summarization\")\n",
    "summary = summarizer(article_text, max_length=200, min_length=50, do_sample=False)[0][\"summary_text\"]\n",
    "print(\"Tóm tắt:\", summary)\n",
    "app = FastAPI()\n",
    "@app.get(\"/summary\")\n",
    "def get_summary(article_id: str):\n",
    "    doc = collection.find_one({\"_id\": ObjectId(article_id)})\n",
    "    summary = summarizer(doc[\"content\"], max_length=200)[0][\"summary_text\"]\n",
    "    return {\"title\": doc[\"title\"], \"summary\": summary}\n",
    "    collection.update_one({\"_id\": doc[\"_id\"]}, {\"$set\": {\"summary\": summary}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4826750b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Dùng tóm tắt đơn giản (3 câu đầu)\n",
      "✓ Đã kết nối MongoDB\n",
      "\n",
      "============================================================\n",
      "BẮT ĐẦU SCRAPE\n",
      "============================================================\n",
      "\n",
      "Đang khởi động Edge...\n",
      "Đang tải trang: https://laodong.vn/xa-hoi/chuan-bi-khoi-cong-loat-du-an-giao-thong-lon-1586625.ldo\n",
      "Đang đợi nội dung load...\n",
      "✓ Tìm thấy nội dung với: article\n",
      "✓ Đã lấy được 4137 ký tự\n",
      "✓ Đã đóng Edge\n",
      "\n",
      "✓ Tiêu đề: Chuẩn bị khởi công loạt dự án giao thông lớn...\n",
      "✓ Nội dung: 4137 ký tự\n",
      "\n",
      "Nội dung đầu tiên:\n",
      "Chuẩn bị khởi công loạt dự án giao thông lớn Thứ hai, 06/10/2025 12:30 (GMT+7) TPHCM chuẩn bị khởi công loạt dự án giao thông lớn trị giá hàng chục nghìn tỉ đồng, từ Metro số 2, cao tốc TPHCM - Mộc Bài đến Vành đai 2, kỳ vọng mở rộng cửa ngõ và thúc đẩy phát triển đô thị. Ùn tắc tại cửa ngõ phía Đôn...\n",
      "\n",
      "============================================================\n",
      "TÓM TẮT:\n",
      "============================================================\n",
      "Chuẩn bị khởi công loạt dự án giao thông lớn Thứ hai, 06/10/2025 12:30 (GMT+7) TPHCM chuẩn bị khởi công loạt dự án giao thông lớn trị giá hàng chục nghìn tỉ đồng, từ Metro số 2, cao tốc TPHCM - Mộc Bài đến Vành đai 2, kỳ vọng mở rộng cửa ngõ và thúc đẩy phát triển đô thị. Ùn tắc tại cửa ngõ phía Đông Bắc TPHCM. Ảnh: Anh Tú Trong số các công trình sắp khởi công, tuyến Metro số 2 (Bến Thành - Tham Lương) nhận được sự quan tâm và kỳ vọng lớn của người dân.\n",
      "============================================================\n",
      "\n",
      "✓ Đã lưu vào MongoDB với ID: 68e677125ec0ad732b49c4f3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from bson import ObjectId\n",
    "import time\n",
    "\n",
    "# Khởi tạo model (fallback mode - không cần PyTorch)\n",
    "print(\"⚠ Dùng tóm tắt đơn giản (3 câu đầu)\")\n",
    "USE_AI_SUMMARY = False\n",
    "\n",
    "# Kết nối MongoDB\n",
    "try:\n",
    "    client = MongoClient(\"mongodb://localhost:27017/\", serverSelectionTimeoutMS=5000)\n",
    "    client.server_info()\n",
    "    db = client[\"traffic_news\"]\n",
    "    collection = db[\"articles\"]\n",
    "    print(\"✓ Đã kết nối MongoDB\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Lỗi kết nối MongoDB: {e}\")\n",
    "    collection = None\n",
    "\n",
    "def scrape_article_selenium(url):\n",
    "    \"\"\"Lấy nội dung bài báo bằng Selenium\"\"\"\n",
    "    driver = None\n",
    "    try:\n",
    "        # Setup Edge headless\n",
    "        edge_options = webdriver.EdgeOptions()\n",
    "        edge_options.add_argument(\"--headless\")  # Chạy ngầm, không mở cửa sổ\n",
    "        edge_options.add_argument(\"--no-sandbox\")\n",
    "        edge_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        edge_options.add_argument(\"--disable-gpu\")\n",
    "        edge_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\")\n",
    "        \n",
    "        print(\"Đang khởi động Edge...\")\n",
    "        driver = webdriver.Edge(options=edge_options)\n",
    "        driver.set_page_load_timeout(30)\n",
    "        \n",
    "        print(f\"Đang tải trang: {url}\")\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Đợi JavaScript load xong (đợi nội dung xuất hiện)\n",
    "        print(\"Đang đợi nội dung load...\")\n",
    "        wait = WebDriverWait(driver, 15)\n",
    "        \n",
    "        # Thử nhiều selector (Lao Động thường dùng)\n",
    "        selectors = [\n",
    "            (By.CSS_SELECTOR, \"div.detail-content-body\"),\n",
    "            (By.CSS_SELECTOR, \"div.detail__content\"),\n",
    "            (By.CSS_SELECTOR, \"article.detail\"),\n",
    "            (By.CSS_SELECTOR, \"div.article-content\"),\n",
    "            (By.XPATH, \"//div[contains(@class, 'detail') and contains(@class, 'content')]\"),\n",
    "            (By.TAG_NAME, \"article\")\n",
    "        ]\n",
    "        \n",
    "        content_element = None\n",
    "        used_selector = None\n",
    "        \n",
    "        for by, selector in selectors:\n",
    "            try:\n",
    "                content_element = wait.until(\n",
    "                    EC.presence_of_element_located((by, selector))\n",
    "                )\n",
    "                if content_element and len(content_element.text) > 200:\n",
    "                    used_selector = selector\n",
    "                    print(f\"✓ Tìm thấy nội dung với: {selector}\")\n",
    "                    break\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        if not content_element:\n",
    "            # Fallback: Đợi 3s rồi lấy toàn bộ body\n",
    "            print(\"⚠ Không tìm thấy selector cụ thể, lấy toàn bộ body...\")\n",
    "            time.sleep(3)\n",
    "            content_element = driver.find_element(By.TAG_NAME, \"body\")\n",
    "        \n",
    "        # Lấy title\n",
    "        try:\n",
    "            title_element = driver.find_element(By.CSS_SELECTOR, \"h1.detail__title, h1\")\n",
    "            title = title_element.text.strip()\n",
    "        except:\n",
    "            title = driver.title\n",
    "        \n",
    "        # Lấy nội dung\n",
    "        article_text = content_element.text.strip()\n",
    "        \n",
    "        # Làm sạch (loại bỏ navigation, ads...)\n",
    "        lines = [line.strip() for line in article_text.split(\"\\n\") if line.strip()]\n",
    "        # Loại bỏ các dòng ngắn (thường là menu, ads)\n",
    "        lines = [line for line in lines if len(line) > 30]\n",
    "        article_text = \" \".join(lines)\n",
    "        \n",
    "        print(f\"✓ Đã lấy được {len(article_text)} ký tự\")\n",
    "        \n",
    "        return title, article_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Lỗi Selenium: {e}\")\n",
    "        print(f\"   Tip: Đảm bảo Edge đã được cài đặt hoặc cài Edge WebDriver\")\n",
    "        return None, None\n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "            print(\"✓ Đã đóng Edge\")\n",
    "\n",
    "def simple_summary(text, num_sentences=3):\n",
    "    \"\"\"Tóm tắt đơn giản bằng cách lấy N câu đầu\"\"\"\n",
    "    sentences = text.replace(\"?\", \".\").replace(\"!\", \".\").split(\". \")\n",
    "    sentences = [s.strip() for s in sentences if len(s.strip()) > 20]\n",
    "    return \". \".join(sentences[:num_sentences]) + \".\"\n",
    "\n",
    "# Scraping\n",
    "url = \"https://laodong.vn/xa-hoi/chuan-bi-khoi-cong-loat-du-an-giao-thong-lon-1586625.ldo\"\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"BẮT ĐẦU SCRAPE\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "title, article_text = scrape_article_selenium(url)\n",
    "\n",
    "if article_text and len(article_text) > 100:\n",
    "    print(f\"\\n✓ Tiêu đề: {title[:100]}...\")\n",
    "    print(f\"✓ Nội dung: {len(article_text)} ký tự\")\n",
    "    print(f\"\\nNội dung đầu tiên:\\n{article_text[:300]}...\\n\")\n",
    "    \n",
    "    # Tóm tắt\n",
    "    summary = simple_summary(article_text, num_sentences=3)\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"TÓM TẮT:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(summary)\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Lưu vào MongoDB\n",
    "    if collection is not None:\n",
    "        try:\n",
    "            document = {\n",
    "                \"url\": url,\n",
    "                \"title\": title,\n",
    "                \"content\": article_text,\n",
    "                \"summary\": summary,\n",
    "                \"date\": datetime.now(),\n",
    "                \"scraped_at\": datetime.now()\n",
    "            }\n",
    "            result = collection.insert_one(document)\n",
    "            print(f\"✓ Đã lưu vào MongoDB với ID: {result.inserted_id}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Lỗi khi lưu MongoDB: {e}\\n\")\n",
    "else:\n",
    "    print(\"✗ Không lấy được nội dung đủ dài\\n\")\n",
    "\n",
    "# FastAPI\n",
    "app = FastAPI(title=\"Traffic News API\")\n",
    "\n",
    "@app.get(\"/\")\n",
    "def root():\n",
    "    return {\n",
    "        \"message\": \"Traffic News Summarization API\",\n",
    "        \"endpoints\": {\n",
    "            \"GET /articles\": \"Danh sách bài viết\",\n",
    "            \"GET /summary/{article_id}\": \"Tóm tắt bài viết\",\n",
    "            \"POST /scrape\": \"Scrape URL mới (body: {\\\"url\\\": \\\"...\\\"})\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "@app.get(\"/summary/{article_id}\")\n",
    "def get_summary(article_id: str):\n",
    "    \"\"\"Lấy tóm tắt bài viết theo ID\"\"\"\n",
    "    if collection is None:\n",
    "        raise HTTPException(status_code=503, detail=\"MongoDB không khả dụng\")\n",
    "    \n",
    "    try:\n",
    "        doc = collection.find_one({\"_id\": ObjectId(article_id)})\n",
    "        \n",
    "        if not doc:\n",
    "            raise HTTPException(status_code=404, detail=\"Không tìm thấy bài viết\")\n",
    "        \n",
    "        # Nếu đã có summary\n",
    "        if \"summary\" in doc and doc[\"summary\"]:\n",
    "            return {\n",
    "                \"id\": str(doc[\"_id\"]),\n",
    "                \"title\": doc[\"title\"],\n",
    "                \"summary\": doc[\"summary\"],\n",
    "                \"content_length\": len(doc.get(\"content\", \"\")),\n",
    "                \"cached\": True\n",
    "            }\n",
    "        \n",
    "        # Tạo mới\n",
    "        if not doc.get(\"content\"):\n",
    "            raise HTTPException(status_code=400, detail=\"Bài viết không có nội dung\")\n",
    "        \n",
    "        summary = simple_summary(doc[\"content\"], num_sentences=3)\n",
    "        \n",
    "        # Cập nhật vào DB\n",
    "        collection.update_one(\n",
    "            {\"_id\": doc[\"_id\"]}, \n",
    "            {\"$set\": {\"summary\": summary}}\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"id\": str(doc[\"_id\"]),\n",
    "            \"title\": doc[\"title\"],\n",
    "            \"summary\": summary,\n",
    "            \"content_length\": len(doc[\"content\"]),\n",
    "            \"cached\": False\n",
    "        }\n",
    "        \n",
    "    except HTTPException:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Lỗi: {str(e)}\")\n",
    "\n",
    "@app.get(\"/articles\")\n",
    "def list_articles(limit: int = 10):\n",
    "    \"\"\"Liệt kê bài viết\"\"\"\n",
    "    if collection is None:\n",
    "        raise HTTPException(status_code=503, detail=\"MongoDB không khả dụng\")\n",
    "    \n",
    "    docs = collection.find().sort(\"date\", -1).limit(limit)\n",
    "    articles = []\n",
    "    for doc in docs:\n",
    "        articles.append({\n",
    "            \"id\": str(doc[\"_id\"]),\n",
    "            \"title\": doc[\"title\"],\n",
    "            \"date\": doc[\"date\"].isoformat(),\n",
    "            \"url\": doc.get(\"url\", \"\"),\n",
    "            \"has_summary\": \"summary\" in doc\n",
    "        })\n",
    "    return {\"articles\": articles, \"count\": len(articles)}\n",
    "\n",
    "@app.post(\"/scrape\")\n",
    "def scrape_new_article(data: dict):\n",
    "    \"\"\"Scrape URL mới\"\"\"\n",
    "    url = data.get(\"url\")\n",
    "    if not url:\n",
    "        raise HTTPException(status_code=400, detail=\"Thiếu URL\")\n",
    "    \n",
    "    title, content = scrape_article_selenium(url)\n",
    "    \n",
    "    if not content:\n",
    "        raise HTTPException(status_code=400, detail=\"Không scrape được nội dung\")\n",
    "    \n",
    "    summary = simple_summary(content, num_sentences=3)\n",
    "    \n",
    "    if collection:\n",
    "        doc = {\n",
    "            \"url\": url,\n",
    "            \"title\": title,\n",
    "            \"content\": content,\n",
    "            \"summary\": summary,\n",
    "            \"date\": datetime.now()\n",
    "        }\n",
    "        result = collection.insert_one(doc)\n",
    "        return {\n",
    "            \"id\": str(result.inserted_id),\n",
    "            \"title\": title,\n",
    "            \"summary\": summary,\n",
    "            \"content_length\": len(content)\n",
    "        }\n",
    "    \n",
    "    return {\"title\": title, \"summary\": summary, \"content_length\": len(content)}\n",
    "\n",
    "# Chạy: uvicorn filename:app --reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aab5ff3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Đã kết nối MongoDB với full-text search\n",
      "❌ Lệnh không hợp lệ: --f=c:\\Users\\ASUS\\AppData\\Roaming\\jupyter\\runtime\\kernel-v31c77da465df6157aa337d65d287250ab347656fe.json\n",
      "Sử dụng: python C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python313\\site-packages\\ipykernel_launcher.py [scrape|stats]\n",
      "\n",
      "======================================================================\n",
      "🚀 TP.HCM TRAFFIC NEWS SCRAPER\n",
      "======================================================================\n",
      "\n",
      "[1] Scrape 5 bài/nguồn\n",
      "[2] Xem thống kê\n",
      "\n",
      "======================================================================\n",
      "🚀 TP.HCM TRAFFIC NEWS INTELLIGENCE SYSTEM 2025\n",
      "======================================================================\n",
      "📅 Thời gian: 2025-01-01 → 2025-10-09\n",
      "📰 Đang scrape 5 bài/nguồn...\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "🚀 CHIẾN DỊCH SCRAPING: GIAO THÔNG TP.HCM 2025\n",
      "   Thời gian: 2025-01-01 → 2025-10-09\n",
      "======================================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "🔍 BẮT ĐẦU: Báo Lao Động\n",
      "============================================================\n",
      "\n",
      "📡 Bước 1: Lấy URLs từ RSS...\n",
      "  📡 Đang đọc RSS: https://laodong.vn/rss/giao-thong.rss\n",
      "  ✓ Tìm thấy 0 URLs từ RSS\n",
      "  📡 Đang đọc RSS: https://laodong.vn/rss/xa-hoi.rss\n",
      "  ✓ Tìm thấy 0 URLs từ RSS\n",
      "\n",
      "📄 Bước 2: Lấy URLs từ trang danh sách...\n",
      "  📄 Trang 1: https://laodong.vn/giao-thong/...\n",
      "  ✓ Tìm thấy 7 URLs\n",
      "  📄 Trang 2: https://laodong.vn/giao-thong/trang-2.htm...\n",
      "  ✓ Tìm thấy 0 URLs\n",
      "\n",
      "📊 Tổng cộng: 4 URLs duy nhất\n",
      "\n",
      "📰 Bước 3: Scrape chi tiết...\n",
      "\n",
      "[1/4] https://laodong.vn/giao-thong/tau-hoa-phuong-do-duoc-gan-bien-cong-tri...\n",
      "  ✅ Đã lưu: Tàu Hoa Phượng Đỏ được gắn biển công trình chào mừ...\n",
      "  📂 Danh mục: projects\n",
      "  📅 Ngày: 2025-10-09\n",
      "\n",
      "[2/4] https://www.dmca.com/Protection/Status.aspx?ID=2a0ef338-d2a6-4097-82db...\n",
      "  ✗ Lỗi scrape: Message: timeout: Timed out receiving message from renderer: 28.900\n",
      "  (Session info: MicrosoftEdge=141.0.3537.57)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x7ff6a5b32a15+51877]\n",
      "\tGetHandleVerifier [0x0x7ff6a5b32a74+51972]\n",
      "\tGetHandleVerifier [0x0x7ff6a5ce93c5+1848405]\n",
      "\t(No symbol) [0x0x7ff6a589220b]\n",
      "\t(No symbol) [0x0x7ff6a5891f81]\n",
      "\t(No symbol) [0x0x7ff6a588feeb]\n",
      "\t(No symbol) [0x0x7ff6a589070b]\n",
      "\t(No symbol) [0x0x7ff6a589c522]\n",
      "\t(No symbol) [0x0x7ff6a58af76d]\n",
      "\t(No symbol) [0x0x7ff6a58b5a4a]\n",
      "\t(No symbol) [0x0x7ff6a58910d8]\n",
      "\t(No symbol) [0x0x7ff6a5890e0a]\n",
      "\t(No symbol) [0x0x7ff6a58af51e]\n",
      "\t(No symbol) [0x0x7ff6a592f59e]\n",
      "\t(No symbol) [0x0x7ff6a5911df3]\n",
      "\t(No symbol) [0x0x7ff6a58e5b36]\n",
      "\t(No symbol) [0x0x7ff6a58e4d80]\n",
      "\t(No symbol) [0x0x7ff6a58e5973]\n",
      "\t(No symbol) [0x0x7ff6a59ae4a5]\n",
      "\t(No symbol) [0x0x7ff6a59aa75d]\n",
      "\tGetHandleVerifier [0x0x7ff6a5b5d253+226019]\n",
      "\tGetHandleVerifier [0x0x7ff6a5b4c421+156849]\n",
      "\tGetHandleVerifier [0x0x7ff6a5b54919+190889]\n",
      "\tGetHandleVerifier [0x0x7ff6a5b39b54+80868]\n",
      "\tGetHandleVerifier [0x0x7ff6a5b39ca3+81203]\n",
      "\tGetHandleVerifier [0x0x7ff6a5b28226+8886]\n",
      "\tBaseThreadInitThunk [0x0x7fff2502259d+29]\n",
      "\tRtlUserThreadStart [0x0x7fff25a6af78+40]\n",
      "\n",
      "\n",
      "[3/4] https://laodong.vn/giao-thong/da-nang-phan-quyen-to-chuc-van-hanh-giao...\n",
      "  ✅ Đã lưu: Đà Nẵng phân quyền tổ chức vận hành giao thông đến...\n",
      "  📂 Danh mục: projects\n",
      "  📅 Ngày: 2025-10-09\n",
      "\n",
      "[4/4] https://laodong.vn/giao-thong/chieu-810-ha-noi-con-13-vi-tri-ngap-sau-...\n",
      "  ✅ Đã lưu: Chiều 8.10, Hà Nội còn 13 vị trí ngập sâu...\n",
      "  📂 Danh mục: projects\n",
      "  📅 Ngày: 2025-10-09\n",
      "\n",
      "✅ Hoàn thành Báo Lao Động\n",
      "   Đã lưu: 3 / 3 bài\n",
      "\n",
      "============================================================\n",
      "🔍 BẮT ĐẦU: VnExpress\n",
      "============================================================\n",
      "\n",
      "📡 Bước 1: Lấy URLs từ RSS...\n",
      "  📡 Đang đọc RSS: https://vnexpress.net/rss/giao-thong.rss\n",
      "  ✓ Tìm thấy 20 URLs từ RSS\n",
      "\n",
      "📄 Bước 2: Lấy URLs từ trang danh sách...\n",
      "  📄 Trang 1: https://vnexpress.net/giao-thong...\n",
      "  ✓ Tìm thấy 0 URLs\n",
      "\n",
      "📊 Tổng cộng: 20 URLs duy nhất\n",
      "\n",
      "📰 Bước 3: Scrape chi tiết...\n",
      "\n",
      "[1/5] https://vnexpress.net/hai-cau-day-vang-lon-nhat-cao-toc-ben-luc-sau-10...\n",
      "  ✅ Đã lưu: Hai cầu dây văng lớn nhất cao tốc Bến Lức sau 10 n...\n",
      "  📂 Danh mục: projects\n",
      "  📅 Ngày: 2025-10-09\n",
      "\n",
      "[2/5] https://vnexpress.net/duong-ket-noi-tp-hcm-tay-ninh-mo-rong-gan-3-lan-...\n",
      "  ✅ Đã lưu: Đường kết nối TP HCM - Tây Ninh mở rộng gần 3 lần...\n",
      "  📂 Danh mục: projects\n",
      "  📅 Ngày: 2025-10-09\n",
      "\n",
      "[3/5] https://vnexpress.net/tp-hcm-yeu-cau-khoi-phuc-pha-binh-quoi-giam-un-t...\n",
      "  ✅ Đã lưu: TP HCM yêu cầu khôi phục phà Bình Quới giảm ùn tắc...\n",
      "  📂 Danh mục: projects\n",
      "  📅 Ngày: 2025-10-09\n",
      "\n",
      "[4/5] https://vnexpress.net/duong-3-800-ty-dong-noi-tp-hcm-dong-nai-day-dac-...\n",
      "  ✅ Đã lưu: Đường 3.800 tỷ đồng nối TP HCM - Đồng Nai dày đặc ...\n",
      "  📂 Danh mục: projects\n",
      "  📅 Ngày: 2025-10-09\n",
      "\n",
      "[5/5] https://vnexpress.net/lui-thoi-han-chuyen-doi-tai-khoan-giao-thong-voi...\n",
      "  ⏭️  Không liên quan TP.HCM\n",
      "\n",
      "✅ Hoàn thành VnExpress\n",
      "   Đã lưu: 4 / 4 bài\n",
      "\n",
      "============================================================\n",
      "🔍 BẮT ĐẦU: Báo Tuổi Trẻ\n",
      "============================================================\n",
      "\n",
      "📡 Bước 1: Lấy URLs từ RSS...\n",
      "  📡 Đang đọc RSS: https://tuoitre.vn/rss/giao-thong.rss\n",
      "  ✓ Tìm thấy 20 URLs từ RSS\n",
      "\n",
      "📄 Bước 2: Lấy URLs từ trang danh sách...\n",
      "  📄 Trang 1: https://tuoitre.vn/giao-thong.htm...\n",
      "  ✓ Tìm thấy 0 URLs\n",
      "\n",
      "📊 Tổng cộng: 20 URLs duy nhất\n",
      "\n",
      "📰 Bước 3: Scrape chi tiết...\n",
      "\n",
      "[1/5] https://tuoitre.vn/khoi-dong-giai-dua-o-to-dia-hinh-co-quy-mo-lon-nhat...\n",
      "  ⏭️  Không liên quan TP.HCM\n",
      "\n",
      "[2/5] https://tuoitre.vn/indonesia-chi-con-1-co-hoi-gianh-ve-du-world-cup-20...\n",
      "  ⏭️  Không liên quan TP.HCM\n",
      "\n",
      "[3/5] https://tuoitre.vn/he-lo-so-phan-moi-cua-khu-dat-vang-ben-song-han-tun...\n",
      "  ⏭️  Không liên quan TP.HCM\n",
      "\n",
      "[4/5] https://tuoitre.vn/khoi-to-giam-doc-cong-ty-vang-bac-mao-thiet-bao-tha...\n",
      "  ⏭️  Không liên quan TP.HCM\n",
      "\n",
      "[5/5] https://tuoitre.vn/nguoi-viet-nhiem-ky-sinh-trung-tu-nhung-mon-an-dac-...\n",
      "  ⏭️  Không liên quan TP.HCM\n",
      "\n",
      "✅ Hoàn thành Báo Tuổi Trẻ\n",
      "   Đã lưu: 0 / 0 bài\n",
      "\n",
      "============================================================\n",
      "🔍 BẮT ĐẦU: Sở GTVT TP.HCM\n",
      "============================================================\n",
      "\n",
      "📡 Bước 1: Lấy URLs từ RSS...\n",
      "\n",
      "📄 Bước 2: Lấy URLs từ trang danh sách...\n",
      "  📄 Trang 1: https://giaothong.hochiminhcity.gov.vn/render/rendertintuc.a...\n",
      "  ✓ Tìm thấy 0 URLs\n",
      "\n",
      "📊 Tổng cộng: 0 URLs duy nhất\n",
      "\n",
      "📰 Bước 3: Scrape chi tiết...\n",
      "\n",
      "✅ Hoàn thành Sở GTVT TP.HCM\n",
      "   Đã lưu: 0 / 0 bài\n",
      "\n",
      "============================================================\n",
      "🔍 BẮT ĐẦU: Báo Thanh Niên\n",
      "============================================================\n",
      "\n",
      "📡 Bước 1: Lấy URLs từ RSS...\n",
      "  📡 Đang đọc RSS: https://thanhnien.vn/rss/giao-thong.rss\n",
      "  ✓ Tìm thấy 0 URLs từ RSS\n",
      "\n",
      "📄 Bước 2: Lấy URLs từ trang danh sách...\n",
      "  📄 Trang 1: https://thanhnien.vn/giao-thong/...\n",
      "  ✓ Tìm thấy 0 URLs\n",
      "\n",
      "📊 Tổng cộng: 0 URLs duy nhất\n",
      "\n",
      "📰 Bước 3: Scrape chi tiết...\n",
      "\n",
      "✅ Hoàn thành Báo Thanh Niên\n",
      "   Đã lưu: 0 / 0 bài\n",
      "\n",
      "======================================================================\n",
      "✅ HOÀN THÀNH!\n",
      "======================================================================\n",
      "📊 Kết quả:\n",
      "   • Tổng URLs: 44\n",
      "   • Đã scrape: 7\n",
      "   • Đã lưu DB: 7\n",
      "   • Trùng lặp: 0\n",
      "\n",
      "   🔹 Báo Lao Động: 3 bài\n",
      "\n",
      "   🔹 VnExpress: 4 bài\n",
      "\n",
      "   🔹 Báo Tuổi Trẻ: 0 bài\n",
      "\n",
      "   🔹 Sở GTVT TP.HCM: 0 bài\n",
      "\n",
      "   🔹 Báo Thanh Niên: 0 bài\n",
      "\n",
      "======================================================================\n",
      "💡 Xem dữ liệu:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1129\u001b[39m\n\u001b[32m   1126\u001b[39m         show_stats_simple()\n\u001b[32m   1128\u001b[39m \u001b[38;5;66;03m# Chạy interactive mode thay vì dùng sys.argv\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1129\u001b[39m \u001b[43minteractive_mode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1124\u001b[39m, in \u001b[36minteractive_mode\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1121\u001b[39m choice = \u001b[38;5;28minput\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mChọn (1/2): \u001b[39m\u001b[33m\"\u001b[39m).strip()\n\u001b[32m   1123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m choice == \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1124\u001b[39m     \u001b[43mquick_scrape\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1125\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m choice == \u001b[33m\"\u001b[39m\u001b[33m2\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1126\u001b[39m     show_stats_simple()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 976\u001b[39m, in \u001b[36mquick_scrape\u001b[39m\u001b[34m(num_articles_per_source)\u001b[39m\n\u001b[32m    974\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m70\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    975\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m💡 Xem dữ liệu:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m976\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   python \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[34;43m__file__\u001b[39;49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m stats\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    977\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   hoặc chạy API: uvicorn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[34m__file__\u001b[39m.replace(\u001b[33m'\u001b[39m\u001b[33m.py\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:app --reload\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    978\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m70\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime, timedelta\n",
    "from fastapi import FastAPI, HTTPException, BackgroundTasks, Query\n",
    "from bson import ObjectId\n",
    "from typing import List, Optional, Dict\n",
    "import time\n",
    "import re\n",
    "import hashlib\n",
    "from collections import Counter\n",
    "import math\n",
    "from difflib import SequenceMatcher\n",
    "import feedparser\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "# ==================== CẤU HÌNH NÂNG CAO ====================\n",
    "\n",
    "# Thời gian lọc\n",
    "START_DATE = datetime(2025, 1, 1)\n",
    "CURRENT_DATE = datetime.now()\n",
    "\n",
    "# Từ khóa TP.HCM (để lọc bài viết)\n",
    "HCMC_KEYWORDS = [\n",
    "    \"tp.hcm\", \"tp hcm\", \"hồ chí minh\", \"ho chi minh\", \"sài gòn\", \"saigon\",\n",
    "    \"thành phố hồ chí minh\", \"tphcm\", \"hcmc\"\n",
    "]\n",
    "\n",
    "# Từ khóa giao thông TP.HCM\n",
    "TRAFFIC_KEYWORDS = {\n",
    "    \"projects\": [\"metro\", \"tàu điện\", \"cao tốc\", \"cầu\", \"đường vành đai\", \"brt\", \n",
    "                 \"khởi công\", \"hoàn thành\", \"dự án\", \"đầu tư\", \"xây dựng\"],\n",
    "    \"infrastructure\": [\"hạ tầng\", \"giao thông\", \"đường bộ\", \"đường sắt\", \"đường thủy\",\n",
    "                      \"sân bay\", \"cảng\", \"bến xe\", \"trạm\", \"nút giao thông\"],\n",
    "    \"issues\": [\"kẹt xe\", \"ùn tắc\", \"tai nạn\", \"ngập nước\", \"ổ gà\", \"hư hỏng\"],\n",
    "    \"planning\": [\"quy hoạch\", \"kế hoạch\", \"phê duyệt\", \"triển khai\", \"điều chỉnh\"],\n",
    "    \"transport\": [\"xe buýt\", \"xe bus\", \"taxi\", \"grab\", \"vận tải\", \"xe máy\", \"ô tô\"]\n",
    "}\n",
    "\n",
    "# Cấu hình nguồn tin chi tiết hơn\n",
    "NEWS_SOURCES = {\n",
    "    \"laodong\": {\n",
    "        \"name\": \"Báo Lao Động\",\n",
    "        \"base_url\": \"https://laodong.vn\",\n",
    "        \"rss_feeds\": [\n",
    "            \"https://laodong.vn/rss/giao-thong.rss\",\n",
    "            \"https://laodong.vn/rss/xa-hoi.rss\"\n",
    "        ],\n",
    "        \"list_urls\": [\n",
    "            \"https://laodong.vn/giao-thong/\",\n",
    "            \"https://laodong.vn/giao-thong/trang-{page}.htm\"  # Pagination\n",
    "        ],\n",
    "        \"selectors\": {\n",
    "            \"article_links\": \"h3.title-news a, a[href*='/giao-thong/']\",\n",
    "            \"title\": \"h1.detail__title, h1\",\n",
    "            \"content\": \"div.detail-content-body, div.detail__content\",\n",
    "            \"date\": \"span.time-update, time\"\n",
    "        },\n",
    "        \"date_format\": \"%d/%m/%Y %H:%M\"\n",
    "    },\n",
    "    \"vnexpress\": {\n",
    "        \"name\": \"VnExpress\",\n",
    "        \"base_url\": \"https://vnexpress.net\",\n",
    "        \"rss_feeds\": [\"https://vnexpress.net/rss/giao-thong.rss\"],\n",
    "        \"list_urls\": [\n",
    "            \"https://vnexpress.net/giao-thong\",\n",
    "            \"https://vnexpress.net/giao-thong-p{page}\"\n",
    "        ],\n",
    "        \"selectors\": {\n",
    "            \"article_links\": \"h3.title-news a, h2.title-news a\",\n",
    "            \"title\": \"h1.title-detail\",\n",
    "            \"content\": \"article.fck_detail, div.fck_detail\",\n",
    "            \"date\": \"span.date\"\n",
    "        },\n",
    "        \"date_format\": \"%d/%m/%Y, %H:%M\"\n",
    "    },\n",
    "    \"tuoitre\": {\n",
    "        \"name\": \"Báo Tuổi Trẻ\",\n",
    "        \"base_url\": \"https://tuoitre.vn\",\n",
    "        \"rss_feeds\": [\"https://tuoitre.vn/rss/giao-thong.rss\"],\n",
    "        \"list_urls\": [\n",
    "            \"https://tuoitre.vn/giao-thong.htm\",\n",
    "            \"https://tuoitre.vn/giao-thong/trang-{page}.htm\"\n",
    "        ],\n",
    "        \"selectors\": {\n",
    "            \"article_links\": \"h3.title-news a, a[href*='/giao-thong/']\",\n",
    "            \"title\": \"h1.detail-title\",\n",
    "            \"content\": \"div.detail-content, div#main-detail-content\",\n",
    "            \"date\": \"div.date-time\"\n",
    "        },\n",
    "        \"date_format\": \"%d/%m/%Y %H:%M\"\n",
    "    },\n",
    "    \"hochiminhcity\": {\n",
    "        \"name\": \"Sở GTVT TP.HCM\",\n",
    "        \"base_url\": \"https://giaothong.hochiminhcity.gov.vn\",\n",
    "        \"list_urls\": [\n",
    "            \"https://giaothong.hochiminhcity.gov.vn/render/rendertintuc.aspx?CateID=1460\"\n",
    "        ],\n",
    "        \"selectors\": {\n",
    "            \"article_links\": \"a[href*='detail']\",\n",
    "            \"title\": \"h1, .title-detail, h1.article-title\",\n",
    "            \"content\": \"div.content-detail, div.article-content, article, div#content\",\n",
    "            \"date\": \"span.date, div.date-time\"\n",
    "        },\n",
    "        \"date_format\": \"%d/%m/%Y\"\n",
    "    },\n",
    "    \"thanhnien\": {\n",
    "        \"name\": \"Báo Thanh Niên\",\n",
    "        \"base_url\": \"https://thanhnien.vn\",\n",
    "        \"rss_feeds\": [\"https://thanhnien.vn/rss/giao-thong.rss\"],\n",
    "        \"list_urls\": [\n",
    "            \"https://thanhnien.vn/giao-thong/\",\n",
    "            \"https://thanhnien.vn/giao-thong/trang-{page}.html\"\n",
    "        ],\n",
    "        \"selectors\": {\n",
    "            \"article_links\": \"h2.story__heading a, a.story__link\",\n",
    "            \"title\": \"h1.detail-title\",\n",
    "            \"content\": \"div.detail-content, #contentdetail\",\n",
    "            \"date\": \"time, span.time\"\n",
    "        },\n",
    "        \"date_format\": \"%d/%m/%Y %H:%M\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# ==================== KẾT NỐI DATABASE ====================\n",
    "\n",
    "try:\n",
    "    client = MongoClient(\"mongodb://localhost:27017/\", serverSelectionTimeoutMS=5000)\n",
    "    client.server_info()\n",
    "    db = client[\"hcmc_traffic_intelligence\"]\n",
    "    articles_col = db[\"articles\"]\n",
    "    categories_col = db[\"categories\"]\n",
    "    summaries_col = db[\"summaries\"]\n",
    "    \n",
    "    # Index nâng cao\n",
    "    articles_col.create_index([(\"url\", 1)], unique=True)\n",
    "    articles_col.create_index([(\"published_date\", -1)])\n",
    "    articles_col.create_index([(\"source\", 1), (\"published_date\", -1)])\n",
    "    articles_col.create_index([(\"category\", 1)])\n",
    "    articles_col.create_index([(\"is_hcmc\", 1)])\n",
    "    articles_col.create_index([(\"content_hash\", 1)])\n",
    "    articles_col.create_index([\n",
    "        (\"title\", \"text\"), \n",
    "        (\"content\", \"text\"), \n",
    "        (\"summary\", \"text\")\n",
    "    ])  # Full-text search\n",
    "    \n",
    "    print(\"✓ Đã kết nối MongoDB với full-text search\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Lỗi MongoDB: {e}\")\n",
    "    articles_col = None\n",
    "\n",
    "# ==================== THUẬT TOÁN TÓM TẮT NÂNG CAO ====================\n",
    "\n",
    "class AdvancedSummarizer:\n",
    "    \"\"\"Thuật toán tóm tắt sử dụng TextRank + TF-IDF\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def preprocess_text(text: str) -> List[str]:\n",
    "        \"\"\"Tách câu và làm sạch\"\"\"\n",
    "        # Tách câu theo dấu câu\n",
    "        sentences = re.split(r'[.!?]\\s+', text)\n",
    "        \n",
    "        # Làm sạch\n",
    "        cleaned = []\n",
    "        for sent in sentences:\n",
    "            sent = sent.strip()\n",
    "            # Loại bỏ câu quá ngắn hoặc quá dài\n",
    "            if 20 < len(sent) < 300:\n",
    "                cleaned.append(sent)\n",
    "        \n",
    "        return cleaned\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_word_freq(sentences: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"Tính tần suất từ (TF)\"\"\"\n",
    "        words = []\n",
    "        \n",
    "        # Stopwords tiếng Việt cơ bản\n",
    "        stopwords = {\n",
    "            'và', 'của', 'có', 'được', 'là', 'trong', 'với', 'cho', 'các', \n",
    "            'một', 'này', 'đã', 'từ', 'những', 'để', 'người', 'không', 'như',\n",
    "            'về', 'theo', 'năm', 'tại', 'đến', 'khi', 'ngày', 'trên', 'sau',\n",
    "            'vào', 'thì', 'sẽ', 'ra', 'đang', 'nên', 'bị', 'hay', 'nhưng'\n",
    "        }\n",
    "        \n",
    "        for sent in sentences:\n",
    "            # Tách từ đơn giản (có thể dùng pyvi để tách từ tốt hơn)\n",
    "            tokens = re.findall(r'\\w+', sent.lower())\n",
    "            words.extend([w for w in tokens if w not in stopwords and len(w) > 2])\n",
    "        \n",
    "        # Tính tần suất\n",
    "        word_count = Counter(words)\n",
    "        max_freq = max(word_count.values()) if word_count else 1\n",
    "        \n",
    "        return {word: count/max_freq for word, count in word_count.items()}\n",
    "    \n",
    "    @staticmethod\n",
    "    def sentence_similarity(sent1: str, sent2: str, word_freq: Dict[str, float]) -> float:\n",
    "        \"\"\"Tính độ tương đồng giữa 2 câu\"\"\"\n",
    "        words1 = set(re.findall(r'\\w+', sent1.lower()))\n",
    "        words2 = set(re.findall(r'\\w+', sent2.lower()))\n",
    "        \n",
    "        if not words1 or not words2:\n",
    "            return 0.0\n",
    "        \n",
    "        # Tính số từ chung có trọng số\n",
    "        common_words = words1.intersection(words2)\n",
    "        score = sum(word_freq.get(word, 0) for word in common_words)\n",
    "        \n",
    "        # Normalize\n",
    "        return score / (math.log(len(words1)) + math.log(len(words2)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def textrank(sentences: List[str], word_freq: Dict[str, float], top_n: int = 3) -> List[str]:\n",
    "        \"\"\"TextRank algorithm để chọn câu quan trọng\"\"\"\n",
    "        n = len(sentences)\n",
    "        if n == 0:\n",
    "            return []\n",
    "        \n",
    "        # Build similarity matrix\n",
    "        similarity_matrix = [[0.0] * n for _ in range(n)]\n",
    "        \n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                if i != j:\n",
    "                    similarity_matrix[i][j] = AdvancedSummarizer.sentence_similarity(\n",
    "                        sentences[i], sentences[j], word_freq\n",
    "                    )\n",
    "        \n",
    "        # PageRank algorithm\n",
    "        scores = [1.0] * n\n",
    "        damping = 0.85\n",
    "        \n",
    "        for _ in range(10):  # 10 iterations\n",
    "            new_scores = [0.0] * n\n",
    "            \n",
    "            for i in range(n):\n",
    "                rank_sum = sum(\n",
    "                    similarity_matrix[j][i] * scores[j] / \n",
    "                    (sum(similarity_matrix[j]) + 1e-8)\n",
    "                    for j in range(n) if i != j\n",
    "                )\n",
    "                new_scores[i] = (1 - damping) + damping * rank_sum\n",
    "            \n",
    "            scores = new_scores\n",
    "        \n",
    "        # Chọn top câu và giữ thứ tự xuất hiện\n",
    "        ranked = [(score, idx, sent) for idx, (score, sent) in enumerate(zip(scores, sentences))]\n",
    "        ranked.sort(reverse=True)\n",
    "        \n",
    "        top_sentences = sorted(ranked[:top_n], key=lambda x: x[1])\n",
    "        \n",
    "        return [sent for _, _, sent in top_sentences]\n",
    "    \n",
    "    @staticmethod\n",
    "    def summarize(text: str, num_sentences: int = 3, method: str = \"textrank\") -> str:\n",
    "        \"\"\"Tóm tắt văn bản\"\"\"\n",
    "        sentences = AdvancedSummarizer.preprocess_text(text)\n",
    "        \n",
    "        if len(sentences) <= num_sentences:\n",
    "            return \". \".join(sentences) + \".\"\n",
    "        \n",
    "        if method == \"textrank\":\n",
    "            word_freq = AdvancedSummarizer.calculate_word_freq(sentences)\n",
    "            summary_sents = AdvancedSummarizer.textrank(sentences, word_freq, num_sentences)\n",
    "        else:\n",
    "            # Fallback: lấy câu đầu\n",
    "            summary_sents = sentences[:num_sentences]\n",
    "        \n",
    "        return \". \".join(summary_sents) + \".\"\n",
    "\n",
    "# ==================== PHÂN LOẠI TỰ ĐỘNG ====================\n",
    "\n",
    "def categorize_article(title: str, content: str) -> str:\n",
    "    \"\"\"Phân loại bài viết theo nội dung\"\"\"\n",
    "    text = (title + \" \" + content).lower()\n",
    "    \n",
    "    scores = {}\n",
    "    for category, keywords in TRAFFIC_KEYWORDS.items():\n",
    "        score = sum(1 for kw in keywords if kw in text)\n",
    "        scores[category] = score\n",
    "    \n",
    "    if max(scores.values()) == 0:\n",
    "        return \"general\"\n",
    "    \n",
    "    return max(scores, key=scores.get)\n",
    "\n",
    "def is_hcmc_related(title: str, content: str) -> bool:\n",
    "    \"\"\"Kiểm tra có liên quan đến TP.HCM không\"\"\"\n",
    "    text = (title + \" \" + content).lower()\n",
    "    return any(kw in text for kw in HCMC_KEYWORDS)\n",
    "\n",
    "# ==================== SCRAPING NÂNG CAO ====================\n",
    "\n",
    "def init_driver():\n",
    "    \"\"\"Khởi tạo Edge driver\"\"\"\n",
    "    edge_options = webdriver.EdgeOptions()\n",
    "    edge_options.add_argument(\"--headless\")\n",
    "    edge_options.add_argument(\"--no-sandbox\")\n",
    "    edge_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    edge_options.add_argument(\"--disable-gpu\")\n",
    "    edge_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64)\")\n",
    "    \n",
    "    driver = webdriver.Edge(options=edge_options)\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "def parse_date(date_str: str, date_format: str) -> Optional[datetime]:\n",
    "    \"\"\"Parse ngày tháng từ nhiều format\"\"\"\n",
    "    if not date_str:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Thử format được cung cấp\n",
    "        return datetime.strptime(date_str.strip(), date_format)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Thử các format phổ biến\n",
    "    formats = [\n",
    "        \"%d/%m/%Y %H:%M\",\n",
    "        \"%d/%m/%Y\",\n",
    "        \"%d-%m-%Y\",\n",
    "        \"%Y-%m-%d\",\n",
    "        \"%d.%m.%Y\"\n",
    "    ]\n",
    "    \n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            return datetime.strptime(date_str.strip(), fmt)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return None\n",
    "\n",
    "def scrape_from_rss(source_key: str) -> List[str]:\n",
    "    \"\"\"Lấy URLs từ RSS feed\"\"\"\n",
    "    source = NEWS_SOURCES[source_key]\n",
    "    urls = []\n",
    "    \n",
    "    if \"rss_feeds\" not in source:\n",
    "        return []\n",
    "    \n",
    "    for rss_url in source[\"rss_feeds\"]:\n",
    "        try:\n",
    "            print(f\"  📡 Đang đọc RSS: {rss_url}\")\n",
    "            feed = feedparser.parse(rss_url)\n",
    "            \n",
    "            for entry in feed.entries[:20]:  # Lấy 20 bài mới nhất\n",
    "                url = entry.get(\"link\", \"\")\n",
    "                pub_date = entry.get(\"published_parsed\", None)\n",
    "                \n",
    "                # Kiểm tra thời gian\n",
    "                if pub_date:\n",
    "                    pub_datetime = datetime(*pub_date[:6])\n",
    "                    if pub_datetime >= START_DATE:\n",
    "                        urls.append(url)\n",
    "                else:\n",
    "                    urls.append(url)  # Không có ngày thì lấy luôn\n",
    "            \n",
    "            print(f\"  ✓ Tìm thấy {len(urls)} URLs từ RSS\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Lỗi đọc RSS: {e}\")\n",
    "    \n",
    "    return urls\n",
    "\n",
    "def scrape_from_list_page(source_key: str, max_pages: int = 3) -> List[str]:\n",
    "    \"\"\"Lấy URLs từ trang danh sách với pagination\"\"\"\n",
    "    source = NEWS_SOURCES[source_key]\n",
    "    driver = None\n",
    "    all_urls = set()\n",
    "    \n",
    "    try:\n",
    "        driver = init_driver()\n",
    "        \n",
    "        for page in range(1, max_pages + 1):\n",
    "            # Lấy URL của trang\n",
    "            if len(source[\"list_urls\"]) > 1 and page > 1:\n",
    "                url = source[\"list_urls\"][1].format(page=page)\n",
    "            else:\n",
    "                url = source[\"list_urls\"][0]\n",
    "                if page > 1:\n",
    "                    break  # Không có pagination\n",
    "            \n",
    "            print(f\"  📄 Trang {page}: {url[:60]}...\")\n",
    "            driver.get(url)\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # Lấy links\n",
    "            links = driver.find_elements(By.CSS_SELECTOR, source[\"selectors\"][\"article_links\"])\n",
    "            \n",
    "            page_urls = 0\n",
    "            for link in links:\n",
    "                href = link.get_attribute(\"href\")\n",
    "                if href and href.startswith(\"http\"):\n",
    "                    all_urls.add(href)\n",
    "                    page_urls += 1\n",
    "            \n",
    "            print(f\"  ✓ Tìm thấy {page_urls} URLs\")\n",
    "            \n",
    "            if page_urls == 0:\n",
    "                break  # Không còn bài mới\n",
    "        \n",
    "        return list(all_urls)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Lỗi scrape list: {e}\")\n",
    "        return list(all_urls)\n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "\n",
    "def scrape_article_advanced(url: str, source_key: str) -> Optional[Dict]:\n",
    "    \"\"\"Scrape 1 bài viết với metadata đầy đủ\"\"\"\n",
    "    source = NEWS_SOURCES[source_key]\n",
    "    driver = None\n",
    "    \n",
    "    try:\n",
    "        driver = init_driver()\n",
    "        driver.get(url)\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        \n",
    "        # Lấy title\n",
    "        try:\n",
    "            title_elem = wait.until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, source[\"selectors\"][\"title\"]))\n",
    "            )\n",
    "            title = title_elem.text.strip()\n",
    "        except:\n",
    "            title = driver.title\n",
    "        \n",
    "        # Lấy content\n",
    "        try:\n",
    "            content_elem = driver.find_element(By.CSS_SELECTOR, source[\"selectors\"][\"content\"])\n",
    "            content = content_elem.text.strip()\n",
    "        except:\n",
    "            content = driver.find_element(By.TAG_NAME, \"body\").text.strip()\n",
    "        \n",
    "        # Lấy ngày đăng\n",
    "        pub_date = None\n",
    "        if \"date\" in source[\"selectors\"]:\n",
    "            try:\n",
    "                date_elem = driver.find_element(By.CSS_SELECTOR, source[\"selectors\"][\"date\"])\n",
    "                date_str = date_elem.text.strip()\n",
    "                pub_date = parse_date(date_str, source.get(\"date_format\", \"%d/%m/%Y\"))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if pub_date is None:\n",
    "            pub_date = datetime.now()\n",
    "        \n",
    "        # Kiểm tra thời gian\n",
    "        if pub_date < START_DATE:\n",
    "            print(f\"  ⏭️  Bài cũ ({pub_date.date()}), bỏ qua\")\n",
    "            return None\n",
    "        \n",
    "        # Làm sạch content\n",
    "        lines = [l.strip() for l in content.split(\"\\n\") if len(l.strip()) > 30]\n",
    "        content = \" \".join(lines)\n",
    "        \n",
    "        # Kiểm tra TP.HCM\n",
    "        is_hcmc = is_hcmc_related(title, content)\n",
    "        if not is_hcmc:\n",
    "            print(f\"  ⏭️  Không liên quan TP.HCM\")\n",
    "            return None\n",
    "        \n",
    "        # Phân loại\n",
    "        category = categorize_article(title, content)\n",
    "        \n",
    "        # Tóm tắt nâng cao\n",
    "        summary = AdvancedSummarizer.summarize(content, num_sentences=3, method=\"textrank\")\n",
    "        \n",
    "        return {\n",
    "            \"url\": url,\n",
    "            \"source\": source_key,\n",
    "            \"source_name\": source[\"name\"],\n",
    "            \"title\": title,\n",
    "            \"content\": content,\n",
    "            \"summary\": summary,\n",
    "            \"category\": category,\n",
    "            \"is_hcmc\": is_hcmc,\n",
    "            \"content_hash\": hashlib.md5(content.encode()).hexdigest(),\n",
    "            \"published_date\": pub_date,\n",
    "            \"scraped_at\": datetime.now()\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Lỗi scrape: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "\n",
    "# ==================== SCRAPE CAMPAIGN ====================\n",
    "\n",
    "def scrape_source_complete(source_key: str, max_articles: int = 50):\n",
    "    \"\"\"Scrape hoàn chỉnh 1 nguồn (RSS + List pages + Date filter)\"\"\"\n",
    "    source = NEWS_SOURCES[source_key]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"🔍 BẮT ĐẦU: {source['name']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Bước 1: Lấy URLs từ RSS\n",
    "    print(f\"\\n📡 Bước 1: Lấy URLs từ RSS...\")\n",
    "    rss_urls = scrape_from_rss(source_key)\n",
    "    \n",
    "    # Bước 2: Lấy URLs từ list pages\n",
    "    print(f\"\\n📄 Bước 2: Lấy URLs từ trang danh sách...\")\n",
    "    list_urls = scrape_from_list_page(source_key, max_pages=5)\n",
    "    \n",
    "    # Gộp và loại trùng\n",
    "    all_urls = list(set(rss_urls + list_urls))\n",
    "    print(f\"\\n📊 Tổng cộng: {len(all_urls)} URLs duy nhất\")\n",
    "    \n",
    "    # Bước 3: Scrape từng bài\n",
    "    print(f\"\\n📰 Bước 3: Scrape chi tiết...\")\n",
    "    \n",
    "    stats = {\n",
    "        \"total_urls\": len(all_urls),\n",
    "        \"scraped\": 0,\n",
    "        \"saved\": 0,\n",
    "        \"duplicates\": 0,\n",
    "        \"not_hcmc\": 0,\n",
    "        \"old_articles\": 0,\n",
    "        \"errors\": 0\n",
    "    }\n",
    "    \n",
    "    for idx, url in enumerate(all_urls[:max_articles], 1):\n",
    "        print(f\"\\n[{idx}/{min(len(all_urls), max_articles)}] {url[:70]}...\")\n",
    "        \n",
    "        # Kiểm tra đã tồn tại\n",
    "        if articles_col is not None:\n",
    "            existing = articles_col.find_one({\"url\": url})\n",
    "            if existing is not None:\n",
    "                print(f\"  ⏭️  Đã tồn tại trong DB\")\n",
    "                stats[\"duplicates\"] += 1\n",
    "                continue\n",
    "        \n",
    "        # Scrape\n",
    "        article = scrape_article_advanced(url, source_key)\n",
    "        \n",
    "        if article is None:\n",
    "            stats[\"errors\"] += 1\n",
    "            continue\n",
    "        \n",
    "        stats[\"scraped\"] += 1\n",
    "        \n",
    "        # Lưu vào DB\n",
    "        if articles_col is not None:\n",
    "            try:\n",
    "                articles_col.insert_one(article)\n",
    "                print(f\"  ✅ Đã lưu: {article['title'][:50]}...\")\n",
    "                print(f\"  📂 Danh mục: {article['category']}\")\n",
    "                print(f\"  📅 Ngày: {article['published_date'].date()}\")\n",
    "                stats[\"saved\"] += 1\n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ Lỗi lưu DB: {e}\")\n",
    "                stats[\"duplicates\"] += 1\n",
    "        \n",
    "        time.sleep(2)  # Tránh spam\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def scrape_all_sources_2025(max_per_source: int = 50):\n",
    "    \"\"\"Scrape tất cả nguồn với focus vào TP.HCM 2025\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"🚀 CHIẾN DỊCH SCRAPING: GIAO THÔNG TP.HCM 2025\")\n",
    "    print(f\"   Thời gian: {START_DATE.date()} → {CURRENT_DATE.date()}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    total_stats = {\n",
    "        \"sources\": {},\n",
    "        \"grand_total\": {\n",
    "            \"urls\": 0,\n",
    "            \"scraped\": 0,\n",
    "            \"saved\": 0,\n",
    "            \"duplicates\": 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for source_key in NEWS_SOURCES.keys():\n",
    "        stats = scrape_source_complete(source_key, max_per_source)\n",
    "        total_stats[\"sources\"][source_key] = stats\n",
    "        \n",
    "        total_stats[\"grand_total\"][\"urls\"] += stats[\"total_urls\"]\n",
    "        total_stats[\"grand_total\"][\"scraped\"] += stats[\"scraped\"]\n",
    "        total_stats[\"grand_total\"][\"saved\"] += stats[\"saved\"]\n",
    "        total_stats[\"grand_total\"][\"duplicates\"] += stats[\"duplicates\"]\n",
    "        \n",
    "        print(f\"\\n✅ Hoàn thành {NEWS_SOURCES[source_key]['name']}\")\n",
    "        print(f\"   Đã lưu: {stats['saved']} / {stats['scraped']} bài\")\n",
    "    \n",
    "    return total_stats\n",
    "\n",
    "# ==================== FASTAPI ====================\n",
    "\n",
    "app = FastAPI(title=\"TP.HCM Traffic Intelligence 2025\")\n",
    "\n",
    "@app.get(\"/\")\n",
    "def root():\n",
    "    return {\n",
    "        \"name\": \"TP.HCM Traffic News Intelligence System 2025\",\n",
    "        \"period\": f\"{START_DATE.date()} to {CURRENT_DATE.date()}\",\n",
    "        \"sources\": len(NEWS_SOURCES),\n",
    "        \"features\": [\n",
    "            \"Advanced TextRank summarization\",\n",
    "            \"Auto-categorization\",\n",
    "            \"HCMC-specific filtering\",\n",
    "            \"Full-text search\",\n",
    "            \"RSS + Pagination scraping\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "@app.get(\"/articles\")\n",
    "def list_articles(\n",
    "    category: Optional[str] = None,\n",
    "    source: Optional[str] = None,\n",
    "    from_date: Optional[str] = Query(None, description=\"YYYY-MM-DD\"),\n",
    "    to_date: Optional[str] = Query(None, description=\"YYYY-MM-DD\"),\n",
    "    search: Optional[str] = None,\n",
    "    limit: int = 20\n",
    "):\n",
    "    \"\"\"Danh sách bài viết với filter nâng cao\"\"\"\n",
    "    if articles_col is None:\n",
    "        raise HTTPException(503, \"Database không khả dụng\")\n",
    "    \n",
    "    query = {\"is_hcmc\": True}\n",
    "    \n",
    "    if category:\n",
    "        query[\"category\"] = category\n",
    "    \n",
    "    if source:\n",
    "        query[\"source\"] = source\n",
    "    \n",
    "    # Date range\n",
    "    date_query = {}\n",
    "    if from_date:\n",
    "        date_query[\"$gte\"] = datetime.strptime(from_date, \"%Y-%m-%d\")\n",
    "    if to_date:\n",
    "        date_query[\"$lte\"] = datetime.strptime(to_date, \"%Y-%m-%d\")\n",
    "    if date_query:\n",
    "        query[\"published_date\"] = date_query\n",
    "    \n",
    "    # Full-text search\n",
    "    if search:\n",
    "        query[\"$text\"] = {\"$search\": search}\n",
    "    \n",
    "    docs = articles_col.find(query).sort(\"published_date\", -1).limit(limit)\n",
    "    \n",
    "    articles = []\n",
    "    for doc in docs:\n",
    "        articles.append({\n",
    "            \"id\": str(doc[\"_id\"]),\n",
    "            \"title\": doc[\"title\"],\n",
    "            \"source\": doc[\"source_name\"],\n",
    "            \"category\": doc.get(\"category\", \"general\"),\n",
    "            \"published_date\": doc[\"published_date\"].isoformat(),\n",
    "            \"summary\": doc.get(\"summary\", \"\")[:200],\n",
    "            \"url\": doc[\"url\"]\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        \"articles\": articles,\n",
    "        \"count\": len(articles),\n",
    "        \"filters\": {\n",
    "            \"category\": category,\n",
    "            \"source\": source,\n",
    "            \"from\": from_date,\n",
    "            \"to\": to_date,\n",
    "            \"search\": search\n",
    "        }\n",
    "    }\n",
    "\n",
    "@app.get(\"/stats\")\n",
    "def get_statistics():\n",
    "    \"\"\"Thống kê chi tiết\"\"\"\n",
    "    if articles_col is None:\n",
    "        raise HTTPException(503, \"Database không khả dụng\")\n",
    "    \n",
    "    total = articles_col.count_documents({\"is_hcmc\": True})\n",
    "    \n",
    "    # By category\n",
    "    pipeline_category = [\n",
    "        {\"$match\": {\"is_hcmc\": True}},\n",
    "        {\"$group\": {\"_id\": \"$category\", \"count\": {\"$sum\": 1}}},\n",
    "        {\"$sort\": {\"count\": -1}}\n",
    "    ]\n",
    "    by_category = {doc[\"_id\"]: doc[\"count\"] for doc in articles_col.aggregate(pipeline_category)}\n",
    "    \n",
    "    # By source\n",
    "    pipeline_source = [\n",
    "        {\"$match\": {\"is_hcmc\": True}},\n",
    "        {\"$group\": {\"_id\": \"$source_name\", \"count\": {\"$sum\": 1}}},\n",
    "        {\"$sort\": {\"count\": -1}}\n",
    "    ]\n",
    "    by_source = {doc[\"_id\"]: doc[\"count\"] for doc in articles_col.aggregate(pipeline_source)}\n",
    "    \n",
    "    # By month\n",
    "    pipeline_monthly = [\n",
    "        {\"$match\": {\"is_hcmc\": True, \"published_date\": {\"$gte\": START_DATE}}},\n",
    "        {\"$group\": {\n",
    "            \"_id\": {\n",
    "                \"year\": {\"$year\": \"$published_date\"},\n",
    "                \"month\": {\"$month\": \"$published_date\"}\n",
    "            },\n",
    "            \"count\": {\"$sum\": 1}\n",
    "        }},\n",
    "        {\"$sort\": {\"_id.year\": 1, \"_id.month\": 1}}\n",
    "    ]\n",
    "    monthly_data = []\n",
    "    for doc in articles_col.aggregate(pipeline_monthly):\n",
    "        monthly_data.append({\n",
    "            \"month\": f\"{doc['_id']['year']}-{doc['_id']['month']:02d}\",\n",
    "            \"count\": doc[\"count\"]\n",
    "        })\n",
    "    \n",
    "    # Recent articles\n",
    "    recent_7days = articles_col.count_documents({\n",
    "        \"is_hcmc\": True,\n",
    "        \"published_date\": {\"$gte\": datetime.now() - timedelta(days=7)}\n",
    "    })\n",
    "    \n",
    "    return {\n",
    "        \"total_articles\": total,\n",
    "        \"articles_last_7days\": recent_7days,\n",
    "        \"by_category\": by_category,\n",
    "        \"by_source\": by_source,\n",
    "        \"monthly_trend\": monthly_data,\n",
    "        \"period\": {\n",
    "            \"start\": START_DATE.isoformat(),\n",
    "            \"end\": CURRENT_DATE.isoformat()\n",
    "        }\n",
    "    }\n",
    "\n",
    "@app.get(\"/articles/{article_id}\")\n",
    "def get_article_detail(article_id: str):\n",
    "    \"\"\"Chi tiết bài viết với summary nâng cao\"\"\"\n",
    "    if articles_col is None:\n",
    "        raise HTTPException(503, \"Database không khả dụng\")\n",
    "    \n",
    "    doc = articles_col.find_one({\"_id\": ObjectId(article_id)})\n",
    "    if doc is None:\n",
    "        raise HTTPException(404, \"Không tìm thấy bài viết\")\n",
    "    \n",
    "    return {\n",
    "        \"id\": str(doc[\"_id\"]),\n",
    "        \"title\": doc[\"title\"],\n",
    "        \"source\": doc[\"source_name\"],\n",
    "        \"category\": doc.get(\"category\", \"general\"),\n",
    "        \"url\": doc[\"url\"],\n",
    "        \"content\": doc[\"content\"],\n",
    "        \"summary\": doc.get(\"summary\", \"\"),\n",
    "        \"published_date\": doc[\"published_date\"].isoformat(),\n",
    "        \"scraped_at\": doc[\"scraped_at\"].isoformat(),\n",
    "        \"is_hcmc\": doc.get(\"is_hcmc\", False)\n",
    "    }\n",
    "\n",
    "@app.get(\"/categories\")\n",
    "def list_categories():\n",
    "    \"\"\"Danh sách các danh mục\"\"\"\n",
    "    return {\n",
    "        \"categories\": [\n",
    "            {\"key\": \"projects\", \"name\": \"Dự án\", \"keywords\": TRAFFIC_KEYWORDS[\"projects\"]},\n",
    "            {\"key\": \"infrastructure\", \"name\": \"Hạ tầng\", \"keywords\": TRAFFIC_KEYWORDS[\"infrastructure\"]},\n",
    "            {\"key\": \"issues\", \"name\": \"Vấn đề\", \"keywords\": TRAFFIC_KEYWORDS[\"issues\"]},\n",
    "            {\"key\": \"planning\", \"name\": \"Quy hoạch\", \"keywords\": TRAFFIC_KEYWORDS[\"planning\"]},\n",
    "            {\"key\": \"transport\", \"name\": \"Vận tải\", \"keywords\": TRAFFIC_KEYWORDS[\"transport\"]},\n",
    "            {\"key\": \"general\", \"name\": \"Chung\", \"keywords\": []}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "@app.post(\"/scrape/all\")\n",
    "def trigger_full_scrape(background_tasks: BackgroundTasks, max_per_source: int = 50):\n",
    "    \"\"\"Khởi động scraping toàn bộ (chạy background)\"\"\"\n",
    "    background_tasks.add_task(scrape_all_sources_2025, max_per_source)\n",
    "    return {\n",
    "        \"status\": \"started\",\n",
    "        \"message\": f\"Đang scrape tất cả nguồn (tối đa {max_per_source} bài/nguồn)\",\n",
    "        \"target_period\": f\"{START_DATE.date()} to {CURRENT_DATE.date()}\"\n",
    "    }\n",
    "\n",
    "@app.post(\"/scrape/{source_key}\")\n",
    "def trigger_source_scrape(source_key: str, max_articles: int = 50):\n",
    "    \"\"\"Scrape 1 nguồn cụ thể\"\"\"\n",
    "    if source_key not in NEWS_SOURCES:\n",
    "        raise HTTPException(404, f\"Nguồn '{source_key}' không tồn tại\")\n",
    "    \n",
    "    stats = scrape_source_complete(source_key, max_articles)\n",
    "    \n",
    "    return {\n",
    "        \"source\": NEWS_SOURCES[source_key][\"name\"],\n",
    "        \"stats\": stats\n",
    "    }\n",
    "\n",
    "@app.get(\"/search\")\n",
    "def search_articles(\n",
    "    q: str = Query(..., description=\"Từ khóa tìm kiếm\"),\n",
    "    limit: int = 20\n",
    "):\n",
    "    \"\"\"Tìm kiếm full-text\"\"\"\n",
    "    if articles_col is None:\n",
    "        raise HTTPException(503, \"Database không khả dụng\")\n",
    "    \n",
    "    docs = articles_col.find(\n",
    "        {\"$text\": {\"$search\": q}, \"is_hcmc\": True},\n",
    "        {\"score\": {\"$meta\": \"textScore\"}}\n",
    "    ).sort([(\"score\", {\"$meta\": \"textScore\"})]).limit(limit)\n",
    "    \n",
    "    results = []\n",
    "    for doc in docs:\n",
    "        results.append({\n",
    "            \"id\": str(doc[\"_id\"]),\n",
    "            \"title\": doc[\"title\"],\n",
    "            \"summary\": doc.get(\"summary\", \"\")[:200],\n",
    "            \"source\": doc[\"source_name\"],\n",
    "            \"category\": doc.get(\"category\", \"general\"),\n",
    "            \"published_date\": doc[\"published_date\"].isoformat(),\n",
    "            \"relevance_score\": doc.get(\"score\", 0),\n",
    "            \"url\": doc[\"url\"]\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        \"query\": q,\n",
    "        \"results\": results,\n",
    "        \"count\": len(results)\n",
    "    }\n",
    "\n",
    "@app.get(\"/trending\")\n",
    "def get_trending_topics(days: int = 7, top_n: int = 10):\n",
    "    \"\"\"Các chủ đề nổi bật trong N ngày qua\"\"\"\n",
    "    if articles_col is None:\n",
    "        raise HTTPException(503, \"Database không khả dụng\")\n",
    "    \n",
    "    # Lấy bài viết gần đây\n",
    "    recent_docs = articles_col.find({\n",
    "        \"is_hcmc\": True,\n",
    "        \"published_date\": {\"$gte\": datetime.now() - timedelta(days=days)}\n",
    "    })\n",
    "    \n",
    "    # Đếm từ khóa\n",
    "    all_text = \"\"\n",
    "    for doc in recent_docs:\n",
    "        all_text += doc[\"title\"] + \" \" + doc[\"content\"]\n",
    "    \n",
    "    # Trích xuất từ khóa\n",
    "    words = re.findall(r'\\w+', all_text.lower())\n",
    "    \n",
    "    # Stopwords\n",
    "    stopwords = {\n",
    "        'và', 'của', 'có', 'được', 'là', 'trong', 'với', 'cho', 'các', \n",
    "        'một', 'này', 'đã', 'từ', 'những', 'để', 'người', 'không', 'như',\n",
    "        'về', 'theo', 'năm', 'tại', 'đến', 'khi', 'ngày', 'trên', 'sau'\n",
    "    }\n",
    "    \n",
    "    words = [w for w in words if w not in stopwords and len(w) > 3]\n",
    "    \n",
    "    # Top keywords\n",
    "    keyword_freq = Counter(words).most_common(top_n)\n",
    "    \n",
    "    trending = [{\"keyword\": word, \"count\": count} for word, count in keyword_freq]\n",
    "    \n",
    "    return {\n",
    "        \"period_days\": days,\n",
    "        \"trending_keywords\": trending\n",
    "    }\n",
    "\n",
    "@app.post(\"/regenerate-summary/{article_id}\")\n",
    "def regenerate_summary(article_id: str, num_sentences: int = 3):\n",
    "    \"\"\"Tạo lại tóm tắt với thuật toán mới\"\"\"\n",
    "    if articles_col is None:\n",
    "        raise HTTPException(503, \"Database không khả dụng\")\n",
    "    \n",
    "    doc = articles_col.find_one({\"_id\": ObjectId(article_id)})\n",
    "    if doc is None:\n",
    "        raise HTTPException(404, \"Không tìm thấy bài viết\")\n",
    "    \n",
    "    # Tạo summary mới\n",
    "    new_summary = AdvancedSummarizer.summarize(\n",
    "        doc[\"content\"], \n",
    "        num_sentences=num_sentences,\n",
    "        method=\"textrank\"\n",
    "    )\n",
    "    \n",
    "    # Cập nhật DB\n",
    "    articles_col.update_one(\n",
    "        {\"_id\": doc[\"_id\"]},\n",
    "        {\"$set\": {\"summary\": new_summary, \"summary_updated_at\": datetime.now()}}\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"id\": str(doc[\"_id\"]),\n",
    "        \"old_summary\": doc.get(\"summary\", \"\"),\n",
    "        \"new_summary\": new_summary\n",
    "    }\n",
    "\n",
    "@app.get(\"/export\")\n",
    "def export_articles(\n",
    "    format: str = Query(\"json\", description=\"json hoặc csv\"),\n",
    "    category: Optional[str] = None,\n",
    "    from_date: Optional[str] = None,\n",
    "    to_date: Optional[str] = None\n",
    "):\n",
    "    \"\"\"Export dữ liệu\"\"\"\n",
    "    if articles_col is None:\n",
    "        raise HTTPException(503, \"Database không khả dụng\")\n",
    "    \n",
    "    query = {\"is_hcmc\": True}\n",
    "    \n",
    "    if category:\n",
    "        query[\"category\"] = category\n",
    "    \n",
    "    date_query = {}\n",
    "    if from_date:\n",
    "        date_query[\"$gte\"] = datetime.strptime(from_date, \"%Y-%m-%d\")\n",
    "    if to_date:\n",
    "        date_query[\"$lte\"] = datetime.strptime(to_date, \"%Y-%m-%d\")\n",
    "    if date_query:\n",
    "        query[\"published_date\"] = date_query\n",
    "    \n",
    "    docs = articles_col.find(query).sort(\"published_date\", -1)\n",
    "    \n",
    "    articles = []\n",
    "    for doc in docs:\n",
    "        articles.append({\n",
    "            \"id\": str(doc[\"_id\"]),\n",
    "            \"title\": doc[\"title\"],\n",
    "            \"source\": doc[\"source_name\"],\n",
    "            \"category\": doc.get(\"category\", \"general\"),\n",
    "            \"summary\": doc.get(\"summary\", \"\"),\n",
    "            \"published_date\": doc[\"published_date\"].isoformat(),\n",
    "            \"url\": doc[\"url\"]\n",
    "        })\n",
    "    \n",
    "    if format == \"csv\":\n",
    "        # Simple CSV format\n",
    "        csv_data = \"ID,Title,Source,Category,Date,URL\\n\"\n",
    "        for art in articles:\n",
    "            csv_data += f\"{art['id']},\\\"{art['title']}\\\",{art['source']},{art['category']},{art['published_date']},{art['url']}\\n\"\n",
    "        \n",
    "        return {\"format\": \"csv\", \"data\": csv_data, \"count\": len(articles)}\n",
    "    \n",
    "    return {\"format\": \"json\", \"articles\": articles, \"count\": len(articles)}\n",
    "\n",
    "# ==================== CHẠY NHANH (SIMPLE MODE) ====================\n",
    "\n",
    "def quick_scrape(num_articles_per_source: int = 5):\n",
    "    \"\"\"Chạy scrape đơn giản - không cần sys.argv\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"🚀 TP.HCM TRAFFIC NEWS INTELLIGENCE SYSTEM 2025\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"📅 Thời gian: {START_DATE.date()} → {CURRENT_DATE.date()}\")\n",
    "    print(f\"📰 Đang scrape {num_articles_per_source} bài/nguồn...\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    results = scrape_all_sources_2025(max_per_source=num_articles_per_source)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"✅ HOÀN THÀNH!\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"📊 Kết quả:\")\n",
    "    print(f\"   • Tổng URLs: {results['grand_total']['urls']}\")\n",
    "    print(f\"   • Đã scrape: {results['grand_total']['scraped']}\")\n",
    "    print(f\"   • Đã lưu DB: {results['grand_total']['saved']}\")\n",
    "    print(f\"   • Trùng lặp: {results['grand_total']['duplicates']}\")\n",
    "    \n",
    "    for source_key, stats in results[\"sources\"].items():\n",
    "        name = NEWS_SOURCES[source_key][\"name\"]\n",
    "        print(f\"\\n   🔹 {name}: {stats['saved']} bài\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"💡 Xem dữ liệu:\")\n",
    "    print(f\"   python {__file__} stats\")\n",
    "    print(f\"   hoặc chạy API: uvicorn {__file__.replace('.py', '')}:app --reload\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def show_stats_simple():\n",
    "    \"\"\"Hiển thị thống kê đơn giản\"\"\"\n",
    "    if articles_col is None:\n",
    "        print(\"❌ Không kết nối được MongoDB\")\n",
    "        return\n",
    "    \n",
    "    total = articles_col.count_documents({\"is_hcmc\": True})\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"📊 THỐNG KÊ TỔNG QUAN\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Tổng số bài viết về TP.HCM: {total}\")\n",
    "    \n",
    "    # By category\n",
    "    print(f\"\\n📂 Theo danh mục:\")\n",
    "    for category in [\"projects\", \"infrastructure\", \"issues\", \"planning\", \"transport\", \"general\"]:\n",
    "        count = articles_col.count_documents({\"is_hcmc\": True, \"category\": category})\n",
    "        if count > 0:\n",
    "            cat_name = {\n",
    "                \"projects\": \"Dự án\",\n",
    "                \"infrastructure\": \"Hạ tầng\", \n",
    "                \"issues\": \"Vấn đề\",\n",
    "                \"planning\": \"Quy hoạch\",\n",
    "                \"transport\": \"Vận tải\",\n",
    "                \"general\": \"Chung\"\n",
    "            }.get(category, category)\n",
    "            print(f\"   • {cat_name}: {count} bài\")\n",
    "    \n",
    "    # By source\n",
    "    print(f\"\\n📰 Theo nguồn:\")\n",
    "    for source_key, source in NEWS_SOURCES.items():\n",
    "        count = articles_col.count_documents({\"is_hcmc\": True, \"source\": source_key})\n",
    "        if count > 0:\n",
    "            print(f\"   • {source['name']}: {count} bài\")\n",
    "    \n",
    "    # Recent\n",
    "    recent = articles_col.count_documents({\n",
    "        \"is_hcmc\": True,\n",
    "        \"published_date\": {\"$gte\": datetime.now() - timedelta(days=7)}\n",
    "    })\n",
    "    print(f\"\\n📅 7 ngày gần đây: {recent} bài\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\\n\")\n",
    "\n",
    "# ==================== CLI MODE ====================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    \n",
    "    # Kiểm tra có argument không\n",
    "    if len(sys.argv) > 1:\n",
    "        command = sys.argv[1]\n",
    "        \n",
    "        if command == \"scrape\":\n",
    "            # Scrape all sources\n",
    "            max_articles = int(sys.argv[2]) if len(sys.argv) > 2 else 50\n",
    "            results = scrape_all_sources_2025(max_per_source=max_articles)\n",
    "            \n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"📊 KẾT QUẢ TỔNG HỢP\")\n",
    "            print(f\"{'='*70}\")\n",
    "            print(f\"Tổng URLs: {results['grand_total']['urls']}\")\n",
    "            print(f\"Đã scrape: {results['grand_total']['scraped']}\")\n",
    "            print(f\"Đã lưu: {results['grand_total']['saved']}\")\n",
    "            print(f\"Trùng lặp: {results['grand_total']['duplicates']}\")\n",
    "            \n",
    "            print(f\"\\n📈 Chi tiết theo nguồn:\")\n",
    "            for source_key, stats in results[\"sources\"].items():\n",
    "                name = NEWS_SOURCES[source_key][\"name\"]\n",
    "                print(f\"\\n  🔹 {name}\")\n",
    "                print(f\"     URLs: {stats['total_urls']}\")\n",
    "                print(f\"     Đã lưu: {stats['saved']}/{stats['scraped']}\")\n",
    "                print(f\"     Không phải HCM: {stats.get('not_hcmc', 0)}\")\n",
    "                print(f\"     Bài cũ: {stats.get('old_articles', 0)}\")\n",
    "            \n",
    "            print(f\"\\n{'='*70}\")\n",
    "        \n",
    "        elif command == \"stats\":\n",
    "            # Show statistics\n",
    "            show_stats_simple()\n",
    "        \n",
    "        else:\n",
    "            print(f\"❌ Lệnh không hợp lệ: {command}\")\n",
    "            print(f\"Sử dụng: python {sys.argv[0]} [scrape|stats]\")\n",
    "    \n",
    "    else:\n",
    "        # Không có argument - chạy mode mặc định\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"🚀 TP.HCM TRAFFIC NEWS INTELLIGENCE SYSTEM 2025\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"📅 Thời gian: {START_DATE.date()} → {CURRENT_DATE.date()}\")\n",
    "        print(f\"📰 Nguồn tin: {len(NEWS_SOURCES)} báo\")\n",
    "        print(f\"🤖 Thuật toán: TextRank + TF-IDF\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        print(\"Chọn chế độ:\")\n",
    "        print(\"  [1] Scrape nhanh (5 bài/nguồn)\")\n",
    "        print(\"  [2] Xem thống kê\")\n",
    "        print(\"  [3] Hướng dẫn sử dụng\")\n",
    "        \n",
    "        try:\n",
    "            choice = input(\"\\nNhập lựa chọn (1/2/3): \").strip()\n",
    "            \n",
    "            if choice == \"1\":\n",
    "                quick_scrape(num_articles_per_source=5)\n",
    "            elif choice == \"2\":\n",
    "                show_stats_simple()\n",
    "            else:\n",
    "                print(f\"\\n📖 HƯỚNG DẪN SỬ DỤNG:\")\n",
    "                print(f\"\\n  1️⃣  Scrape dữ liệu:\")\n",
    "                print(f\"     python {sys.argv[0]} scrape [số_bài]\")\n",
    "                print(f\"     Ví dụ: python {sys.argv[0]} scrape 50\")\n",
    "                \n",
    "                print(f\"\\n  2️⃣  Xem thống kê:\")\n",
    "                print(f\"     python {sys.argv[0]} stats\")\n",
    "                \n",
    "                print(f\"\\n  3️⃣  Chạy API server:\")\n",
    "                print(f\"     uvicorn {sys.argv[0].replace('.py', '')}:app --reload\")\n",
    "                \n",
    "                print(f\"\\n  4️⃣  Test API:\")\n",
    "                print(f\"     curl http://localhost:8000/stats\")\n",
    "                print(f\"     curl http://localhost:8000/articles?category=projects\")\n",
    "                print(f\"     curl http://localhost:8000/search?q=metro\")\n",
    "                \n",
    "                print(f\"\\n{'='*70}\\n\")\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            print(f\"\\n\\n👋 Tạm biệt!\\n\")\n",
    "\n",
    "# ==================== Thêm vào cuối file ====================\n",
    "def interactive_mode():\n",
    "    \"\"\"Chạy interactive để tránh lỗi ipykernel\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"🚀 TP.HCM TRAFFIC NEWS SCRAPER\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\n[1] Scrape 5 bài/nguồn\")\n",
    "    print(\"[2] Xem thống kê\")\n",
    "    \n",
    "    choice = input(\"\\nChọn (1/2): \").strip()\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        quick_scrape(5)\n",
    "    elif choice == \"2\":\n",
    "        show_stats_simple()\n",
    "\n",
    "# Chạy interactive mode thay vì dùng sys.argv\n",
    "interactive_mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ecc959",
   "metadata": {},
   "source": [
    "GIAI ĐOẠN 1: THU THẬP DỮ LIỆU THÔ (RAW DATA COLLECTION)\n",
    "=======================================================\n",
    "Mục tiêu: Scrape bài báo từ nhiều nguồn và lưu vào MongoDB collection \"raw_data\"\n",
    "Không làm sạch, không lọc - chỉ thu thập thô"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25af4e9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '--f=c:\\\\Users\\\\ASUS\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-v3e88060ec6ce0abe0108c4929c27dbcc5f1d608e1.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 345\u001b[39m\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sys.argv) > \u001b[32m1\u001b[39m:\n\u001b[32m    344\u001b[39m     \u001b[38;5;66;03m# Chạy với tham số: python 1_data_collection.py 100\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m     max_articles = \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43margv\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    347\u001b[39m     \u001b[38;5;66;03m# Mặc định: 50 bài/nguồn\u001b[39;00m\n\u001b[32m    348\u001b[39m     max_articles = \u001b[32m50\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: invalid literal for int() with base 10: '--f=c:\\\\Users\\\\ASUS\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-v3e88060ec6ce0abe0108c4929c27dbcc5f1d608e1.json'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "GIAI ĐOẠN 1: THU THẬP DỮ LIỆU THÔ (RAW DATA COLLECTION)\n",
    "=======================================================\n",
    "Mục tiêu: Scrape bài báo từ nhiều nguồn và lưu vào MongoDB collection \"raw_data\"\n",
    "Không làm sạch, không lọc - chỉ thu thập thô\n",
    "\"\"\"\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "import feedparser\n",
    "import time\n",
    "import hashlib\n",
    "\n",
    "# ==================== CẤU HÌNH ====================\n",
    "\n",
    "# Cấu hình MongoDB\n",
    "MONGODB_URI = \"mongodb://localhost:27017/\"\n",
    "DATABASE_NAME = \"hcmc_traffic_summarization\"\n",
    "RAW_COLLECTION = \"raw_data\"  # Collection cho dữ liệu thô\n",
    "\n",
    "# Cấu hình nguồn tin\n",
    "NEWS_SOURCES = {\n",
    "    \"laodong\": {\n",
    "        \"name\": \"Báo Lao Động\",\n",
    "        \"base_url\": \"https://laodong.vn\",\n",
    "        \"rss_feeds\": [\"https://laodong.vn/rss/giao-thong.rss\"],\n",
    "        \"selectors\": {\n",
    "            \"title\": \"h1.detail__title, h1\",\n",
    "            \"content\": \"div.detail-content-body, div.detail__content\",\n",
    "            \"date\": \"span.time-update, time\"\n",
    "        }\n",
    "    },\n",
    "    \"vnexpress\": {\n",
    "        \"name\": \"VnExpress\",\n",
    "        \"base_url\": \"https://vnexpress.net\",\n",
    "        \"rss_feeds\": [\"https://vnexpress.net/rss/giao-thong.rss\"],\n",
    "        \"selectors\": {\n",
    "            \"title\": \"h1.title-detail\",\n",
    "            \"content\": \"article.fck_detail, div.fck_detail\",\n",
    "            \"date\": \"span.date\"\n",
    "        }\n",
    "    },\n",
    "    \"tuoitre\": {\n",
    "        \"name\": \"Báo Tuổi Trẻ\",\n",
    "        \"base_url\": \"https://tuoitre.vn\",\n",
    "        \"rss_feeds\": [\"https://tuoitre.vn/rss/giao-thong.rss\"],\n",
    "        \"selectors\": {\n",
    "            \"title\": \"h1.detail-title\",\n",
    "            \"content\": \"div.detail-content, div#main-detail-content\",\n",
    "            \"date\": \"div.date-time\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# ==================== KẾT NỐI DATABASE ====================\n",
    "\n",
    "def connect_mongodb():\n",
    "    \"\"\"\n",
    "    Kết nối MongoDB và tạo collection raw_data\n",
    "    \n",
    "    Returns:\n",
    "        Collection object hoặc None nếu lỗi\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = MongoClient(MONGODB_URI, serverSelectionTimeoutMS=5000)\n",
    "        client.server_info()  # Test connection\n",
    "        \n",
    "        db = client[DATABASE_NAME]\n",
    "        raw_col = db[RAW_COLLECTION]\n",
    "        \n",
    "        # Tạo index để tránh trùng lặp URL\n",
    "        raw_col.create_index([(\"url\", 1)], unique=True)\n",
    "        raw_col.create_index([(\"scraped_at\", -1)])\n",
    "        raw_col.create_index([(\"source\", 1)])\n",
    "        \n",
    "        print(f\"✅ Đã kết nối MongoDB: {DATABASE_NAME}.{RAW_COLLECTION}\")\n",
    "        return raw_col\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Lỗi kết nối MongoDB: {e}\")\n",
    "        return None\n",
    "\n",
    "# ==================== SCRAPING FUNCTIONS ====================\n",
    "\n",
    "def init_selenium_driver():\n",
    "    \"\"\"\n",
    "    Khởi tạo Selenium WebDriver (Edge headless)\n",
    "    \n",
    "    Returns:\n",
    "        WebDriver object\n",
    "    \"\"\"\n",
    "    options = webdriver.EdgeOptions()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64)\")\n",
    "    \n",
    "    driver = webdriver.Edge(options=options)\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "def get_urls_from_rss(rss_url, max_items=50):\n",
    "    \"\"\"\n",
    "    Lấy danh sách URLs từ RSS feed\n",
    "    \n",
    "    Args:\n",
    "        rss_url: URL của RSS feed\n",
    "        max_items: Số lượng bài tối đa\n",
    "    \n",
    "    Returns:\n",
    "        List of URLs\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"  📡 Đang đọc RSS: {rss_url}\")\n",
    "        feed = feedparser.parse(rss_url)\n",
    "        \n",
    "        urls = []\n",
    "        for entry in feed.entries[:max_items]:\n",
    "            url = entry.get(\"link\", \"\")\n",
    "            if url:\n",
    "                urls.append(url)\n",
    "        \n",
    "        print(f\"  ✅ Tìm thấy {len(urls)} URLs từ RSS\")\n",
    "        return urls\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Lỗi đọc RSS: {e}\")\n",
    "        return []\n",
    "\n",
    "def scrape_article_raw(url, source_key, driver):\n",
    "    \"\"\"\n",
    "    Scrape 1 bài báo và trả về dữ liệu THÔ (không làm sạch)\n",
    "    \n",
    "    Args:\n",
    "        url: URL bài báo\n",
    "        source_key: Key của nguồn trong NEWS_SOURCES\n",
    "        driver: Selenium WebDriver\n",
    "    \n",
    "    Returns:\n",
    "        Dict chứa dữ liệu thô hoặc None nếu lỗi\n",
    "    \"\"\"\n",
    "    source = NEWS_SOURCES[source_key]\n",
    "    \n",
    "    try:\n",
    "        driver.get(url)\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        \n",
    "        # Lấy title (raw - không làm sạch)\n",
    "        try:\n",
    "            title_elem = wait.until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, source[\"selectors\"][\"title\"]))\n",
    "            )\n",
    "            title = title_elem.text\n",
    "        except:\n",
    "            title = driver.title\n",
    "        \n",
    "        # Lấy content (raw - bao gồm cả HTML)\n",
    "        try:\n",
    "            content_elem = driver.find_element(By.CSS_SELECTOR, source[\"selectors\"][\"content\"])\n",
    "            # Lấy cả innerHTML để giữ cấu trúc\n",
    "            content_html = content_elem.get_attribute(\"innerHTML\")\n",
    "            content_text = content_elem.text\n",
    "        except:\n",
    "            content_html = driver.find_element(By.TAG_NAME, \"body\").get_attribute(\"innerHTML\")\n",
    "            content_text = driver.find_element(By.TAG_NAME, \"body\").text\n",
    "        \n",
    "        # Lấy ngày đăng (raw string)\n",
    "        date_raw = \"\"\n",
    "        if \"date\" in source[\"selectors\"]:\n",
    "            try:\n",
    "                date_elem = driver.find_element(By.CSS_SELECTOR, source[\"selectors\"][\"date\"])\n",
    "                date_raw = date_elem.text\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Tạo hash để phát hiện duplicate sau này\n",
    "        content_hash = hashlib.md5(content_text.encode('utf-8')).hexdigest()\n",
    "        \n",
    "        return {\n",
    "            \"url\": url,\n",
    "            \"source\": source_key,\n",
    "            \"source_name\": source[\"name\"],\n",
    "            \"title\": title,  # RAW title\n",
    "            \"content_text\": content_text,  # RAW text\n",
    "            \"content_html\": content_html,  # RAW HTML\n",
    "            \"date_raw\": date_raw,  # RAW date string\n",
    "            \"content_hash\": content_hash,\n",
    "            \"scraped_at\": datetime.now(),\n",
    "            \"processing_status\": \"raw\"  # Đánh dấu chưa xử lý\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Lỗi scrape {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# ==================== MAIN SCRAPING PIPELINE ====================\n",
    "\n",
    "def scrape_source(source_key, raw_collection, max_articles=50):\n",
    "    \"\"\"\n",
    "    Scrape 1 nguồn tin và lưu vào MongoDB\n",
    "    \n",
    "    Args:\n",
    "        source_key: Key của nguồn trong NEWS_SOURCES\n",
    "        raw_collection: MongoDB collection object\n",
    "        max_articles: Số lượng bài tối đa\n",
    "    \n",
    "    Returns:\n",
    "        Dict chứa thống kê\n",
    "    \"\"\"\n",
    "    source = NEWS_SOURCES[source_key]\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"🔍 Đang scrape: {source['name']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Bước 1: Lấy URLs từ RSS\n",
    "    all_urls = []\n",
    "    for rss_url in source.get(\"rss_feeds\", []):\n",
    "        urls = get_urls_from_rss(rss_url, max_items=max_articles)\n",
    "        all_urls.extend(urls)\n",
    "    \n",
    "    # Loại bỏ duplicate URLs\n",
    "    all_urls = list(set(all_urls))\n",
    "    print(f\"\\n📊 Tổng cộng: {len(all_urls)} URLs duy nhất\")\n",
    "    \n",
    "    # Bước 2: Scrape từng bài\n",
    "    driver = None\n",
    "    stats = {\n",
    "        \"total_urls\": len(all_urls),\n",
    "        \"scraped_success\": 0,\n",
    "        \"saved_to_db\": 0,\n",
    "        \"duplicates\": 0,\n",
    "        \"errors\": 0\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        driver = init_selenium_driver()\n",
    "        \n",
    "        for idx, url in enumerate(all_urls[:max_articles], 1):\n",
    "            print(f\"\\n[{idx}/{min(len(all_urls), max_articles)}] {url[:60]}...\")\n",
    "            \n",
    "            # Kiểm tra đã tồn tại trong DB chưa\n",
    "            if raw_collection.find_one({\"url\": url}):\n",
    "                print(f\"  ⏭️  Đã tồn tại trong DB\")\n",
    "                stats[\"duplicates\"] += 1\n",
    "                continue\n",
    "            \n",
    "            # Scrape bài báo\n",
    "            article_data = scrape_article_raw(url, source_key, driver)\n",
    "            \n",
    "            if article_data is None:\n",
    "                stats[\"errors\"] += 1\n",
    "                continue\n",
    "            \n",
    "            stats[\"scraped_success\"] += 1\n",
    "            \n",
    "            # Lưu vào MongoDB (raw data)\n",
    "            try:\n",
    "                raw_collection.insert_one(article_data)\n",
    "                print(f\"  ✅ Đã lưu raw data: {article_data['title'][:50]}...\")\n",
    "                stats[\"saved_to_db\"] += 1\n",
    "            except Exception as e:\n",
    "                print(f\"  ❌ Lỗi lưu DB: {e}\")\n",
    "                stats[\"duplicates\"] += 1\n",
    "            \n",
    "            time.sleep(2)  # Tránh spam server\n",
    "    \n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def scrape_all_sources(max_per_source=50):\n",
    "    \"\"\"\n",
    "    Scrape tất cả nguồn tin\n",
    "    \n",
    "    Args:\n",
    "        max_per_source: Số bài tối đa mỗi nguồn\n",
    "    \n",
    "    Returns:\n",
    "        Dict chứa thống kê tổng hợp\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"🚀 GIAI ĐOẠN 1: THU THẬP DỮ LIỆU THÔ\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Kết nối MongoDB\n",
    "    raw_collection = connect_mongodb()\n",
    "    if raw_collection is None:\n",
    "        print(\"❌ Không thể kết nối MongoDB. Dừng scraping.\")\n",
    "        return None\n",
    "    \n",
    "    # Scrape từng nguồn\n",
    "    total_stats = {\n",
    "        \"sources\": {},\n",
    "        \"grand_total\": {\n",
    "            \"scraped\": 0,\n",
    "            \"saved\": 0,\n",
    "            \"duplicates\": 0,\n",
    "            \"errors\": 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for source_key in NEWS_SOURCES.keys():\n",
    "        stats = scrape_source(source_key, raw_collection, max_per_source)\n",
    "        total_stats[\"sources\"][source_key] = stats\n",
    "        \n",
    "        total_stats[\"grand_total\"][\"scraped\"] += stats[\"scraped_success\"]\n",
    "        total_stats[\"grand_total\"][\"saved\"] += stats[\"saved_to_db\"]\n",
    "        total_stats[\"grand_total\"][\"duplicates\"] += stats[\"duplicates\"]\n",
    "        total_stats[\"grand_total\"][\"errors\"] += stats[\"errors\"]\n",
    "    \n",
    "    # In báo cáo tổng kết\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"✅ HOÀN THÀNH GIAI ĐOẠN 1\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"📊 Thống kê tổng hợp:\")\n",
    "    print(f\"   • Đã scrape thành công: {total_stats['grand_total']['scraped']}\")\n",
    "    print(f\"   • Đã lưu vào raw_data: {total_stats['grand_total']['saved']}\")\n",
    "    print(f\"   • Trùng lặp: {total_stats['grand_total']['duplicates']}\")\n",
    "    print(f\"   • Lỗi: {total_stats['grand_total']['errors']}\")\n",
    "    \n",
    "    print(f\"\\n📈 Chi tiết theo nguồn:\")\n",
    "    for source_key, stats in total_stats[\"sources\"].items():\n",
    "        name = NEWS_SOURCES[source_key][\"name\"]\n",
    "        print(f\"   • {name}: {stats['saved_to_db']} bài\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"➡️  Tiếp theo: Chạy 2_data_filtering.py để lọc & gán nhãn\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return total_stats\n",
    "\n",
    "# ==================== CHẠY CHƯƠNG TRÌNH ====================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    \n",
    "    if len(sys.argv) > 1:\n",
    "        # Chạy với tham số: python 1_data_collection.py 100\n",
    "        max_articles = int(sys.argv[1])\n",
    "    else:\n",
    "        # Mặc định: 50 bài/nguồn\n",
    "        max_articles = 50\n",
    "    \n",
    "    print(f\"⚙️  Cấu hình: Tối đa {max_articles} bài/nguồn\\n\")\n",
    "    \n",
    "    # Bắt đầu scraping\n",
    "    results = scrape_all_sources(max_per_source=max_articles)\n",
    "    \n",
    "    if results:\n",
    "        print(f\"\\n✅ Dữ liệu thô đã được lưu vào MongoDB collection '{RAW_COLLECTION}'\")\n",
    "        print(f\"📊 Tổng số bài: {results['grand_total']['saved']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e9ea8e",
   "metadata": {},
   "source": [
    "GIAI ĐOẠN 2: LỌC VÀ GÁN NHÃN (DATA FILTERING & LABELING)\n",
    "=========================================================\n",
    "Mục tiêu: \n",
    "- Lọc chỉ giữ bài viết về giao thông TP.HCM năm 2025\n",
    "- Gán nhãn category tự động (projects/infrastructure/issues/planning/transport)\n",
    "- Phát hiện và loại bỏ duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04031e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GIAI ĐOẠN 2: LỌC VÀ GÁN NHÃN (DATA FILTERING & LABELING)\n",
    "=========================================================\n",
    "Mục tiêu: \n",
    "- Lọc chỉ giữ bài viết về giao thông TP.HCM năm 2025\n",
    "- Gán nhãn category tự động (projects/infrastructure/issues/planning/transport)\n",
    "- Phát hiện và loại bỏ duplicate\n",
    "\"\"\"\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# ==================== CẤU HÌNH ====================\n",
    "\n",
    "# MongoDB\n",
    "MONGODB_URI = \"mongodb://localhost:27017/\"\n",
    "DATABASE_NAME = \"hcmc_traffic_summarization\"\n",
    "RAW_COLLECTION = \"raw_data\"\n",
    "FILTERED_COLLECTION = \"filtered_data\"  # Collection sau khi lọc\n",
    "\n",
    "# Thời gian lọc\n",
    "START_DATE = datetime(2025, 1, 1)\n",
    "END_DATE = datetime(2025, 12, 31)\n",
    "\n",
    "# Từ khóa TP.HCM (để xác định bài viết thuộc TP.HCM)\n",
    "HCMC_KEYWORDS = [\n",
    "    \"tp.hcm\", \"tp hcm\", \"tphcm\", \"hcmc\",\n",
    "    \"hồ chí minh\", \"ho chi minh\",\n",
    "    \"sài gòn\", \"saigon\",\n",
    "    \"thành phố hồ chí minh\",\n",
    "    # Địa danh cụ thể\n",
    "    \"quận 1\", \"quận 2\", \"quận 3\", \"quận 4\", \"quận 5\",\n",
    "    \"bình thạnh\", \"tân bình\", \"phú nhuận\", \"gò vấp\",\n",
    "    \"thủ đức\", \"bình tân\", \"tân phú\",\n",
    "    \"cầu sài gòn\", \"cầu phú mỹ\", \"cầu bình triệu\",\n",
    "    \"đại lộ võ văn kiệt\", \"đại lộ đông tây\"\n",
    "]\n",
    "\n",
    "# Từ khóa giao thông (để xác định bài viết về giao thông)\n",
    "TRAFFIC_KEYWORDS = [\n",
    "    # Cơ sở hạ tầng\n",
    "    \"giao thông\", \"metro\", \"tàu điện\", \"cao tốc\", \"cầu\", \"đường\",\n",
    "    \"vành đai\", \"brt\", \"đường sắt\", \"đường bộ\",\n",
    "    \n",
    "    # Dự án\n",
    "    \"dự án\", \"khởi công\", \"hoàn thành\", \"đầu tư\", \"xây dựng\",\n",
    "    \"triển khai\", \"thi công\",\n",
    "    \n",
    "    # Vấn đề\n",
    "    \"kẹt xe\", \"ùn tắc\", \"tai nạn\", \"giao thông\", \"ngập nước\",\n",
    "    \"ổ gà\", \"hư hỏng\",\n",
    "    \n",
    "    # Phương tiện\n",
    "    \"xe buýt\", \"xe bus\", \"taxi\", \"grab\", \"xe máy\", \"ô tô\",\n",
    "    \"vận tải\", \"xe điện\",\n",
    "    \n",
    "    # Quy hoạch\n",
    "    \"quy hoạch\", \"kế hoạch\", \"phê duyệt\", \"điều chỉnh\"\n",
    "]\n",
    "\n",
    "# Danh mục và từ khóa đặc trưng\n",
    "CATEGORY_KEYWORDS = {\n",
    "    \"projects\": {\n",
    "        \"name\": \"Dự án giao thông\",\n",
    "        \"keywords\": [\n",
    "            \"metro\", \"tàu điện\", \"cao tốc\", \"cầu\", \"khởi công\",\n",
    "            \"hoàn thành\", \"dự án\", \"đầu tư\", \"xây dựng\", \"triển khai\",\n",
    "            \"thi công\", \"đường vành đai\", \"brt\"\n",
    "        ]\n",
    "    },\n",
    "    \"infrastructure\": {\n",
    "        \"name\": \"Hạ tầng giao thông\",\n",
    "        \"keywords\": [\n",
    "            \"hạ tầng\", \"đường bộ\", \"đường sắt\", \"đường thủy\",\n",
    "            \"sân bay\", \"cảng\", \"bến xe\", \"trạm\", \"nút giao thông\",\n",
    "            \"cơ sở hạ tầng\", \"hệ thống\"\n",
    "        ]\n",
    "    },\n",
    "    \"issues\": {\n",
    "        \"name\": \"Vấn đề giao thông\",\n",
    "        \"keywords\": [\n",
    "            \"kẹt xe\", \"ùn tắc\", \"tai nạn\", \"ngập nước\", \"ổ gà\",\n",
    "            \"hư hỏng\", \"ách tắc\", \"tắc đường\", \"va chạm\",\n",
    "            \"đụng độ\", \"sự cố\"\n",
    "        ]\n",
    "    },\n",
    "    \"planning\": {\n",
    "        \"name\": \"Quy hoạch giao thông\",\n",
    "        \"keywords\": [\n",
    "            \"quy hoạch\", \"kế hoạch\", \"phê duyệt\", \"điều chỉnh\",\n",
    "            \"đề án\", \"chính sách\", \"pháp luật\", \"quyết định\",\n",
    "            \"chỉ đạo\", \"nghiên cứu\"\n",
    "        ]\n",
    "    },\n",
    "    \"transport\": {\n",
    "        \"name\": \"Phương tiện vận tải\",\n",
    "        \"keywords\": [\n",
    "            \"xe buýt\", \"xe bus\", \"taxi\", \"grab\", \"xe máy\", \"ô tô\",\n",
    "            \"vận tải\", \"xe điện\", \"xe công cộng\", \"xe khách\",\n",
    "            \"tuyến xe\", \"điểm dừng\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# ==================== KẾT NỐI DATABASE ====================\n",
    "\n",
    "def connect_mongodb():\n",
    "    \"\"\"\n",
    "    Kết nối MongoDB và tạo collection filtered_data\n",
    "    \n",
    "    Returns:\n",
    "        Tuple (raw_collection, filtered_collection)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = MongoClient(MONGODB_URI, serverSelectionTimeoutMS=5000)\n",
    "        client.server_info()\n",
    "        \n",
    "        db = client[DATABASE_NAME]\n",
    "        raw_col = db[RAW_COLLECTION]\n",
    "        filtered_col = db[FILTERED_COLLECTION]\n",
    "        \n",
    "        # Tạo index\n",
    "        filtered_col.create_index([(\"url\", 1)], unique=True)\n",
    "        filtered_col.create_index([(\"published_date\", -1)])\n",
    "        filtered_col.create_index([(\"category\", 1)])\n",
    "        filtered_col.create_index([(\"is_hcmc\", 1)])\n",
    "        \n",
    "        print(f\"✅ Đã kết nối MongoDB\")\n",
    "        print(f\"   Raw data: {raw_col.count_documents({})}\")\n",
    "        return raw_col, filtered_col\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Lỗi kết nối MongoDB: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# ==================== FILTERING FUNCTIONS ====================\n",
    "\n",
    "def is_hcmc_related(text):\n",
    "    \"\"\"\n",
    "    Kiểm tra bài viết có liên quan đến TP.HCM không\n",
    "    \n",
    "    Args:\n",
    "        text: Nội dung cần kiểm tra (title + content)\n",
    "    \n",
    "    Returns:\n",
    "        bool: True nếu liên quan TP.HCM\n",
    "    \"\"\"\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Đếm số lần xuất hiện từ khóa TP.HCM\n",
    "    hcmc_count = sum(1 for kw in HCMC_KEYWORDS if kw in text_lower)\n",
    "    \n",
    "    # Quy tắc: Phải có ít nhất 2 từ khóa TP.HCM HOẶC 1 từ khóa xuất hiện nhiều lần\n",
    "    if hcmc_count >= 2:\n",
    "        return True\n",
    "    \n",
    "    # Kiểm tra từ khóa chính xuất hiện nhiều lần\n",
    "    for kw in [\"tp.hcm\", \"tphcm\", \"hồ chí minh\", \"sài gòn\"]:\n",
    "        if text_lower.count(kw) >= 2:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def is_traffic_related(text):\n",
    "    \"\"\"\n",
    "    Kiểm tra bài viết có liên quan đến giao thông không\n",
    "    \n",
    "    Args:\n",
    "        text: Nội dung cần kiểm tra\n",
    "    \n",
    "    Returns:\n",
    "        bool: True nếu liên quan giao thông\n",
    "    \"\"\"\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Đếm số lần xuất hiện từ khóa giao thông\n",
    "    traffic_count = sum(1 for kw in TRAFFIC_KEYWORDS if kw in text_lower)\n",
    "    \n",
    "    # Quy tắc: Phải có ít nhất 3 từ khóa giao thông\n",
    "    return traffic_count >= 3\n",
    "\n",
    "def parse_date_from_raw(date_raw):\n",
    "    \"\"\"\n",
    "    Parse ngày tháng từ chuỗi raw\n",
    "    \n",
    "    Args:\n",
    "        date_raw: Chuỗi ngày tháng chưa xử lý\n",
    "    \n",
    "    Returns:\n",
    "        datetime object hoặc None\n",
    "    \"\"\"\n",
    "    if not date_raw:\n",
    "        return None\n",
    "    \n",
    "    # Danh sách các format phổ biến\n",
    "    formats = [\n",
    "        \"%d/%m/%Y %H:%M\",\n",
    "        \"%d/%m/%Y, %H:%M\",\n",
    "        \"%d-%m-%Y %H:%M\",\n",
    "        \"%d/%m/%Y\",\n",
    "        \"%d-%m-%Y\",\n",
    "        \"%Y-%m-%d\",\n",
    "    ]\n",
    "    \n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            return datetime.strptime(date_raw.strip(), fmt)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Thử trích xuất ngày từ chuỗi bằng regex\n",
    "    match = re.search(r'(\\d{1,2})[/-](\\d{1,2})[/-](\\d{4})', date_raw)\n",
    "    if match:\n",
    "        day, month, year = match.groups()\n",
    "        try:\n",
    "            return datetime(int(year), int(month), int(day))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return None\n",
    "\n",
    "def categorize_article(title, content):\n",
    "    \"\"\"\n",
    "    Phân loại bài viết tự động theo category\n",
    "    \n",
    "    Args:\n",
    "        title: Tiêu đề bài viết\n",
    "        content: Nội dung bài viết\n",
    "    \n",
    "    Returns:\n",
    "        str: Category key (projects/infrastructure/issues/planning/transport/general)\n",
    "    \"\"\"\n",
    "    text = (title + \" \" + content).lower()\n",
    "    \n",
    "    # Đếm số lần xuất hiện từ khóa của mỗi category\n",
    "    scores = {}\n",
    "    for cat_key, cat_data in CATEGORY_KEYWORDS.items():\n",
    "        score = sum(1 for kw in cat_data[\"keywords\"] if kw in text)\n",
    "        scores[cat_key] = score\n",
    "    \n",
    "    # Lấy category có điểm cao nhất\n",
    "    if max(scores.values()) == 0:\n",
    "        return \"general\"\n",
    "    \n",
    "    return max(scores, key=scores.get)\n",
    "\n",
    "def detect_duplicate(content_hash, filtered_col):\n",
    "    \"\"\"\n",
    "    Phát hiện bài viết trùng lặp dựa vào content_hash\n",
    "    \n",
    "    Args:\n",
    "        content_hash: MD5 hash của content\n",
    "        filtered_col: MongoDB collection\n",
    "    \n",
    "    Returns:\n",
    "        bool: True nếu là duplicate\n",
    "    \"\"\"\n",
    "    existing = filtered_col.find_one({\"content_hash\": content_hash})\n",
    "    return existing is not None\n",
    "\n",
    "# ==================== MAIN FILTERING PIPELINE ====================\n",
    "\n",
    "def filter_article(raw_doc, filtered_col):\n",
    "    \"\"\"\n",
    "    Lọc và xử lý 1 bài viết raw\n",
    "    \n",
    "    Args:\n",
    "        raw_doc: Document từ raw_data collection\n",
    "        filtered_col: Collection để lưu kết quả\n",
    "    \n",
    "    Returns:\n",
    "        Dict với status và message\n",
    "    \"\"\"\n",
    "    url = raw_doc.get(\"url\", \"\")\n",
    "    title = raw_doc.get(\"title\", \"\")\n",
    "    content = raw_doc.get(\"content_text\", \"\")\n",
    "    \n",
    "    # Bước 1: Kiểm tra duplicate\n",
    "    if detect_duplicate(raw_doc.get(\"content_hash\"), filtered_col):\n",
    "        return {\"status\": \"duplicate\", \"reason\": \"Nội dung trùng lặp\"}\n",
    "    \n",
    "    # Bước 2: Kiểm tra liên quan TP.HCM\n",
    "    if not is_hcmc_related(title + \" \" + content):\n",
    "        return {\"status\": \"not_hcmc\", \"reason\": \"Không liên quan TP.HCM\"}\n",
    "    \n",
    "    # Bước 3: Kiểm tra liên quan giao thông\n",
    "    if not is_traffic_related(title + \" \" + content):\n",
    "        return {\"status\": \"not_traffic\", \"reason\": \"Không liên quan giao thông\"}\n",
    "    \n",
    "    # Bước 4: Parse ngày đăng\n",
    "    published_date = parse_date_from_raw(raw_doc.get(\"date_raw\", \"\"))\n",
    "    \n",
    "    # Nếu không parse được ngày, dùng scraped_at\n",
    "    if published_date is None:\n",
    "        published_date = raw_doc.get(\"scraped_at\", datetime.now())\n",
    "    \n",
    "    # Bước 5: Kiểm tra năm 2025\n",
    "    if published_date.year != 2025:\n",
    "        return {\"status\": \"wrong_year\", \"reason\": f\"Năm {published_date.year}, không phải 2025\"}\n",
    "    \n",
    "    # Bước 6: Gán category\n",
    "    category = categorize_article(title, content)\n",
    "    \n",
    "    # Tạo document mới cho filtered_data\n",
    "    filtered_doc = {\n",
    "        \"url\": url,\n",
    "        \"source\": raw_doc.get(\"source\"),\n",
    "        \"source_name\": raw_doc.get(\"source_name\"),\n",
    "        \"title\": title,\n",
    "        \"content_text\": content,\n",
    "        \"content_html\": raw_doc.get(\"content_html\"),\n",
    "        \"content_hash\": raw_doc.get(\"content_hash\"),\n",
    "        \"published_date\": published_date,\n",
    "        \"category\": category,\n",
    "        \"category_name\": CATEGORY_KEYWORDS.get(category, {}).get(\"name\", \"Chung\"),\n",
    "        \"is_hcmc\": True,\n",
    "        \"is_traffic\": True,\n",
    "        \"scraped_at\": raw_doc.get(\"scraped_at\"),\n",
    "        \"filtered_at\": datetime.now(),\n",
    "        \"processing_status\": \"filtered\"  # Đánh dấu đã lọc\n",
    "    }\n",
    "    \n",
    "    return {\"status\": \"pass\", \"doc\": filtered_doc, \"category\": category}\n",
    "\n",
    "def run_filtering(batch_size=100):\n",
    "    \"\"\"\n",
    "    Chạy quá trình lọc cho toàn bộ raw_data\n",
    "    \n",
    "    Args:\n",
    "        batch_size: Số lượng document xử lý mỗi lần\n",
    "    \n",
    "    Returns:\n",
    "        Dict chứa thống kê\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"🔍 GIAI ĐOẠN 2: LỌC VÀ GÁN NHÃN\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Kết nối MongoDB\n",
    "    raw_col, filtered_col = connect_mongodb()\n",
    "    if raw_col is None or filtered_col is None:\n",
    "        print(\"❌ Không thể kết nối MongoDB\")\n",
    "        return None\n",
    "    \n",
    "    # Lấy tất cả raw documents chưa xử lý\n",
    "    total_raw = raw_col.count_documents({\"processing_status\": \"raw\"})\n",
    "    print(f\"📊 Tổng số bài raw chưa xử lý: {total_raw}\")\n",
    "    \n",
    "    stats = {\n",
    "        \"total_processed\": 0,\n",
    "        \"passed\": 0,\n",
    "        \"not_hcmc\": 0,\n",
    "        \"not_traffic\": 0,\n",
    "        \"wrong_year\": 0,\n",
    "        \"duplicates\": 0,\n",
    "        \"errors\": 0,\n",
    "        \"by_category\": Counter()\n",
    "    }\n",
    "    \n",
    "    # Xử lý từng batch\n",
    "    cursor = raw_col.find({\"processing_status\": \"raw\"}).batch_size(batch_size)\n",
    "    \n",
    "    for idx, raw_doc in enumerate(cursor, 1):\n",
    "        print(f\"\\r[{idx}/{total_raw}] Đang xử lý...\", end=\"\", flush=True)\n",
    "        \n",
    "        stats[\"total_processed\"] += 1\n",
    "        \n",
    "        try:\n",
    "            # Lọc bài viết\n",
    "            result = filter_article(raw_doc, filtered_col)\n",
    "            \n",
    "            if result[\"status\"] == \"pass\":\n",
    "                # Lưu vào filtered_data\n",
    "                try:\n",
    "                    filtered_col.insert_one(result[\"doc\"])\n",
    "                    stats[\"passed\"] += 1\n",
    "                    stats[\"by_category\"][result[\"category\"]] += 1\n",
    "                    \n",
    "                    # Cập nhật trạng thái trong raw_data\n",
    "                    raw_col.update_one(\n",
    "                        {\"_id\": raw_doc[\"_id\"]},\n",
    "                        {\"$set\": {\"processing_status\": \"filtered\"}}\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"\\n  ❌ Lỗi lưu DB: {e}\")\n",
    "                    stats[\"errors\"] += 1\n",
    "            \n",
    "            elif result[\"status\"] == \"duplicate\":\n",
    "                stats[\"duplicates\"] += 1\n",
    "            elif result[\"status\"] == \"not_hcmc\":\n",
    "                stats[\"not_hcmc\"] += 1\n",
    "            elif result[\"status\"] == \"not_traffic\":\n",
    "                stats[\"not_traffic\"] += 1\n",
    "            elif result[\"status\"] == \"wrong_year\":\n",
    "                stats[\"wrong_year\"] += 1\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"\\n  ❌ Lỗi xử lý: {e}\")\n",
    "            stats[\"errors\"] += 1\n",
    "    \n",
    "    print()  # Xuống dòng sau progress bar\n",
    "    \n",
    "    # In báo cáo\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"✅ HOÀN THÀNH GIAI ĐOẠN 2\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"📊 Thống kê:\")\n",
    "    print(f\"   • Tổng số đã xử lý: {stats['total_processed']}\")\n",
    "    print(f\"   • Đã lọc thành công: {stats['passed']} ({stats['passed']/stats['total_processed']*100:.1f}%)\")\n",
    "    print(f\"   • Không phải TP.HCM: {stats['not_hcmc']}\")\n",
    "    print(f\"   • Không phải giao thông: {stats['not_traffic']}\")\n",
    "    print(f\"   • Không phải năm 2025: {stats['wrong_year']}\")\n",
    "    print(f\"   • Trùng lặp: {stats['duplicates']}\")\n",
    "    print(f\"   • Lỗi: {stats['errors']}\")\n",
    "    \n",
    "    print(f\"\\n📂 Phân loại theo category:\")\n",
    "    for cat_key, count in stats[\"by_category\"].most_common():\n",
    "        cat_name = CATEGORY_KEYWORDS.get(cat_key, {}).get(\"name\", cat_key)\n",
    "        print(f\"   • {cat_name}: {count} bài\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"➡️  Tiếp theo: Chạy 3_data_cleaning.py để làm sạch dữ liệu\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# ==================== CHẠY CHƯƠNG TRÌNH ====================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Chạy filtering\n",
    "    results = run_filtering(batch_size=100)\n",
    "    \n",
    "    if results:\n",
    "        print(f\"\\n✅ Dữ liệu đã lọc được lưu vào collection '{FILTERED_COLLECTION}'\")\n",
    "        print(f\"📊 Số bài đạt chuẩn: {results['passed']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2795e67b",
   "metadata": {},
   "source": [
    "GIAI ĐOẠN 3: LÀM SẠCH DỮ LIỆU (DATA CLEANING)\n",
    "==============================================\n",
    "Mục tiêu:\n",
    "- Loại bỏ HTML tags, ký tự đặc biệt, quảng cáo\n",
    "- Chuẩn hóa unicode, dấu câu, khoảng trắng\n",
    "- Tách câu (sentence segmentation)\n",
    "- Lưu vào collection \"clean_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1273daf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GIAI ĐOẠN 3: LÀM SẠCH DỮ LIỆU (DATA CLEANING)\n",
    "==============================================\n",
    "Mục tiêu:\n",
    "- Loại bỏ HTML tags, ký tự đặc biệt, quảng cáo\n",
    "- Chuẩn hóa unicode, dấu câu, khoảng trắng\n",
    "- Tách câu (sentence segmentation)\n",
    "- Lưu vào collection \"clean_data\"\n",
    "\"\"\"\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "\n",
    "# ==================== CẤU HÌNH ====================\n",
    "\n",
    "# MongoDB\n",
    "MONGODB_URI = \"mongodb://localhost:27017/\"\n",
    "DATABASE_NAME = \"hcmc_traffic_summarization\"\n",
    "FILTERED_COLLECTION = \"filtered_data\"\n",
    "CLEAN_COLLECTION = \"clean_data\"  # Collection sau khi làm sạch\n",
    "\n",
    "# Các mẫu quảng cáo/spam cần loại bỏ\n",
    "SPAM_PATTERNS = [\n",
    "    r\"quảng cáo\",\n",
    "    r\"liên hệ:?\\s*\\d\",\n",
    "    r\"hotline:?\\s*\\d\",\n",
    "    r\"đăng ký ngay\",\n",
    "    r\"click here\",\n",
    "    r\"xem thêm tại\",\n",
    "    r\"theo\\s+\\w+\\.vn\",  # Nguồn trích dẫn cuối bài\n",
    "]\n",
    "\n",
    "# Các từ/cụm từ không mang thông tin (stopwords mở rộng)\n",
    "NOISE_PHRASES = [\n",
    "    \"theo đó\", \"bên cạnh đó\", \"ngoài ra\",\n",
    "    \"như đã biết\", \"được biết\", \"có thể nói\",\n",
    "    \"cũng theo\", \"đồng thời\", \"trong khi đó\"\n",
    "]\n",
    "\n",
    "# ==================== KẾT NỐI DATABASE ====================\n",
    "\n",
    "def connect_mongodb():\n",
    "    \"\"\"\n",
    "    Kết nối MongoDB và tạo collection clean_data\n",
    "    \n",
    "    Returns:\n",
    "        Tuple (filtered_collection, clean_collection)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = MongoClient(MONGODB_URI, serverSelectionTimeoutMS=5000)\n",
    "        client.server_info()\n",
    "        \n",
    "        db = client[DATABASE_NAME]\n",
    "        filtered_col = db[FILTERED_COLLECTION]\n",
    "        clean_col = db[CLEAN_COLLECTION]\n",
    "        \n",
    "        # Tạo index\n",
    "        clean_col.create_index([(\"url\", 1)], unique=True)\n",
    "        clean_col.create_index([(\"published_date\", -1)])\n",
    "        clean_col.create_index([(\"category\", 1)])\n",
    "        \n",
    "        print(f\"✅ Đã kết nối MongoDB\")\n",
    "        print(f\"   Filtered data: {filtered_col.count_documents({})}\")\n",
    "        return filtered_col, clean_col\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Lỗi kết nối MongoDB: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# ==================== TEXT CLEANING FUNCTIONS ====================\n",
    "\n",
    "def remove_html(html_text):\n",
    "    \"\"\"\n",
    "    Loại bỏ HTML tags\n",
    "    \n",
    "    Args:\n",
    "        html_text: Chuỗi chứa HTML\n",
    "    \n",
    "    Returns:\n",
    "        str: Text thuần túy\n",
    "    \"\"\"\n",
    "    if not html_text:\n",
    "        return \"\"\n",
    "    \n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    \n",
    "    # Loại bỏ script và style tags\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.decompose()\n",
    "    \n",
    "    # Lấy text\n",
    "    text = soup.get_text()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def normalize_unicode(text):\n",
    "    \"\"\"\n",
    "    Chuẩn hóa Unicode (NFC normalization)\n",
    "    Ví dụ: \"café\" có thể được encode theo nhiều cách\n",
    "    \n",
    "    Args:\n",
    "        text: Chuỗi cần chuẩn hóa\n",
    "    \n",
    "    Returns:\n",
    "        str: Chuỗi đã chuẩn hóa\n",
    "    \"\"\"\n",
    "    return unicodedata.normalize('NFC', text)\n",
    "\n",
    "def remove_special_chars(text):\n",
    "    \"\"\"\n",
    "    Loại bỏ ký tự đặc biệt không cần thiết\n",
    "    \n",
    "    Args:\n",
    "        text: Chuỗi cần xử lý\n",
    "    \n",
    "    Returns:\n",
    "        str: Chuỗi đã làm sạch\n",
    "    \"\"\"\n",
    "    # Giữ lại: chữ cái, số, dấu câu cơ bản, khoảng trắng\n",
    "    text = re.sub(r'[^\\w\\s.,!?;:()\\-–—\\'\\\"°%/]', ' ', text)\n",
    "    \n",
    "    # Loại bỏ emoji\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_spam_content(text):\n",
    "    \"\"\"\n",
    "    Loại bỏ nội dung quảng cáo/spam\n",
    "    \n",
    "    Args:\n",
    "        text: Chuỗi cần xử lý\n",
    "    \n",
    "    Returns:\n",
    "        str: Chuỗi đã loại bỏ spam\n",
    "    \"\"\"\n",
    "    for pattern in SPAM_PATTERNS:\n",
    "        text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def normalize_whitespace(text):\n",
    "    \"\"\"\n",
    "    Chuẩn hóa khoảng trắng\n",
    "    - Loại bỏ khoảng trắng thừa\n",
    "    - Loại bỏ dòng trống\n",
    "    \n",
    "    Args:\n",
    "        text: Chuỗi cần xử lý\n",
    "    \n",
    "    Returns:\n",
    "        str: Chuỗi đã chuẩn hóa\n",
    "    \"\"\"\n",
    "    # Thay thế nhiều khoảng trắng bằng 1\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Loại bỏ khoảng trắng đầu/cuối mỗi dòng\n",
    "    lines = [line.strip() for line in text.split('\\n')]\n",
    "    \n",
    "    # Loại bỏ dòng trống\n",
    "    lines = [line for line in lines if line]\n",
    "    \n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "def normalize_punctuation(text):\n",
    "    \"\"\"\n",
    "    Chuẩn hóa dấu câu\n",
    "    \n",
    "    Args:\n",
    "        text: Chuỗi cần xử lý\n",
    "    \n",
    "    Returns:\n",
    "        str: Chuỗi đã chuẩn hóa dấu câu\n",
    "    \"\"\"\n",
    "    # Chuẩn hóa dấu ngoặc kép\n",
    "    text = text.replace('\"', '\"').replace('\"', '\"')\n",
    "    text = text.replace(\"'\", \"'\").replace(\"'\", \"'\")\n",
    "    \n",
    "    # Chuẩn hóa dấu gạch ngang\n",
    "    text = text.replace('–', '-').replace('—', '-')\n",
    "    \n",
    "    # Thêm khoảng trắng sau dấu câu nếu thiếu\n",
    "    text = re.sub(r'([.,!?;:])([^\\s])', r'\\1 \\2', text)\n",
    "    \n",
    "    # Loại bỏ khoảng trắng trước dấu câu\n",
    "    text = re.sub(r'\\s+([.,!?;:])', r'\\1', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def segment_sentences(text):\n",
    "    \"\"\"\n",
    "    Tách câu trong văn bản tiếng Việt\n",
    "    \n",
    "    Args:\n",
    "        text: Chuỗi văn bản\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: Danh sách các câu\n",
    "    \"\"\"\n",
    "    # Tách theo dấu câu kết thúc\n",
    "    sentences = re.split(r'[.!?]\\s+', text)\n",
    "    \n",
    "    # Làm sạch và lọc câu\n",
    "    cleaned_sentences = []\n",
    "    for sent in sentences:\n",
    "        sent = sent.strip()\n",
    "        \n",
    "        # Chỉ giữ câu có độ dài hợp lý (20-500 ký tự)\n",
    "        if 20 <= len(sent) <= 500:\n",
    "            # Loại bỏ câu chỉ chứa số hoặc ký tự đặc biệt\n",
    "            if re.search(r'[a-zA-Zàáảãạăắằẳẵặâấầẩẫậèéẻẽẹêếềểễệìíỉĩịòóỏõọôốồổỗộơớờởỡợùúủũụưứừửữựỳýỷỹỵđ]', sent):\n",
    "                cleaned_sentences.append(sent)\n",
    "    \n",
    "    return cleaned_sentences\n",
    "\n",
    "def clean_title(title):\n",
    "    \"\"\"\n",
    "    Làm sạch tiêu đề\n",
    "    \n",
    "    Args:\n",
    "        title: Tiêu đề cần làm sạch\n",
    "    \n",
    "    Returns:\n",
    "        str: Tiêu đề đã làm sạch\n",
    "    \"\"\"\n",
    "    # Loại bỏ emoji và ký tự đặc biệt\n",
    "    title = remove_special_chars(title)\n",
    "    \n",
    "    # Chuẩn hóa\n",
    "    title = normalize_unicode(title)\n",
    "    title = normalize_whitespace(title)\n",
    "    title = normalize_punctuation(title)\n",
    "    \n",
    "    # Loại bỏ các từ như [Video], [Infographic]\n",
    "    title = re.sub(r'\\[[\\w\\s]+\\]', '', title)\n",
    "    \n",
    "    return title.strip()\n",
    "\n",
    "def clean_content(content_text, content_html):\n",
    "    \"\"\"\n",
    "    Làm sạch nội dung bài viết\n",
    "    \n",
    "    Args:\n",
    "        content_text: Nội dung dạng text thuần\n",
    "        content_html: Nội dung dạng HTML\n",
    "    \n",
    "    Returns:\n",
    "        Dict chứa nội dung đã làm sạch và sentences\n",
    "    \"\"\"\n",
    "    # Ưu tiên xử lý từ HTML để có kết quả tốt hơn\n",
    "    if content_html:\n",
    "        text = remove_html(content_html)\n",
    "    else:\n",
    "        text = content_text\n",
    "    \n",
    "    # Pipeline làm sạch\n",
    "    text = normalize_unicode(text)\n",
    "    text = remove_spam_content(text)\n",
    "    text = remove_special_chars(text)\n",
    "    text = normalize_whitespace(text)\n",
    "    text = normalize_punctuation(text)\n",
    "    \n",
    "    # Tách câu\n",
    "    sentences = segment_sentences(text)\n",
    "    \n",
    "    # Gộp lại thành văn bản liền mạch\n",
    "    clean_text = '. '.join(sentences) + '.'\n",
    "    \n",
    "    return {\n",
    "        \"content_clean\": clean_text,\n",
    "        \"sentences\": sentences,\n",
    "        \"num_sentences\": len(sentences),\n",
    "        \"num_words\": len(clean_text.split())\n",
    "    }\n",
    "\n",
    "# ==================== MAIN CLEANING PIPELINE ====================\n",
    "\n",
    "def clean_article(filtered_doc):\n",
    "    \"\"\"\n",
    "    Làm sạch 1 bài viết\n",
    "    \n",
    "    Args:\n",
    "        filtered_doc: Document từ filtered_data collection\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Document đã làm sạch\n",
    "    \"\"\"\n",
    "    # Làm sạch title\n",
    "    clean_title_text = clean_title(filtered_doc.get(\"title\", \"\"))\n",
    "    \n",
    "    # Làm sạch content\n",
    "    content_result = clean_content(\n",
    "        filtered_doc.get(\"content_text\", \"\"),\n",
    "        filtered_doc.get(\"content_html\", \"\")\n",
    "    )\n",
    "    \n",
    "    # Tạo document mới\n",
    "    clean_doc = {\n",
    "        \"url\": filtered_doc.get(\"url\"),\n",
    "        \"source\": filtered_doc.get(\"source\"),\n",
    "        \"source_name\": filtered_doc.get(\"source_name\"),\n",
    "        \n",
    "        # Dữ liệu đã làm sạch\n",
    "        \"title_clean\": clean_title_text,\n",
    "        \"content_clean\": content_result[\"content_clean\"],\n",
    "        \"sentences\": content_result[\"sentences\"],\n",
    "        \n",
    "        # Metadata\n",
    "        \"num_sentences\": content_result[\"num_sentences\"],\n",
    "        \"num_words\": content_result[\"num_words\"],\n",
    "        \"content_hash\": filtered_doc.get(\"content_hash\"),\n",
    "        \n",
    "        # Phân loại\n",
    "        \"published_date\": filtered_doc.get(\"published_date\"),\n",
    "        \"category\": filtered_doc.get(\"category\"),\n",
    "        \"category_name\": filtered_doc.get(\"category_name\"),\n",
    "        \"is_hcmc\": filtered_doc.get(\"is_hcmc\"),\n",
    "        \"is_traffic\": filtered_doc.get(\"is_traffic\"),\n",
    "        \n",
    "        # Timestamps\n",
    "        \"scraped_at\": filtered_doc.get(\"scraped_at\"),\n",
    "        \"filtered_at\": filtered_doc.get(\"filtered_at\"),\n",
    "        \"cleaned_at\": datetime.now(),\n",
    "        \"processing_status\": \"cleaned\"  # Đánh dấu đã làm sạch\n",
    "    }\n",
    "    \n",
    "    return clean_doc\n",
    "\n",
    "def run_cleaning(batch_size=100):\n",
    "    \"\"\"\n",
    "    Chạy quá trình làm sạch cho toàn bộ filtered_data\n",
    "    \n",
    "    Args:\n",
    "        batch_size: Số lượng document xử lý mỗi lần\n",
    "    \n",
    "    Returns:\n",
    "        Dict chứa thống kê\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"🧹 GIAI ĐOẠN 3: LÀM SẠCH DỮ LIỆU\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Kết nối MongoDB\n",
    "    filtered_col, clean_col = connect_mongodb()\n",
    "    if filtered_col is None or clean_col is None:\n",
    "        print(\"❌ Không thể kết nối MongoDB\")\n",
    "        return None\n",
    "    \n",
    "    # Lấy tất cả filtered documents chưa làm sạch\n",
    "    total_filtered = filtered_col.count_documents({\"processing_status\": \"filtered\"})\n",
    "    print(f\"📊 Tổng số bài cần làm sạch: {total_filtered}\")\n",
    "    \n",
    "    stats = {\n",
    "        \"total_processed\": 0,\n",
    "        \"cleaned\": 0,\n",
    "        \"errors\": 0,\n",
    "        \"total_sentences\": 0,\n",
    "        \"total_words\": 0\n",
    "    }\n",
    "    \n",
    "    # Xử lý từng batch\n",
    "    cursor = filtered_col.find({\"processing_status\": \"filtered\"}).batch_size(batch_size)\n",
    "    \n",
    "    for idx, filtered_doc in enumerate(cursor, 1):\n",
    "        print(f\"\\r[{idx}/{total_filtered}] Đang xử lý...\", end=\"\", flush=True)\n",
    "        \n",
    "        stats[\"total_processed\"] += 1\n",
    "        \n",
    "        try:\n",
    "            # Làm sạch bài viết\n",
    "            clean_doc = clean_article(filtered_doc)\n",
    "            \n",
    "            # Lưu vào clean_data\n",
    "            try:\n",
    "                clean_col.insert_one(clean_doc)\n",
    "                stats[\"cleaned\"] += 1\n",
    "                stats[\"total_sentences\"] += clean_doc[\"num_sentences\"]\n",
    "                stats[\"total_words\"] += clean_doc[\"num_words\"]\n",
    "                \n",
    "                # Cập nhật trạng thái trong filtered_data\n",
    "                filtered_col.update_one(\n",
    "                    {\"_id\": filtered_doc[\"_id\"]},\n",
    "                    {\"$set\": {\"processing_status\": \"cleaned\"}}\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"\\n  ❌ Lỗi lưu DB: {e}\")\n",
    "                stats[\"errors\"] += 1\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"\\n  ❌ Lỗi xử lý: {e}\")\n",
    "            stats[\"errors\"] += 1\n",
    "    \n",
    "    print()  # Xuống dòng sau progress bar\n",
    "    \n",
    "    # In báo cáo\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"✅ HOÀN THÀNH GIAI ĐOẠN 3\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"📊 Thống kê:\")\n",
    "    print(f\"   • Tổng số đã xử lý: {stats['total_processed']}\")\n",
    "    print(f\"   • Đã làm sạch thành công: {stats['cleaned']}\")\n",
    "    print(f\"   • Lỗi: {stats['errors']}\")\n",
    "    print(f\"   • Tổng số câu: {stats['total_sentences']}\")\n",
    "    print(f\"   • Tổng số từ: {stats['total_words']}\")\n",
    "    \n",
    "    if stats['cleaned'] > 0:\n",
    "        avg_sentences = stats['total_sentences'] / stats['cleaned']\n",
    "        avg_words = stats['total_words'] / stats['cleaned']\n",
    "        print(f\"   • Trung bình: {avg_sentences:.1f} câu/bài, {avg_words:.0f} từ/bài\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"➡️  Tiếp theo: Chạy 4_model_training.py để huấn luyện mô hình\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# ==================== CHẠY CHƯƠNG TRÌNH ====================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Chạy cleaning\n",
    "    results = run_cleaning(batch_size=100)\n",
    "    \n",
    "    if results:\n",
    "        print(f\"\\n✅ Dữ liệu đã làm sạch được lưu vào collection '{CLEAN_COLLECTION}'\")\n",
    "        print(f\"📊 Số bài đã xử lý: {results['cleaned']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a24335",
   "metadata": {},
   "source": [
    "GIAI ĐOẠN 4: HUẤN LUYỆN MÔ HÌNH (MODEL TRAINING)\n",
    "=================================================\n",
    "Mục tiêu:\n",
    "- Sử dụng ViT5 (Vietnamese T5) cho tóm tắt abstractive\n",
    "- Fine-tune trên dữ liệu giao thông TP.HCM\n",
    "- Chia train/val/test: 70/15/15\n",
    "- Đánh giá bằng ROUGE, BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caad2207",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GIAI ĐOẠN 4: HUẤN LUYỆN MÔ HÌNH (MODEL TRAINING)\n",
    "=================================================\n",
    "Mục tiêu:\n",
    "- Sử dụng ViT5 (Vietnamese T5) cho tóm tắt abstractive\n",
    "- Fine-tune trên dữ liệu giao thông TP.HCM\n",
    "- Chia train/val/test: 70/15/15\n",
    "- Đánh giá bằng ROUGE, BLEU\n",
    "\"\"\"\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Đánh giá metrics\n",
    "try:\n",
    "    from rouge_score import rouge_scorer\n",
    "    ROUGE_AVAILABLE = True\n",
    "except:\n",
    "    ROUGE_AVAILABLE = False\n",
    "    print(\"⚠️  Chưa cài rouge_score. Chạy: pip install rouge-score\")\n",
    "\n",
    "# ==================== CẤU HÌNH ====================\n",
    "\n",
    "# MongoDB\n",
    "MONGODB_URI = \"mongodb://localhost:27017/\"\n",
    "DATABASE_NAME = \"hcmc_traffic_summarization\"\n",
    "CLEAN_COLLECTION = \"clean_data\"\n",
    "\n",
    "# Mô hình\n",
    "MODEL_NAME = \"VietAI/vit5-base\"  # Mô hình ViT5 pre-trained\n",
    "OUTPUT_DIR = \"./models/vit5_hcmc_traffic\"  # Thư mục lưu model\n",
    "\n",
    "# Training hyperparameters\n",
    "MAX_INPUT_LENGTH = 512  # Độ dài tối đa của input (bài báo)\n",
    "MAX_TARGET_LENGTH = 128  # Độ dài tối đa của summary\n",
    "BATCH_SIZE = 4  # Giảm nếu thiếu RAM/VRAM\n",
    "LEARNING_RATE = 5e-5\n",
    "NUM_EPOCHS = 5\n",
    "WARMUP_STEPS = 500\n",
    "\n",
    "# Data split\n",
    "TRAIN_RATIO = 0.70\n",
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.15\n",
    "\n",
    "# ==================== KẾT NỐI DATABASE ====================\n",
    "\n",
    "def connect_mongodb():\n",
    "    \"\"\"\n",
    "    Kết nối MongoDB và lấy clean_data\n",
    "    \n",
    "    Returns:\n",
    "        Collection object hoặc None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = MongoClient(MONGODB_URI, serverSelectionTimeoutMS=5000)\n",
    "        client.server_info()\n",
    "        \n",
    "        db = client[DATABASE_NAME]\n",
    "        clean_col = db[CLEAN_COLLECTION]\n",
    "        \n",
    "        total = clean_col.count_documents({})\n",
    "        print(f\"✅ Đã kết nối MongoDB\")\n",
    "        print(f\"   Clean data: {total} bài\")\n",
    "        return clean_col\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Lỗi kết nối MongoDB: {e}\")\n",
    "        return None\n",
    "\n",
    "# ==================== CHUẨN BỊ DỮ LIỆU ====================\n",
    "\n",
    "def load_data_from_mongodb(clean_col):\n",
    "    \"\"\"\n",
    "    Load dữ liệu từ MongoDB và chuẩn bị cho training\n",
    "    \n",
    "    Args:\n",
    "        clean_col: MongoDB collection\n",
    "    \n",
    "    Returns:\n",
    "        List of dicts với format {content, summary}\n",
    "    \"\"\"\n",
    "    print(f\"\\n📥 Đang load dữ liệu từ MongoDB...\")\n",
    "    \n",
    "    data = []\n",
    "    cursor = clean_col.find({\n",
    "        \"processing_status\": \"cleaned\",\n",
    "        \"num_sentences\": {\"$gte\": 5}  # Chỉ lấy bài có ít nhất 5 câu\n",
    "    })\n",
    "    \n",
    "    for doc in cursor:\n",
    "        # Input: Nội dung bài báo đã làm sạch\n",
    "        content = doc.get(\"content_clean\", \"\")\n",
    "        \n",
    "        # Target: Tạo summary từ 3 câu đầu tiên (pseudo-label)\n",
    "        # LƯU Ý: Đây là phương pháp tạm thời. Tốt nhất là có human-labeled summaries\n",
    "        sentences = doc.get(\"sentences\", [])\n",
    "        if len(sentences) >= 5:\n",
    "            # Lấy 3 câu đầu làm summary (extractive)\n",
    "            summary = \". \".join(sentences[:3]) + \".\"\n",
    "            \n",
    "            data.append({\n",
    "                \"content\": content,\n",
    "                \"summary\": summary,\n",
    "                \"category\": doc.get(\"category\", \"general\"),\n",
    "                \"url\": doc.get(\"url\", \"\")\n",
    "            })\n",
    "    \n",
    "    print(f\"✅ Đã load {len(data)} bài có đủ điều kiện\")\n",
    "    return data\n",
    "\n",
    "def split_dataset(data):\n",
    "    \"\"\"\n",
    "    Chia dataset thành train/val/test\n",
    "    \n",
    "    Args:\n",
    "        data: List of dicts\n",
    "    \n",
    "    Returns:\n",
    "        Tuple (train_data, val_data, test_data)\n",
    "    \"\"\"\n",
    "    print(f\"\\n🔀 Đang chia dataset...\")\n",
    "    \n",
    "    # Chia train + temp (val + test)\n",
    "    train_data, temp_data = train_test_split(\n",
    "        data, \n",
    "        test_size=(VAL_RATIO + TEST_RATIO),\n",
    "        random_state=42,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Chia val và test\n",
    "    val_ratio_adjusted = VAL_RATIO / (VAL_RATIO + TEST_RATIO)\n",
    "    val_data, test_data = train_test_split(\n",
    "        temp_data,\n",
    "        test_size=(1 - val_ratio_adjusted),\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Đã chia dataset:\")\n",
    "    print(f\"   • Train: {len(train_data)} bài ({len(train_data)/len(data)*100:.1f}%)\")\n",
    "    print(f\"   • Val: {len(val_data)} bài ({len(val_data)/len(data)*100:.1f}%)\")\n",
    "    print(f\"   • Test: {len(test_data)} bài ({len(test_data)/len(data)*100:.1f}%)\")\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "def prepare_dataset(data, tokenizer):\n",
    "    \"\"\"\n",
    "    Chuẩn bị dataset cho Hugging Face Trainer\n",
    "    \n",
    "    Args:\n",
    "        data: List of dicts\n",
    "        tokenizer: Tokenizer object\n",
    "    \n",
    "    Returns:\n",
    "        Dataset object\n",
    "    \"\"\"\n",
    "    # Chuyển sang format của Hugging Face\n",
    "    dataset_dict = {\n",
    "        \"content\": [item[\"content\"] for item in data],\n",
    "        \"summary\": [item[\"summary\"] for item in data]\n",
    "    }\n",
    "    \n",
    "    dataset = Dataset.from_dict(dataset_dict)\n",
    "    \n",
    "    # Tokenize\n",
    "    def tokenize_function(examples):\n",
    "        # Tokenize input (content)\n",
    "        model_inputs = tokenizer(\n",
    "            examples[\"content\"],\n",
    "            max_length=MAX_INPUT_LENGTH,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "        \n",
    "        # Tokenize target (summary)\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(\n",
    "                examples[\"summary\"],\n",
    "                max_length=MAX_TARGET_LENGTH,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\"\n",
    "            )\n",
    "        \n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "    \n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "    \n",
    "    return tokenized_dataset\n",
    "\n",
    "# ==================== METRICS ====================\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Tính toán metrics cho evaluation\n",
    "    \n",
    "    Args:\n",
    "        eval_pred: Tuple (predictions, labels)\n",
    "    \n",
    "    Returns:\n",
    "        Dict chứa các metrics\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Decode predictions và labels\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 trong labels (padding tokens)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Tính ROUGE scores\n",
    "    if ROUGE_AVAILABLE:\n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=False)\n",
    "        \n",
    "        rouge1_scores = []\n",
    "        rouge2_scores = []\n",
    "        rougeL_scores = []\n",
    "        \n",
    "        for pred, label in zip(decoded_preds, decoded_labels):\n",
    "            scores = scorer.score(label, pred)\n",
    "            rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "            rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "            rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "        \n",
    "        return {\n",
    "            \"rouge1\": np.mean(rouge1_scores),\n",
    "            \"rouge2\": np.mean(rouge2_scores),\n",
    "            \"rougeL\": np.mean(rougeL_scores)\n",
    "        }\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "# ==================== MAIN TRAINING PIPELINE ====================\n",
    "\n",
    "def train_model():\n",
    "    \"\"\"\n",
    "    Huấn luyện mô hình ViT5 cho tóm tắt\n",
    "    \n",
    "    Returns:\n",
    "        Trainer object\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"🤖 GIAI ĐOẠN 4: HUẤN LUYỆN MÔ HÌNH\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Bước 1: Load dữ liệu từ MongoDB\n",
    "    clean_col = connect_mongodb()\n",
    "    if clean_col is None:\n",
    "        print(\"❌ Không thể kết nối MongoDB\")\n",
    "        return None\n",
    "    \n",
    "    data = load_data_from_mongodb(clean_col)\n",
    "    \n",
    "    if len(data) < 100:\n",
    "        print(f\"⚠️  Cảnh báo: Chỉ có {len(data)} bài. Cần ít nhất 100 bài để train tốt.\")\n",
    "        response = input(\"Tiếp tục? (y/n): \")\n",
    "        if response.lower() != 'y':\n",
    "            return None\n",
    "    \n",
    "    # Bước 2: Chia dataset\n",
    "    train_data, val_data, test_data = split_dataset(data)\n",
    "    \n",
    "    # Lưu test set để evaluation sau\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    with open(f\"{OUTPUT_DIR}/test_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(test_data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"✅ Đã lưu test set: {OUTPUT_DIR}/test_data.json\")\n",
    "    \n",
    "    # Bước 3: Load tokenizer và model\n",
    "    print(f\"\\n📦 Đang load mô hình {MODEL_NAME}...\")\n",
    "    \n",
    "    global tokenizer  # Để dùng trong compute_metrics\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    print(f\"✅ Đã load mô hình\")\n",
    "    \n",
    "    # Kiểm tra GPU\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"💻 Device: {device}\")\n",
    "    \n",
    "    if device == \"cpu\":\n",
    "        print(\"⚠️  Cảnh báo: Đang dùng CPU. Training sẽ rất chậm. Khuyến nghị dùng GPU.\")\n",
    "    \n",
    "    # Bước 4: Chuẩn bị datasets\n",
    "    print(f\"\\n🔨 Đang tokenize datasets...\")\n",
    "    train_dataset = prepare_dataset(train_data, tokenizer)\n",
    "    val_dataset = prepare_dataset(val_data, tokenizer)\n",
    "    print(f\"✅ Đã tokenize xong\")\n",
    "    \n",
    "    # Bước 5: Training arguments\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        evaluation_strategy=\"epoch\",  # Evaluate mỗi epoch\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        warmup_steps=WARMUP_STEPS,\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=2,  # Chỉ giữ 2 checkpoint tốt nhất\n",
    "        predict_with_generate=True,  # Generate text khi evaluate\n",
    "        fp16=True if device == \"cuda\" else False,  # Mixed precision training\n",
    "        logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    "        logging_steps=50,\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"rougeL\" if ROUGE_AVAILABLE else \"loss\",\n",
    "        greater_is_better=True if ROUGE_AVAILABLE else False,\n",
    "    )\n",
    "    \n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "    \n",
    "    # Bước 6: Khởi tạo Trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics if ROUGE_AVAILABLE else None\n",
    "    )\n",
    "    \n",
    "    # Bước 7: Bắt đầu training\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"🚀 BẮT ĐẦU TRAINING\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    print(f\"Cấu hình:\")\n",
    "    print(f\"   • Epochs: {NUM_EPOCHS}\")\n",
    "    print(f\"   • Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"   • Learning rate: {LEARNING_RATE}\")\n",
    "    print(f\"   • Device: {device}\")\n",
    "    print(f\"\\n⏱️  Thời gian dự kiến: {len(train_data) * NUM_EPOCHS / BATCH_SIZE / 60:.1f} phút\")\n",
    "    print(f\"\\nĐang training...\\n\")\n",
    "    \n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    # Bước 8: Lưu model\n",
    "    print(f\"\\n💾 Đang lưu model...\")\n",
    "    trainer.save_model(OUTPUT_DIR)\n",
    "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "    \n",
    "    # Lưu training metrics\n",
    "    metrics = train_result.metrics\n",
    "    with open(f\"{OUTPUT_DIR}/train_metrics.json\", \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"✅ HOÀN THÀNH TRAINING\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"📊 Training metrics:\")\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"   • {key}: {value:.4f}\")\n",
    "    \n",
    "    print(f\"\\n💾 Model đã được lưu tại: {OUTPUT_DIR}\")\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"➡️  Tiếp theo: Chạy 5_model_evaluation.py để đánh giá chi tiết\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return trainer\n",
    "\n",
    "# ==================== CHẠY CHƯƠNG TRÌNH ====================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    \n",
    "    print(f\"\\n⚠️  LƯU Ý QUAN TRỌNG:\")\n",
    "    print(f\"   • Training mô hình cần GPU mạnh (ít nhất 8GB VRAM)\")\n",
    "    print(f\"   • Thời gian: 1-3 giờ tùy số lượng bài và GPU\")\n",
    "    print(f\"   • Cần cài đặt: pip install transformers datasets torch rouge-score\")\n",
    "    print(f\"   • Hiện tại đang dùng pseudo-labels (3 câu đầu). Tốt nhất có human-labeled data\\n\")\n",
    "    \n",
    "    # Hỏi xác nhận\n",
    "    if len(sys.argv) > 1 and sys.argv[1] == \"--auto\":\n",
    "        # Auto mode: không hỏi\n",
    "        pass\n",
    "    else:\n",
    "        response = input(\"Bạn có muốn tiếp tục training? (y/n): \")\n",
    "        if response.lower() != 'y':\n",
    "            print(\"❌ Đã hủy training\")\n",
    "            sys.exit(0)\n",
    "    \n",
    "    # Bắt đầu training\n",
    "    trainer = train_model()\n",
    "    \n",
    "    if trainer:\n",
    "        print(f\"\\n✅ Training thành công!\")\n",
    "        print(f\"📁 Model được lưu tại: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf43f07e",
   "metadata": {},
   "source": [
    "GIAI ĐOẠN 5: ĐÁNH GIÁ MÔ HÌNH (MODEL EVALUATION)\n",
    "=================================================\n",
    "Mục tiêu:\n",
    "- Đánh giá mô hình trên test set\n",
    "- Tính ROUGE, BLEU scores\n",
    "- Kiểm tra overfitting\n",
    "- Phân tích lỗi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfe8402",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GIAI ĐOẠN 5: ĐÁNH GIÁ MÔ HÌNH (MODEL EVALUATION)\n",
    "=================================================\n",
    "Mục tiêu:\n",
    "- Đánh giá mô hình trên test set\n",
    "- Tính ROUGE, BLEU scores\n",
    "- Kiểm tra overfitting\n",
    "- Phân tích lỗi\n",
    "\"\"\"\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Metrics\n",
    "try:\n",
    "    from rouge_score import rouge_scorer\n",
    "    ROUGE_AVAILABLE = True\n",
    "except:\n",
    "    ROUGE_AVAILABLE = False\n",
    "    print(\"⚠️  Cần cài: pip install rouge-score\")\n",
    "\n",
    "try:\n",
    "    from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "    BLEU_AVAILABLE = True\n",
    "except:\n",
    "    BLEU_AVAILABLE = False\n",
    "    print(\"⚠️  Cần cài: pip install nltk\")\n",
    "\n",
    "# ==================== CẤU HÌNH ====================\n",
    "\n",
    "MODEL_DIR = \"./models/vit5_hcmc_traffic\"  # Thư mục chứa model đã train\n",
    "TEST_DATA_PATH = f\"{MODEL_DIR}/test_data.json\"  # Test set\n",
    "\n",
    "# Generation parameters\n",
    "MAX_LENGTH = 128\n",
    "MIN_LENGTH = 30\n",
    "NUM_BEAMS = 4  # Beam search\n",
    "NO_REPEAT_NGRAM_SIZE = 3  # Tránh lặp n-gram\n",
    "\n",
    "# ==================== LOAD MODEL ====================\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"\n",
    "    Load mô hình đã fine-tune\n",
    "    \n",
    "    Returns:\n",
    "        Tuple (model, tokenizer, device)\n",
    "    \"\"\"\n",
    "    print(f\"\\n📦 Đang load mô hình từ {MODEL_DIR}...\")\n",
    "    \n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_DIR)\n",
    "        \n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        model = model.to(device)\n",
    "        model.eval()  # Evaluation mode\n",
    "        \n",
    "        print(f\"✅ Đã load mô hình\")\n",
    "        print(f\"💻 Device: {device}\")\n",
    "        \n",
    "        return model, tokenizer, device\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Lỗi load model: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# ==================== GENERATE SUMMARY ====================\n",
    "\n",
    "def generate_summary(content, model, tokenizer, device):\n",
    "    \"\"\"\n",
    "    Tạo summary từ content\n",
    "    \n",
    "    Args:\n",
    "        content: Nội dung bài báo\n",
    "        model: Model object\n",
    "        tokenizer: Tokenizer object\n",
    "        device: Device (cuda/cpu)\n",
    "    \n",
    "    Returns:\n",
    "        str: Summary đã generate\n",
    "    \"\"\"\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(\n",
    "        content,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_length=MAX_LENGTH,\n",
    "            min_length=MIN_LENGTH,\n",
    "            num_beams=NUM_BEAMS,\n",
    "            no_repeat_ngram_size=NO_REPEAT_NGRAM_SIZE,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# ==================== METRICS CALCULATION ====================\n",
    "\n",
    "def calculate_rouge(predictions, references):\n",
    "    \"\"\"\n",
    "    Tính ROUGE scores\n",
    "    \n",
    "    Args:\n",
    "        predictions: List of generated summaries\n",
    "        references: List of reference summaries\n",
    "    \n",
    "    Returns:\n",
    "        Dict chứa ROUGE scores\n",
    "    \"\"\"\n",
    "    if not ROUGE_AVAILABLE:\n",
    "        return {}\n",
    "    \n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=False)\n",
    "    \n",
    "    rouge1_scores = []\n",
    "    rouge2_scores = []\n",
    "    rougeL_scores = []\n",
    "    \n",
    "    for pred, ref in zip(predictions, references):\n",
    "        scores = scorer.score(ref, pred)\n",
    "        rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "        rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "        rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "    \n",
    "    return {\n",
    "        \"rouge1\": {\n",
    "            \"mean\": np.mean(rouge1_scores),\n",
    "            \"std\": np.std(rouge1_scores),\n",
    "            \"min\": np.min(rouge1_scores),\n",
    "            \"max\": np.max(rouge1_scores)\n",
    "        },\n",
    "        \"rouge2\": {\n",
    "            \"mean\": np.mean(rouge2_scores),\n",
    "            \"std\": np.std(rouge2_scores),\n",
    "            \"min\": np.min(rouge2_scores),\n",
    "            \"max\": np.max(rouge2_scores)\n",
    "        },\n",
    "        \"rougeL\": {\n",
    "            \"mean\": np.mean(rougeL_scores),\n",
    "            \"std\": np.std(rougeL_scores),\n",
    "            \"min\": np.min(rougeL_scores),\n",
    "            \"max\": np.max(rougeL_scores)\n",
    "        }\n",
    "    }\n",
    "\n",
    "def calculate_bleu(predictions, references):\n",
    "    \"\"\"\n",
    "    Tính BLEU scores\n",
    "    \n",
    "    Args:\n",
    "        predictions: List of generated summaries\n",
    "        references: List of reference summaries\n",
    "    \n",
    "    Returns:\n",
    "        Dict chứa BLEU scores\n",
    "    \"\"\"\n",
    "    if not BLEU_AVAILABLE:\n",
    "        return {}\n",
    "    \n",
    "    smoothing = SmoothingFunction().method1\n",
    "    bleu_scores = []\n",
    "    \n",
    "    for pred, ref in zip(predictions, references):\n",
    "        pred_tokens = pred.split()\n",
    "        ref_tokens = [ref.split()]  # BLEU expects list of references\n",
    "        \n",
    "        try:\n",
    "            score = sentence_bleu(ref_tokens, pred_tokens, smoothing_function=smoothing)\n",
    "            bleu_scores.append(score)\n",
    "        except:\n",
    "            bleu_scores.append(0.0)\n",
    "    \n",
    "    return {\n",
    "        \"bleu\": {\n",
    "            \"mean\": np.mean(bleu_scores),\n",
    "            \"std\": np.std(bleu_scores),\n",
    "            \"min\": np.min(bleu_scores),\n",
    "            \"max\": np.max(bleu_scores)\n",
    "        }\n",
    "    }\n",
    "\n",
    "def calculate_length_stats(predictions, references):\n",
    "    \"\"\"\n",
    "    Thống kê độ dài summaries\n",
    "    \n",
    "    Args:\n",
    "        predictions: List of generated summaries\n",
    "        references: List of reference summaries\n",
    "    \n",
    "    Returns:\n",
    "        Dict chứa thống kê độ dài\n",
    "    \"\"\"\n",
    "    pred_lengths = [len(p.split()) for p in predictions]\n",
    "    ref_lengths = [len(r.split()) for r in references]\n",
    "    \n",
    "    return {\n",
    "        \"prediction_length\": {\n",
    "            \"mean\": np.mean(pred_lengths),\n",
    "            \"std\": np.std(pred_lengths),\n",
    "            \"min\": np.min(pred_lengths),\n",
    "            \"max\": np.max(pred_lengths)\n",
    "        },\n",
    "        \"reference_length\": {\n",
    "            \"mean\": np.mean(ref_lengths),\n",
    "            \"std\": np.std(ref_lengths)\n",
    "        }\n",
    "    }\n",
    "\n",
    "# ==================== OVERFITTING CHECK ====================\n",
    "\n",
    "def check_overfitting():\n",
    "    \"\"\"\n",
    "    Kiểm tra overfitting bằng cách so sánh train và val loss\n",
    "    \n",
    "    Returns:\n",
    "        Dict chứa phân tích overfitting\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(f\"{MODEL_DIR}/train_metrics.json\", \"r\") as f:\n",
    "            train_metrics = json.load(f)\n",
    "        \n",
    "        # Lấy train và eval loss từ trainer_state.json\n",
    "        with open(f\"{MODEL_DIR}/trainer_state.json\", \"r\") as f:\n",
    "            trainer_state = json.load(f)\n",
    "        \n",
    "        log_history = trainer_state.get(\"log_history\", [])\n",
    "        \n",
    "        train_losses = []\n",
    "        eval_losses = []\n",
    "        \n",
    "        for log in log_history:\n",
    "            if \"loss\" in log:\n",
    "                train_losses.append(log[\"loss\"])\n",
    "            if \"eval_loss\" in log:\n",
    "                eval_losses.append(log[\"eval_loss\"])\n",
    "        \n",
    "        # Phân tích overfitting\n",
    "        if len(train_losses) > 0 and len(eval_losses) > 0:\n",
    "            final_train_loss = train_losses[-1]\n",
    "            final_eval_loss = eval_losses[-1]\n",
    "            \n",
    "            gap = final_eval_loss - final_train_loss\n",
    "            gap_percent = (gap / final_train_loss) * 100\n",
    "            \n",
    "            if gap_percent > 20:\n",
    "                status = \"HIGH_OVERFITTING\"\n",
    "                message = \"Mô hình bị overfit nghiêm trọng. Cần giảm model complexity hoặc tăng data.\"\n",
    "            elif gap_percent > 10:\n",
    "                status = \"MODERATE_OVERFITTING\"\n",
    "                message = \"Mô hình có dấu hiệu overfit nhẹ. Có thể chấp nhận được.\"\n",
    "            else:\n",
    "                status = \"GOOD\"\n",
    "                message = \"Mô hình học tốt, không bị overfit.\"\n",
    "            \n",
    "            return {\n",
    "                \"status\": status,\n",
    "                \"message\": message,\n",
    "                \"final_train_loss\": final_train_loss,\n",
    "                \"final_eval_loss\": final_eval_loss,\n",
    "                \"gap\": gap,\n",
    "                \"gap_percent\": gap_percent,\n",
    "                \"train_losses\": train_losses,\n",
    "                \"eval_losses\": eval_losses\n",
    "            }\n",
    "        \n",
    "        return {\"status\": \"UNKNOWN\", \"message\": \"Không đủ dữ liệu để phân tích\"}\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\"status\": \"ERROR\", \"message\": str(e)}\n",
    "\n",
    "# ==================== MAIN EVALUATION ====================\n",
    "\n",
    "def run_evaluation():\n",
    "    \"\"\"\n",
    "    Chạy evaluation trên test set\n",
    "    \n",
    "    Returns:\n",
    "        Dict chứa kết quả đánh giá\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"📊 GIAI ĐOẠN 5: ĐÁNH GIÁ MÔ HÌNH\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Bước 1: Load model\n",
    "    model, tokenizer, device = load_model()\n",
    "    if model is None:\n",
    "        print(\"❌ Không thể load model. Hãy chạy 4_model_training.py trước.\")\n",
    "        return None\n",
    "    \n",
    "    # Bước 2: Load test data\n",
    "    print(f\"\\n📥 Đang load test data từ {TEST_DATA_PATH}...\")\n",
    "    try:\n",
    "        with open(TEST_DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "            test_data = json.load(f)\n",
    "        print(f\"✅ Đã load {len(test_data)} bài test\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Lỗi load test data: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Bước 3: Generate summaries\n",
    "    print(f\"\\n🤖 Đang generate summaries...\")\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    for idx, item in enumerate(test_data, 1):\n",
    "        print(f\"\\r[{idx}/{len(test_data)}] Đang generate...\", end=\"\", flush=True)\n",
    "        \n",
    "        content = item[\"content\"]\n",
    "        reference = item[\"summary\"]\n",
    "        \n",
    "        # Generate summary\n",
    "        prediction = generate_summary(content, model, tokenizer, device)\n",
    "        \n",
    "        predictions.append(prediction)\n",
    "        references.append(reference)\n",
    "    \n",
    "    print()  # Xuống dòng sau progress bar\n",
    "    \n",
    "    # Bước 4: Tính metrics\n",
    "    print(f\"\\n📏 Đang tính metrics...\")\n",
    "    \n",
    "    rouge_scores = calculate_rouge(predictions, references)\n",
    "    bleu_scores = calculate_bleu(predictions, references)\n",
    "    length_stats = calculate_length_stats(predictions, references)\n",
    "    \n",
    "    # Bước 5: Kiểm tra overfitting\n",
    "    print(f\"\\n🔍 Đang kiểm tra overfitting...\")\n",
    "    overfitting_analysis = check_overfitting()\n",
    "    \n",
    "    # Bước 6: Tổng hợp kết quả\n",
    "    results = {\n",
    "        \"test_size\": len(test_data),\n",
    "        \"rouge_scores\": rouge_scores,\n",
    "        \"bleu_scores\": bleu_scores,\n",
    "        \"length_stats\": length_stats,\n",
    "        \"overfitting_analysis\": overfitting_analysis,\n",
    "        \"evaluated_at\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    # Lưu kết quả\n",
    "    results_path = f\"{MODEL_DIR}/evaluation_results.json\"\n",
    "    with open(results_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # Lưu ví dụ predictions\n",
    "    examples_path = f\"{MODEL_DIR}/evaluation_examples.json\"\n",
    "    examples = []\n",
    "    for i in range(min(10, len(test_data))):  # Lưu 10 ví dụ\n",
    "        examples.append({\n",
    "            \"content\": test_data[i][\"content\"][:200] + \"...\",  # Chỉ lưu 200 ký tự đầu\n",
    "            \"reference\": references[i],\n",
    "            \"prediction\": predictions[i],\n",
    "            \"url\": test_data[i].get(\"url\", \"\")\n",
    "        })\n",
    "    \n",
    "    with open(examples_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(examples, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # In báo cáo\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"✅ HOÀN THÀNH ĐÁNH GIÁ\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    print(f\"\\n📊 KẾT QUẢ ĐÁNH GIÁ:\")\n",
    "    print(f\"\\n🎯 ROUGE Scores:\")\n",
    "    if rouge_scores:\n",
    "        for metric, values in rouge_scores.items():\n",
    "            print(f\"   • {metric.upper()}: {values['mean']:.4f} ± {values['std']:.4f}\")\n",
    "            print(f\"     (min: {values['min']:.4f}, max: {values['max']:.4f})\")\n",
    "    else:\n",
    "        print(\"   ⚠️  Không tính được ROUGE scores\")\n",
    "    \n",
    "    print(f\"\\n📝 BLEU Scores:\")\n",
    "    if bleu_scores:\n",
    "        print(f\"   • BLEU: {bleu_scores['bleu']['mean']:.4f} ± {bleu_scores['bleu']['std']:.4f}\")\n",
    "    else:\n",
    "        print(\"   ⚠️  Không tính được BLEU scores\")\n",
    "    \n",
    "    print(f\"\\n📏 Độ dài Summary:\")\n",
    "    print(f\"   • Generated: {length_stats['prediction_length']['mean']:.1f} từ \"\n",
    "          f\"(min: {length_stats['prediction_length']['min']}, \"\n",
    "          f\"max: {length_stats['prediction_length']['max']})\")\n",
    "    print(f\"   • Reference: {length_stats['reference_length']['mean']:.1f} từ\")\n",
    "    \n",
    "    print(f\"\\n🔍 Phân tích Overfitting:\")\n",
    "    print(f\"   • Status: {overfitting_analysis.get('status', 'UNKNOWN')}\")\n",
    "    print(f\"   • {overfitting_analysis.get('message', 'N/A')}\")\n",
    "    if 'gap_percent' in overfitting_analysis:\n",
    "        print(f\"   • Train/Val gap: {overfitting_analysis['gap_percent']:.2f}%\")\n",
    "    \n",
    "    # Đánh giá tổng thể\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"💡 ĐÁNH GIÁ TỔNG QUAN:\")\n",
    "    \n",
    "    if rouge_scores:\n",
    "        rougeL_mean = rouge_scores['rougeL']['mean']\n",
    "        \n",
    "        if rougeL_mean >= 0.4:\n",
    "            quality = \"TỐT\"\n",
    "            emoji = \"🟢\"\n",
    "            comment = \"Mô hình hoạt động tốt!\"\n",
    "        elif rougeL_mean >= 0.3:\n",
    "            quality = \"KHÁ\"\n",
    "            emoji = \"🟡\"\n",
    "            comment = \"Mô hình hoạt động ổn. Có thể cải thiện thêm.\"\n",
    "        else:\n",
    "            quality = \"CẦN CẢI THIỆN\"\n",
    "            emoji = \"🔴\"\n",
    "            comment = \"Mô hình cần fine-tune thêm hoặc tăng data.\"\n",
    "        \n",
    "        print(f\"{emoji} Chất lượng: {quality}\")\n",
    "        print(f\"   {comment}\")\n",
    "    \n",
    "    print(f\"\\n📁 Kết quả đã lưu:\")\n",
    "    print(f\"   • Chi tiết: {results_path}\")\n",
    "    print(f\"   • Ví dụ: {examples_path}\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"➡️  Tiếp theo: Chạy 6_generate_summaries.py để tóm tắt toàn bộ dữ liệu\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ==================== VẼ BIỂU ĐỒ ====================\n",
    "\n",
    "def plot_training_curves():\n",
    "    \"\"\"\n",
    "    Vẽ biểu đồ train/val loss theo epoch\n",
    "    \"\"\"\n",
    "    print(f\"\\n📈 Đang vẽ biểu đồ training curves...\")\n",
    "    \n",
    "    try:\n",
    "        overfitting_analysis = check_overfitting()\n",
    "        \n",
    "        if overfitting_analysis['status'] not in ['UNKNOWN', 'ERROR']:\n",
    "            train_losses = overfitting_analysis['train_losses']\n",
    "            eval_losses = overfitting_analysis['eval_losses']\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            \n",
    "            # Train loss\n",
    "            plt.plot(range(1, len(train_losses) + 1), train_losses, \n",
    "                    label='Train Loss', marker='o', linewidth=2)\n",
    "            \n",
    "            # Eval loss\n",
    "            eval_epochs = np.linspace(1, len(train_losses), len(eval_losses))\n",
    "            plt.plot(eval_epochs, eval_losses, \n",
    "                    label='Validation Loss', marker='s', linewidth=2)\n",
    "            \n",
    "            plt.xlabel('Epoch', fontsize=12)\n",
    "            plt.ylabel('Loss', fontsize=12)\n",
    "            plt.title('Training và Validation Loss theo Epoch', fontsize=14, fontweight='bold')\n",
    "            plt.legend(fontsize=11)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Lưu biểu đồ\n",
    "            plot_path = f\"{MODEL_DIR}/training_curves.png\"\n",
    "            plt.savefig(plot_path, dpi=300)\n",
    "            print(f\"✅ Đã lưu biểu đồ: {plot_path}\")\n",
    "            \n",
    "            plt.close()\n",
    "        else:\n",
    "            print(f\"⚠️  Không đủ dữ liệu để vẽ biểu đồ\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Lỗi vẽ biểu đồ: {e}\")\n",
    "\n",
    "# ==================== CHẠY CHƯƠNG TRÌNH ====================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    \n",
    "    # Chạy evaluation\n",
    "    results = run_evaluation()\n",
    "    \n",
    "    if results:\n",
    "        # Vẽ biểu đồ (optional)\n",
    "        try:\n",
    "            plot_training_curves()\n",
    "        except:\n",
    "            print(\"⚠️  Không thể vẽ biểu đồ (cần matplotlib)\")\n",
    "        \n",
    "        print(f\"\\n✅ Evaluation hoàn tất!\")\n",
    "        print(f\"\\n💡 Để xem chi tiết:\")\n",
    "        print(f\"   cat {MODEL_DIR}/evaluation_results.json\")\n",
    "        print(f\"   cat {MODEL_DIR}/evaluation_examples.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea503a64",
   "metadata": {},
   "source": [
    "GIAI ĐOẠN 6: TẠO TÓM TẮT CHO TOÀN BỘ DỮ LIỆU (GENERATE SUMMARIES)\n",
    "===================================================================\n",
    "Mục tiêu:\n",
    "- Sử dụng mô hình đã fine-tune để tóm tắt tất cả bài trong clean_data\n",
    "- Thêm phân tích ngữ cảnh giao thông (keywords extraction)\n",
    "- Lưu vào collection \"final_output\" với trường \"summary\" và \"analysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10064dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GIAI ĐOẠN 6: TẠO TÓM TẮT CHO TOÀN BỘ DỮ LIỆU (GENERATE SUMMARIES)\n",
    "===================================================================\n",
    "Mục tiêu:\n",
    "- Sử dụng mô hình đã fine-tune để tóm tắt tất cả bài trong clean_data\n",
    "- Thêm phân tích ngữ cảnh giao thông (keywords extraction)\n",
    "- Lưu vào collection \"final_output\" với trường \"summary\" và \"analysis\"\n",
    "\"\"\"\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# ==================== CẤU HÌNH ====================\n",
    "\n",
    "# MongoDB\n",
    "MONGODB_URI = \"mongodb://localhost:27017/\"\n",
    "DATABASE_NAME = \"hcmc_traffic_summarization\"\n",
    "CLEAN_COLLECTION = \"clean_data\"\n",
    "FINAL_COLLECTION = \"final_output\"  # Collection chứa kết quả cuối cùng\n",
    "\n",
    "# Model\n",
    "MODEL_DIR = \"./models/vit5_hcmc_traffic\"\n",
    "\n",
    "# Generation parameters\n",
    "MAX_LENGTH = 128\n",
    "MIN_LENGTH = 30\n",
    "NUM_BEAMS = 4\n",
    "BATCH_SIZE = 8  # Xử lý theo batch để nhanh hơn\n",
    "\n",
    "# Keywords giao thông để phân tích\n",
    "TRAFFIC_ENTITIES = {\n",
    "    \"infrastructure\": [\n",
    "        \"metro\", \"tàu điện\", \"cao tốc\", \"cầu\", \"đường vành đai\", \n",
    "        \"brt\", \"đường sắt\", \"nút giao thông\", \"cảng\", \"sân bay\"\n",
    "    ],\n",
    "    \"locations\": [\n",
    "        \"quận 1\", \"quận 2\", \"quận 3\", \"thủ đức\", \"bình thạnh\",\n",
    "        \"tân bình\", \"gò vấp\", \"bình tân\", \"cầu sài gòn\"\n",
    "    ],\n",
    "    \"issues\": [\n",
    "        \"kẹt xe\", \"ùn tắc\", \"tai nạn\", \"ngập nước\", \"ổ gà\", \"hư hỏng\"\n",
    "    ],\n",
    "    \"projects\": [\n",
    "        \"dự án\", \"khởi công\", \"hoàn thành\", \"triển khai\", \"đầu tư\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# ==================== KẾT NỐI DATABASE ====================\n",
    "\n",
    "def connect_mongodb():\n",
    "    \"\"\"\n",
    "    Kết nối MongoDB và tạo collection final_output\n",
    "    \n",
    "    Returns:\n",
    "        Tuple (clean_collection, final_collection)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = MongoClient(MONGODB_URI, serverSelectionTimeoutMS=5000)\n",
    "        client.server_info()\n",
    "        \n",
    "        db = client[DATABASE_NAME]\n",
    "        clean_col = db[CLEAN_COLLECTION]\n",
    "        final_col = db[FINAL_COLLECTION]\n",
    "        \n",
    "        # Tạo index\n",
    "        final_col.create_index([(\"url\", 1)], unique=True)\n",
    "        final_col.create_index([(\"published_date\", -1)])\n",
    "        final_col.create_index([(\"category\", 1)])\n",
    "        \n",
    "        # Full-text search index\n",
    "        final_col.create_index([\n",
    "            (\"title_clean\", \"text\"),\n",
    "            (\"summary\", \"text\"),\n",
    "            (\"analysis\", \"text\")\n",
    "        ])\n",
    "        \n",
    "        print(f\"✅ Đã kết nối MongoDB\")\n",
    "        print(f\"   Clean data: {clean_col.count_documents({})}\")\n",
    "        return clean_col, final_col\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Lỗi kết nối MongoDB: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# ==================== LOAD MODEL ====================\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"\n",
    "    Load mô hình đã fine-tune\n",
    "    \n",
    "    Returns:\n",
    "        Tuple (model, tokenizer, device)\n",
    "    \"\"\"\n",
    "    print(f\"\\n📦 Đang load mô hình từ {MODEL_DIR}...\")\n",
    "    \n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_DIR)\n",
    "        \n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        print(f\"✅ Đã load mô hình\")\n",
    "        print(f\"💻 Device: {device}\")\n",
    "        \n",
    "        return model, tokenizer, device\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Lỗi load model: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# ==================== TEXT ANALYSIS ====================\n",
    "\n",
    "def extract_keywords(text, top_n=10):\n",
    "    \"\"\"\n",
    "    Trích xuất keywords từ văn bản\n",
    "    \n",
    "    Args:\n",
    "        text: Văn bản cần trích xuất\n",
    "        top_n: Số lượng keywords\n",
    "    \n",
    "    Returns:\n",
    "        List of keywords\n",
    "    \"\"\"\n",
    "    # Tách từ\n",
    "    words = re.findall(r'\\w+', text.lower())\n",
    "    \n",
    "    # Stopwords tiếng Việt\n",
    "    stopwords = {\n",
    "        'và', 'của', 'có', 'được', 'là', 'trong', 'với', 'cho', 'các',\n",
    "        'một', 'này', 'đã', 'từ', 'những', 'để', 'người', 'không', 'như',\n",
    "        'về', 'theo', 'năm', 'tại', 'đến', 'khi', 'ngày', 'trên', 'sau',\n",
    "        'vào', 'thì', 'sẽ', 'ra', 'đang', 'nên', 'bị', 'hay', 'nhưng'\n",
    "    }\n",
    "    \n",
    "    # Lọc stopwords và từ ngắn\n",
    "    words = [w for w in words if w not in stopwords and len(w) > 3]\n",
    "    \n",
    "    # Đếm tần suất\n",
    "    word_freq = Counter(words)\n",
    "    \n",
    "    # Lấy top keywords\n",
    "    keywords = [word for word, _ in word_freq.most_common(top_n)]\n",
    "    \n",
    "    return keywords\n",
    "\n",
    "def analyze_traffic_context(text):\n",
    "    \"\"\"\n",
    "    Phân tích ngữ cảnh giao thông trong bài viết\n",
    "    \n",
    "    Args:\n",
    "        text: Văn bản cần phân tích\n",
    "    \n",
    "    Returns:\n",
    "        Dict chứa phân tích\n",
    "    \"\"\"\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    analysis = {\n",
    "        \"mentioned_infrastructure\": [],\n",
    "        \"mentioned_locations\": [],\n",
    "        \"mentioned_issues\": [],\n",
    "        \"mentioned_projects\": [],\n",
    "        \"main_topic\": \"\"\n",
    "    }\n",
    "    \n",
    "    # Phát hiện các entities giao thông\n",
    "    for category, keywords in TRAFFIC_ENTITIES.items():\n",
    "        mentioned = [kw for kw in keywords if kw in text_lower]\n",
    "        \n",
    "        if category == \"infrastructure\":\n",
    "            analysis[\"mentioned_infrastructure\"] = mentioned\n",
    "        elif category == \"locations\":\n",
    "            analysis[\"mentioned_locations\"] = mentioned\n",
    "        elif category == \"issues\":\n",
    "            analysis[\"mentioned_issues\"] = mentioned\n",
    "        elif category == \"projects\":\n",
    "            analysis[\"mentioned_projects\"] = mentioned\n",
    "    \n",
    "    # Xác định chủ đề chính\n",
    "    if analysis[\"mentioned_projects\"]:\n",
    "        analysis[\"main_topic\"] = \"Dự án giao thông\"\n",
    "    elif analysis[\"mentioned_issues\"]:\n",
    "        analysis[\"main_topic\"] = \"Vấn đề giao thông\"\n",
    "    elif analysis[\"mentioned_infrastructure\"]:\n",
    "        analysis[\"main_topic\"] = \"Hạ tầng giao thông\"\n",
    "    else:\n",
    "        analysis[\"main_topic\"] = \"Giao thông chung\"\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "def generate_analysis_text(analysis, keywords):\n",
    "    \"\"\"\n",
    "    Tạo văn bản phân tích từ dữ liệu analysis\n",
    "    \n",
    "    Args:\n",
    "        analysis: Dict từ analyze_traffic_context\n",
    "        keywords: List of keywords\n",
    "    \n",
    "    Returns:\n",
    "        str: Văn bản phân tích\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    \n",
    "    # Chủ đề chính\n",
    "    parts.append(f\"Chủ đề: {analysis['main_topic']}.\")\n",
    "    \n",
    "    # Hạ tầng được đề cập\n",
    "    if analysis['mentioned_infrastructure']:\n",
    "        infra = \", \".join(analysis['mentioned_infrastructure'][:3])\n",
    "        parts.append(f\"Hạ tầng: {infra}.\")\n",
    "    \n",
    "    # Địa điểm\n",
    "    if analysis['mentioned_locations']:\n",
    "        locs = \", \".join(analysis['mentioned_locations'][:3])\n",
    "        parts.append(f\"Khu vực: {locs}.\")\n",
    "    \n",
    "    # Vấn đề\n",
    "    if analysis['mentioned_issues']:\n",
    "        issues = \", \".join(analysis['mentioned_issues'][:3])\n",
    "        parts.append(f\"Vấn đề: {issues}.\")\n",
    "    \n",
    "    # Keywords\n",
    "    if keywords:\n",
    "        kw = \", \".join(keywords[:5])\n",
    "        parts.append(f\"Từ khóa: {kw}.\")\n",
    "    \n",
    "    return \" \".join(parts)\n",
    "\n",
    "# ==================== GENERATE SUMMARY ====================\n",
    "\n",
    "def generate_summary(content, model, tokenizer, device):\n",
    "    \"\"\"\n",
    "    Tạo summary từ content\n",
    "    \n",
    "    Args:\n",
    "        content: Nội dung bài báo\n",
    "        model: Model object\n",
    "        tokenizer: Tokenizer object\n",
    "        device: Device (cuda/cpu)\n",
    "    \n",
    "    Returns:\n",
    "        str: Summary đã generate\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        content,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_length=MAX_LENGTH,\n",
    "            min_length=MIN_LENGTH,\n",
    "            num_beams=NUM_BEAMS,\n",
    "            no_repeat_ngram_size=3,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# ==================== MAIN PIPELINE ====================\n",
    "\n",
    "def process_article(clean_doc, model, tokenizer, device):\n",
    "    \"\"\"\n",
    "    Xử lý 1 bài viết: tóm tắt + phân tích\n",
    "    \n",
    "    Args:\n",
    "        clean_doc: Document từ clean_data\n",
    "        model, tokenizer, device: Model components\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Document cho final_output\n",
    "    \"\"\"\n",
    "    content = clean_doc.get(\"content_clean\", \"\")\n",
    "    title = clean_doc.get(\"title_clean\", \"\")\n",
    "    \n",
    "    # Bước 1: Generate summary bằng model\n",
    "    summary = generate_summary(content, model, tokenizer, device)\n",
    "    \n",
    "    # Bước 2: Trích xuất keywords\n",
    "    keywords = extract_keywords(content, top_n=10)\n",
    "    \n",
    "    # Bước 3: Phân tích ngữ cảnh giao thông\n",
    "    analysis_data = analyze_traffic_context(content)\n",
    "    \n",
    "    # Bước 4: Tạo văn bản phân tích\n",
    "    analysis_text = generate_analysis_text(analysis_data, keywords)\n",
    "    \n",
    "    # Tạo final document\n",
    "    final_doc = {\n",
    "        \"url\": clean_doc.get(\"url\"),\n",
    "        \"source\": clean_doc.get(\"source\"),\n",
    "        \"source_name\": clean_doc.get(\"source_name\"),\n",
    "        \n",
    "        # Dữ liệu gốc\n",
    "        \"title_clean\": title,\n",
    "        \"content_clean\": content,  # Giữ lại content gốc\n",
    "        \n",
    "        # Kết quả chính: SUMMARY và ANALYSIS\n",
    "        \"summary\": summary,  # Tóm tắt bằng AI\n",
    "        \"analysis\": analysis_text,  # Phân tích ngữ cảnh\n",
    "        \n",
    "        # Metadata bổ sung\n",
    "        \"keywords\": keywords,\n",
    "        \"traffic_entities\": analysis_data,\n",
    "        \n",
    "        # Phân loại\n",
    "        \"published_date\": clean_doc.get(\"published_date\"),\n",
    "        \"category\": clean_doc.get(\"category\"),\n",
    "        \"category_name\": clean_doc.get(\"category_name\"),\n",
    "        \n",
    "        # Thống kê\n",
    "        \"num_sentences\": clean_doc.get(\"num_sentences\"),\n",
    "        \"num_words\": clean_doc.get(\"num_words\"),\n",
    "        \"summary_length\": len(summary.split()),\n",
    "        \n",
    "        # Timestamps\n",
    "        \"scraped_at\": clean_doc.get(\"scraped_at\"),\n",
    "        \"cleaned_at\": clean_doc.get(\"cleaned_at\"),\n",
    "        \"summarized_at\": datetime.now(),\n",
    "        \"processing_status\": \"completed\"  # Đánh dấu hoàn thành\n",
    "    }\n",
    "    \n",
    "    return final_doc\n",
    "\n",
    "def run_generation(batch_size=BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Chạy tóm tắt cho toàn bộ clean_data\n",
    "    \n",
    "    Args:\n",
    "        batch_size: Số lượng bài xử lý mỗi lần\n",
    "    \n",
    "    Returns:\n",
    "        Dict chứa thống kê\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"🚀 GIAI ĐOẠN 6: TẠO TÓM TẮT CHO TOÀN BỘ DỮ LIỆU\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Bước 1: Kết nối MongoDB\n",
    "    clean_col, final_col = connect_mongodb()\n",
    "    if clean_col is None or final_col is None:\n",
    "        print(\"❌ Không thể kết nối MongoDB\")\n",
    "        return None\n",
    "    \n",
    "    # Bước 2: Load model\n",
    "    model, tokenizer, device = load_model()\n",
    "    if model is None:\n",
    "        print(\"❌ Không thể load model. Hãy chạy 4_model_training.py trước.\")\n",
    "        return None\n",
    "    \n",
    "    # Bước 3: Lấy dữ liệu cần xử lý\n",
    "    total_clean = clean_col.count_documents({\"processing_status\": \"cleaned\"})\n",
    "    print(f\"\\n📊 Tổng số bài cần tóm tắt: {total_clean}\")\n",
    "    \n",
    "    if total_clean == 0:\n",
    "        print(\"❌ Không có dữ liệu để xử lý. Hãy chạy 3_data_cleaning.py trước.\")\n",
    "        return None\n",
    "    \n",
    "    stats = {\n",
    "        \"total_processed\": 0,\n",
    "        \"summarized\": 0,\n",
    "        \"errors\": 0,\n",
    "        \"total_summary_length\": 0\n",
    "    }\n",
    "    \n",
    "    # Bước 4: Xử lý từng bài\n",
    "    cursor = clean_col.find({\"processing_status\": \"cleaned\"})\n",
    "    \n",
    "    for idx, clean_doc in enumerate(cursor, 1):\n",
    "        print(f\"\\r[{idx}/{total_clean}] Đang xử lý...\", end=\"\", flush=True)\n",
    "        \n",
    "        stats[\"total_processed\"] += 1\n",
    "        \n",
    "        try:\n",
    "            # Kiểm tra đã tồn tại trong final_output chưa\n",
    "            url = clean_doc.get(\"url\")\n",
    "            if final_col.find_one({\"url\": url}):\n",
    "                continue  # Bỏ qua nếu đã xử lý\n",
    "            \n",
    "            # Process: tóm tắt + phân tích\n",
    "            final_doc = process_article(clean_doc, model, tokenizer, device)\n",
    "            \n",
    "            # Lưu vào final_output\n",
    "            final_col.insert_one(final_doc)\n",
    "            \n",
    "            stats[\"summarized\"] += 1\n",
    "            stats[\"total_summary_length\"] += final_doc[\"summary_length\"]\n",
    "            \n",
    "            # Cập nhật trạng thái trong clean_data\n",
    "            clean_col.update_one(\n",
    "                {\"_id\": clean_doc[\"_id\"]},\n",
    "                {\"$set\": {\"processing_status\": \"summarized\"}}\n",
    "            )\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"\\n  ❌ Lỗi xử lý: {e}\")\n",
    "            stats[\"errors\"] += 1\n",
    "    \n",
    "    print()  # Xuống dòng sau progress bar\n",
    "    \n",
    "    # In báo cáo\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"✅ HOÀN THÀNH GIAI ĐOẠN 6\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"📊 Thống kê:\")\n",
    "    print(f\"   • Tổng số đã xử lý: {stats['total_processed']}\")\n",
    "    print(f\"   • Đã tóm tắt thành công: {stats['summarized']}\")\n",
    "    print(f\"   • Lỗi: {stats['errors']}\")\n",
    "    \n",
    "    if stats['summarized'] > 0:\n",
    "        avg_summary_length = stats['total_summary_length'] / stats['summarized']\n",
    "        print(f\"   • Độ dài summary trung bình: {avg_summary_length:.1f} từ\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"🎉 HOÀN THÀNH TẤT CẢ CÁC GIAI ĐOẠN!\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\n📁 Dữ liệu cuối cùng đã lưu trong collection: '{FINAL_COLLECTION}'\")\n",
    "    print(f\"\\n💡 Cấu trúc document trong final_output:\")\n",
    "    print(f\"   • title_clean: Tiêu đề đã làm sạch\")\n",
    "    print(f\"   • content_clean: Nội dung đầy đủ đã làm sạch\")\n",
    "    print(f\"   • summary: Tóm tắt bằng AI (ViT5)\")\n",
    "    print(f\"   • analysis: Phân tích ngữ cảnh giao thông\")\n",
    "    print(f\"   • keywords: Từ khóa chính\")\n",
    "    print(f\"   • traffic_entities: Các thực thể giao thông được đề cập\")\n",
    "    print(f\"   • category: Danh mục bài viết\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\\n\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# ==================== CHẠY CHƯƠNG TRÌNH ====================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    \n",
    "    print(f\"\\n📝 Giai đoạn này sẽ:\")\n",
    "    print(f\"   1. Load mô hình ViT5 đã fine-tune\")\n",
    "    print(f\"   2. Tóm tắt từng bài báo trong clean_data\")\n",
    "    print(f\"   3. Phân tích ngữ cảnh giao thông\")\n",
    "    print(f\"   4. Lưu kết quả vào final_output collection\")\n",
    "    print(f\"\\n⏱️  Thời gian dự kiến: 5-30 phút tùy số lượng bài\\n\")\n",
    "    \n",
    "    # Chạy generation\n",
    "    results = run_generation()\n",
    "    \n",
    "    if results:\n",
    "        print(f\"\\n✅ Tóm tắt hoàn tất!\")\n",
    "        print(f\"📊 Đã xử lý: {results['summarized']} bài\")\n",
    "        print(f\"\\n💡 Để xem kết quả, bạn có thể:\")\n",
    "        print(f\"   • Kết nối MongoDB và query collection '{FINAL_COLLECTION}'\")\n",
    "        print(f\"   • Chạy API server để truy vấn qua HTTP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77dbcb0",
   "metadata": {},
   "source": [
    "# HỆ THỐNG TÓM TẮT BÁO GIAO THÔNG TP.HCM 2025\n",
    "\n",
    "## 📋 Tổng quan\n",
    "\n",
    "Hệ thống tự động thu thập, làm sạch, và tóm tắt tin tức giao thông TP.HCM năm 2025 sử dụng mô hình **ViT5** (Vietnamese T5).\n",
    "\n",
    "### 🎯 Mục tiêu\n",
    "- Thu thập dữ liệu từ nhiều nguồn báo tiếng Việt\n",
    "- Lọc chỉ giữ bài về giao thông TP.HCM năm 2025\n",
    "- Làm sạch và chuẩn hóa dữ liệu\n",
    "- Fine-tune mô hình ViT5 cho tóm tắt tiếng Việt\n",
    "- Tạo summary và phân tích ngữ cảnh giao thông\n",
    "\n",
    "---\n",
    "\n",
    "## 🏗️ Kiến trúc hệ thống\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│ 1. DATA COLLECTION (1_data_collection.py)                  │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│ • Scrape từ RSS feeds + pagination                          │\n",
    "│ • Lưu vào MongoDB: raw_data (dữ liệu thô)                  │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "                            ↓\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│ 2. DATA FILTERING (2_data_filtering.py)                    │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│ • Lọc: chỉ giữ bài giao thông TP.HCM 2025                  │\n",
    "│ • Gán nhãn category tự động                                 │\n",
    "│ • Lưu vào MongoDB: filtered_data                            │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "                            ↓\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│ 3. DATA CLEANING (3_data_cleaning.py)                      │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│ • Loại HTML, ký tự đặc biệt, spam                          │\n",
    "│ • Chuẩn hóa unicode, dấu câu, khoảng trắng                 │\n",
    "│ • Tách câu (sentence segmentation)                          │\n",
    "│ • Lưu vào MongoDB: clean_data                               │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "                            ↓\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│ 4. MODEL TRAINING (4_model_training.py)                    │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│ • Load ViT5 pre-trained                                     │\n",
    "│ • Fine-tune trên data giao thông TP.HCM                    │\n",
    "│ • Split: 70% train, 15% val, 15% test                      │\n",
    "│ • Lưu model: ./models/vit5_hcmc_traffic                    │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "                            ↓\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│ 5. MODEL EVALUATION (5_model_evaluation.py)                │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│ • Đánh giá trên test set                                    │\n",
    "│ • Metrics: ROUGE-1, ROUGE-2, ROUGE-L, BLEU                 │\n",
    "│ • Kiểm tra overfitting                                      │\n",
    "│ • Lưu báo cáo: evaluation_results.json                      │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "                            ↓\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│ 6. GENERATE SUMMARIES (6_generate_summaries.py)            │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│ • Tóm tắt toàn bộ clean_data bằng ViT5                     │\n",
    "│ • Phân tích ngữ cảnh giao thông                            │\n",
    "│ • Trích xuất keywords                                       │\n",
    "│ • Lưu vào MongoDB: final_output (KẾT QUẢ CUỐI CÙNG)       │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 Cài đặt\n",
    "\n",
    "### 1. Yêu cầu hệ thống\n",
    "- **Python**: 3.8 hoặc mới hơn\n",
    "- **MongoDB**: 4.0 hoặc mới hơn\n",
    "- **GPU**: Khuyến nghị có GPU với 8GB+ VRAM (training sẽ chậm nếu dùng CPU)\n",
    "- **RAM**: Tối thiểu 8GB\n",
    "- **Disk**: Ít nhất 5GB trống\n",
    "\n",
    "### 2. Cài đặt dependencies\n",
    "\n",
    "```bash\n",
    "# Tạo virtual environment (khuyến nghị)\n",
    "python -m venv venv\n",
    "source venv/bin/activate  # Linux/Mac\n",
    "# hoặc\n",
    "venv\\Scripts\\activate  # Windows\n",
    "\n",
    "# Cài đặt packages\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "**requirements.txt:**\n",
    "```\n",
    "# Web scraping\n",
    "selenium==4.15.0\n",
    "beautifulsoup4==4.12.0\n",
    "feedparser==6.0.10\n",
    "\n",
    "# MongoDB\n",
    "pymongo==4.6.0\n",
    "\n",
    "# Machine Learning\n",
    "torch==2.1.0\n",
    "transformers==4.35.0\n",
    "datasets==2.15.0\n",
    "\n",
    "# Evaluation\n",
    "rouge-score==0.1.2\n",
    "nltk==3.8.1\n",
    "\n",
    "# Utilities\n",
    "numpy==1.24.0\n",
    "scikit-learn==1.3.0\n",
    "\n",
    "# Visualization\n",
    "matplotlib==3.8.0\n",
    "\n",
    "# API (optional)\n",
    "fastapi==0.104.0\n",
    "uvicorn==0.24.0\n",
    "```\n",
    "\n",
    "### 3. Cài đặt Edge WebDriver\n",
    "\n",
    "```bash\n",
    "# Download Edge WebDriver tương ứng với phiên bản Edge của bạn\n",
    "# Đặt vào PATH hoặc cùng thư mục với code\n",
    "```\n",
    "\n",
    "### 4. Khởi động MongoDB\n",
    "\n",
    "```bash\n",
    "# Linux/Mac\n",
    "sudo systemctl start mongod\n",
    "\n",
    "# Windows: MongoDB sẽ tự chạy như service\n",
    "# Hoặc chạy thủ công:\n",
    "mongod --dbpath C:\\data\\db\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Hướng dẫn sử dụng\n",
    "\n",
    "### Quy trình đầy đủ (6 bước)\n",
    "\n",
    "#### **Bước 1: Thu thập dữ liệu thô**\n",
    "```bash\n",
    "python 1_data_collection.py 50\n",
    "# 50 = số bài tối đa mỗi nguồn\n",
    "```\n",
    "**Output:** MongoDB collection `raw_data`\n",
    "\n",
    "---\n",
    "\n",
    "#### **Bước 2: Lọc và gán nhãn**\n",
    "```bash\n",
    "python 2_data_filtering.py\n",
    "```\n",
    "**Output:** MongoDB collection `filtered_data` (chỉ bài về giao thông TP.HCM 2025)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Bước 3: Làm sạch dữ liệu**\n",
    "```bash\n",
    "python 3_data_cleaning.py\n",
    "```\n",
    "**Output:** MongoDB collection `clean_data`\n",
    "\n",
    "---\n",
    "\n",
    "#### **Bước 4: Huấn luyện mô hình** ⚠️ **CẦN GPU**\n",
    "```bash\n",
    "python 4_model_training.py --auto\n",
    "# Bỏ --auto nếu muốn xác nhận trước khi train\n",
    "```\n",
    "**Output:** \n",
    "- Model: `./models/vit5_hcmc_traffic/`\n",
    "- Test set: `./models/vit5_hcmc_traffic/test_data.json`\n",
    "\n",
    "⏱️ **Thời gian:** 1-3 giờ tùy GPU và số lượng dữ liệu\n",
    "\n",
    "---\n",
    "\n",
    "#### **Bước 5: Đánh giá mô hình**\n",
    "```bash\n",
    "python 5_model_evaluation.py\n",
    "```\n",
    "**Output:**\n",
    "- `./models/vit5_hcmc_traffic/evaluation_results.json`\n",
    "- `./models/vit5_hcmc_traffic/evaluation_examples.json`\n",
    "- `./models/vit5_hcmc_traffic/training_curves.png`\n",
    "\n",
    "---\n",
    "\n",
    "#### **Bước 6: Tạo tóm tắt cho toàn bộ dữ liệu** 🎉\n",
    "```bash\n",
    "python 6_generate_summaries.py\n",
    "```\n",
    "**Output:** MongoDB collection `final_output` với:\n",
    "- `summary`: Tóm tắt bằng ViT5\n",
    "- `analysis`: Phân tích ngữ cảnh giao thông\n",
    "- `keywords`: Từ khóa chính\n",
    "- `traffic_entities`: Các thực thể giao thông\n",
    "\n",
    "⏱️ **Thời gian:** 5-30 phút\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Cấu trúc MongoDB Collections\n",
    "\n",
    "### 1. **raw_data** (Dữ liệu thô)\n",
    "```json\n",
    "{\n",
    "  \"url\": \"https://...\",\n",
    "  \"source\": \"laodong\",\n",
    "  \"title\": \"Tiêu đề gốc...\",\n",
    "  \"content_text\": \"Nội dung gốc...\",\n",
    "  \"content_html\": \"<html>...\",\n",
    "  \"date_raw\": \"11/10/2025 14:30\",\n",
    "  \"content_hash\": \"abc123...\",\n",
    "  \"scraped_at\": \"2025-10-11T14:30:00\",\n",
    "  \"processing_status\": \"raw\"\n",
    "}\n",
    "```\n",
    "\n",
    "### 2. **filtered_data** (Đã lọc)\n",
    "```json\n",
    "{\n",
    "  \"url\": \"https://...\",\n",
    "  \"title\": \"Tiêu đề...\",\n",
    "  \"content_text\": \"Nội dung...\",\n",
    "  \"published_date\": \"2025-10-11T00:00:00\",\n",
    "  \"category\": \"projects\",\n",
    "  \"category_name\": \"Dự án giao thông\",\n",
    "  \"is_hcmc\": true,\n",
    "  \"is_traffic\": true,\n",
    "  \"processing_status\": \"filtered\"\n",
    "}\n",
    "```\n",
    "\n",
    "### 3. **clean_data** (Đã làm sạch)\n",
    "```json\n",
    "{\n",
    "  \"url\": \"https://...\",\n",
    "  \"title_clean\": \"Tiêu đề đã làm sạch\",\n",
    "  \"content_clean\": \"Nội dung đã làm sạch...\",\n",
    "  \"sentences\": [\"Câu 1.\", \"Câu 2.\", ...],\n",
    "  \"num_sentences\": 25,\n",
    "  \"num_words\": 450,\n",
    "  \"category\": \"projects\",\n",
    "  \"published_date\": \"2025-10-11T00:00:00\",\n",
    "  \"processing_status\": \"cleaned\"\n",
    "}\n",
    "```\n",
    "\n",
    "### 4. **final_output** (KẾT QUẢ CUỐI CÙNG) ⭐\n",
    "```json\n",
    "{\n",
    "  \"url\": \"https://...\",\n",
    "  \"title_clean\": \"Tiêu đề\",\n",
    "  \"content_clean\": \"Nội dung đầy đủ...\",\n",
    "  \n",
    "  \"summary\": \"Tóm tắt bằng ViT5...\",\n",
    "  \"analysis\": \"Chủ đề: Dự án giao thông. Hạ tầng: metro...\",\n",
    "  \n",
    "  \"keywords\": [\"metro\", \"thủ đức\", \"dự án\", ...],\n",
    "  \"traffic_entities\": {\n",
    "    \"mentioned_infrastructure\": [\"metro\", \"cầu\"],\n",
    "    \"mentioned_locations\": [\"thủ đức\", \"quận 2\"],\n",
    "    \"mentioned_issues\": [],\n",
    "    \"mentioned_projects\": [\"dự án\", \"khởi công\"],\n",
    "    \"main_topic\": \"Dự án giao thông\"\n",
    "  },\n",
    "  \n",
    "  \"category\": \"projects\",\n",
    "  \"published_date\": \"2025-10-11T00:00:00\",\n",
    "  \"summary_length\": 45,\n",
    "  \"summarized_at\": \"2025-10-12T10:00:00\",\n",
    "  \"processing_status\": \"completed\"\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📈 Đánh giá mô hình\n",
    "\n",
    "### Metrics sử dụng\n",
    "\n",
    "| Metric | Ý nghĩa | Điểm tốt |\n",
    "|--------|---------|----------|\n",
    "| **ROUGE-1** | Độ trùng 1-gram (từ đơn) | > 0.35 |\n",
    "| **ROUGE-2** | Độ trùng 2-gram (cụm 2 từ) | > 0.15 |\n",
    "| **ROUGE-L** | Chuỗi con chung dài nhất | > 0.30 |\n",
    "| **BLEU** | Độ chính xác n-gram | > 0.20 |\n",
    "\n",
    "### Đánh giá Overfitting\n",
    "\n",
    "- **Good**: Train/Val gap < 10%\n",
    "- **Moderate**: Gap 10-20%\n",
    "- **High**: Gap > 20% → Cần giảm model complexity hoặc tăng data\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Lưu ý quan trọng\n",
    "\n",
    "### 1. **Về dữ liệu huấn luyện**\n",
    "- Hiện tại code sử dụng **pseudo-labels**: lấy 3 câu đầu tiên làm summary\n",
    "- Đây là phương pháp tạm thời, chưa tối ưu\n",
    "- **Khuyến nghị**: Có human-labeled summaries để train tốt hơn\n",
    "\n",
    "### 2. **Về GPU**\n",
    "- Training ViT5 **yêu cầu GPU** (ít nhất 8GB VRAM)\n",
    "- Nếu dùng CPU: rất chậm (10-20x)\n",
    "- Giải pháp:\n",
    "  - Dùng Google Colab (free GPU)\n",
    "  - Giảm `BATCH_SIZE` trong code\n",
    "  - Dùng mô hình nhỏ hơn (ViT5-small thay vì ViT5-base)\n",
    "\n",
    "### 3. **Về số lượng dữ liệu**\n",
    "- **Tối thiểu**: 100 bài để train\n",
    "- **Khuyến nghị**: 500-1000 bài\n",
    "- **Tốt nhất**: 2000+ bài\n",
    "\n",
    "### 4. **Về thời gian**\n",
    "| Bước | Thời gian | Yêu cầu |\n",
    "|------|-----------|---------|\n",
    "| 1. Collection | 10-30 phút | Internet |\n",
    "| 2. Filtering | 1-5 phút | CPU |\n",
    "| 3. Cleaning | 1-5 phút | CPU |\n",
    "| 4. Training | 1-3 giờ | **GPU** |\n",
    "| 5. Evaluation | 5-15 phút | GPU/CPU |\n",
    "| 6. Generation | 5-30 phút | GPU/CPU |\n",
    "\n",
    "---\n",
    "\n",
    "## 🐛 Xử lý lỗi thường gặp\n",
    "\n",
    "### Lỗi 1: \"Can't connect to MongoDB\"\n",
    "```bash\n",
    "# Kiểm tra MongoDB đã chạy chưa\n",
    "sudo systemctl status mongod\n",
    "\n",
    "# Khởi động MongoDB\n",
    "sudo systemctl start mongod\n",
    "```\n",
    "\n",
    "### Lỗi 2: \"Model not found\"\n",
    "→ Bạn chưa chạy bước 4 (training). Chạy `python 4_model_training.py` trước.\n",
    "\n",
    "### Lỗi 3: \"CUDA out of memory\"\n",
    "→ Giảm `BATCH_SIZE` trong file training (ví dụ: từ 8 → 4 → 2)\n",
    "\n",
    "### Lỗi 4: \"No data in collection\"\n",
    "→ Kiểm tra lại các bước trước đã chạy thành công chưa.\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 Tài liệu tham khảo\n",
    "\n",
    "- **ViT5 Model**: https://huggingface.co/VietAI/vit5-base\n",
    "- **PhoBERT**: https://github.com/VinAIResearch/PhoBERT\n",
    "- **Transformers**: https://huggingface.co/docs/transformers\n",
    "- **ROUGE Metric**: https://github.com/google-research/google-research/tree/master/rouge\n",
    "\n",
    "---\n",
    "\n",
    "## 🤝 Đóng góp\n",
    "\n",
    "Nếu bạn muốn cải thiện hệ thống:\n",
    "1. Thêm nguồn tin mới vào `NEWS_SOURCES`\n",
    "2. Cải thiện thuật toán phân loại\n",
    "3. Thêm metrics đánh giá khác (BERTScore, METEOR)\n",
    "4. Fine-tune với human-labeled data\n",
    "\n",
    "---\n",
    "\n",
    "## 📝 License\n",
    "\n",
    "MIT License - Tự do sử dụng cho mục đích học tập và nghiên cứu.\n",
    "\n",
    "---\n",
    "\n",
    "## 📧 Liên hệ\n",
    "\n",
    "Nếu có thắc mắc, vui lòng tạo issue trên GitHub hoặc liên hệ qua email.\n",
    "\n",
    "---\n",
    "\n",
    "**🎉 Chúc bạn thành công với đề tài tóm tắt báo giao thông TP.HCM 2025!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8d6ee1",
   "metadata": {},
   "source": [
    "API ĐỂ TRUY VẤN KẾT QUẢ TÓM TẮT\n",
    "================================\n",
    "FastAPI server để query dữ liệu từ final_output collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f48d080",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "API ĐỂ TRUY VẤN KẾT QUẢ TÓM TẮT\n",
    "================================\n",
    "FastAPI server để query dữ liệu từ final_output collection\n",
    "\"\"\"\n",
    "\n",
    "from fastapi import FastAPI, HTTPException, Query\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pymongo import MongoClient\n",
    "from bson import ObjectId\n",
    "from datetime import datetime\n",
    "from typing import Optional, List\n",
    "import json\n",
    "\n",
    "# ==================== CẤU HÌNH ====================\n",
    "\n",
    "MONGODB_URI = \"mongodb://localhost:27017/\"\n",
    "DATABASE_NAME = \"hcmc_traffic_summarization\"\n",
    "FINAL_COLLECTION = \"final_output\"\n",
    "\n",
    "# ==================== KẾT NỐI DATABASE ====================\n",
    "\n",
    "try:\n",
    "    client = MongoClient(MONGODB_URI, serverSelectionTimeoutMS=5000)\n",
    "    client.server_info()\n",
    "    db = client[DATABASE_NAME]\n",
    "    final_col = db[FINAL_COLLECTION]\n",
    "    print(f\"✅ Đã kết nối MongoDB: {FINAL_COLLECTION}\")\n",
    "    print(f\"   Tổng số bài: {final_col.count_documents({})}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Lỗi MongoDB: {e}\")\n",
    "    final_col = None\n",
    "\n",
    "# ==================== FASTAPI APP ====================\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"TP.HCM Traffic News Summarization API\",\n",
    "    description=\"API để truy vấn tin tức giao thông TP.HCM đã được tóm tắt bằng ViT5\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# CORS middleware (cho phép truy cập từ frontend)\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# ==================== HELPER FUNCTIONS ====================\n",
    "\n",
    "def serialize_doc(doc):\n",
    "    \"\"\"\n",
    "    Chuyển MongoDB document sang JSON-serializable dict\n",
    "    \"\"\"\n",
    "    if doc is None:\n",
    "        return None\n",
    "    \n",
    "    doc[\"id\"] = str(doc[\"_id\"])\n",
    "    del doc[\"_id\"]\n",
    "    \n",
    "    # Convert datetime to ISO string\n",
    "    for key in [\"published_date\", \"scraped_at\", \"cleaned_at\", \"summarized_at\"]:\n",
    "        if key in doc and isinstance(doc[key], datetime):\n",
    "            doc[key] = doc[key].isoformat()\n",
    "    \n",
    "    return doc\n",
    "\n",
    "# ==================== API ENDPOINTS ====================\n",
    "\n",
    "@app.get(\"/\")\n",
    "def root():\n",
    "    \"\"\"\n",
    "    Thông tin API\n",
    "    \"\"\"\n",
    "    if final_col is None:\n",
    "        raise HTTPException(status_code=503, detail=\"Database không khả dụng\")\n",
    "    \n",
    "    total = final_col.count_documents({})\n",
    "    \n",
    "    return {\n",
    "        \"name\": \"TP.HCM Traffic News Summarization API\",\n",
    "        \"version\": \"1.0.0\",\n",
    "        \"description\": \"API để truy vấn tin tức giao thông TP.HCM đã tóm tắt\",\n",
    "        \"total_articles\": total,\n",
    "        \"endpoints\": {\n",
    "            \"GET /\": \"API info\",\n",
    "            \"GET /articles\": \"Danh sách bài viết (có filter)\",\n",
    "            \"GET /articles/{id}\": \"Chi tiết 1 bài viết\",\n",
    "            \"GET /search\": \"Tìm kiếm full-text\",\n",
    "            \"GET /stats\": \"Thống kê tổng quan\",\n",
    "            \"GET /categories\": \"Danh sách categories\",\n",
    "            \"GET /trending\": \"Từ khóa trending\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "@app.get(\"/articles\")\n",
    "def list_articles(\n",
    "    category: Optional[str] = None,\n",
    "    source: Optional[str] = None,\n",
    "    from_date: Optional[str] = Query(None, description=\"YYYY-MM-DD\"),\n",
    "    to_date: Optional[str] = Query(None, description=\"YYYY-MM-DD\"),\n",
    "    limit: int = Query(20, le=100),\n",
    "    skip: int = Query(0, ge=0)\n",
    "):\n",
    "    \"\"\"\n",
    "    Lấy danh sách bài viết với filter\n",
    "    \n",
    "    - **category**: Lọc theo danh mục (projects/infrastructure/issues/planning/transport)\n",
    "    - **source**: Lọc theo nguồn (laodong/vnexpress/tuoitre)\n",
    "    - **from_date**: Lọc từ ngày (YYYY-MM-DD)\n",
    "    - **to_date**: Lọc đến ngày (YYYY-MM-DD)\n",
    "    - **limit**: Số lượng kết quả (tối đa 100)\n",
    "    - **skip**: Bỏ qua N kết quả đầu (cho pagination)\n",
    "    \"\"\"\n",
    "    if final_col is None:\n",
    "        raise HTTPException(status_code=503, detail=\"Database không khả dụng\")\n",
    "    \n",
    "    # Build query\n",
    "    query = {\"processing_status\": \"completed\"}\n",
    "    \n",
    "    if category:\n",
    "        query[\"category\"] = category\n",
    "    \n",
    "    if source:\n",
    "        query[\"source\"] = source\n",
    "    \n",
    "    # Date filter\n",
    "    date_query = {}\n",
    "    if from_date:\n",
    "        try:\n",
    "            date_query[\"$gte\"] = datetime.strptime(from_date, \"%Y-%m-%d\")\n",
    "        except:\n",
    "            raise HTTPException(status_code=400, detail=\"Invalid from_date format\")\n",
    "    \n",
    "    if to_date:\n",
    "        try:\n",
    "            date_query[\"$lte\"] = datetime.strptime(to_date, \"%Y-%m-%d\")\n",
    "        except:\n",
    "            raise HTTPException(status_code=400, detail=\"Invalid to_date format\")\n",
    "    \n",
    "    if date_query:\n",
    "        query[\"published_date\"] = date_query\n",
    "    \n",
    "    # Execute query\n",
    "    cursor = final_col.find(query).sort(\"published_date\", -1).skip(skip).limit(limit)\n",
    "    \n",
    "    articles = []\n",
    "    for doc in cursor:\n",
    "        articles.append({\n",
    "            \"id\": str(doc[\"_id\"]),\n",
    "            \"title\": doc.get(\"title_clean\", \"\"),\n",
    "            \"summary\": doc.get(\"summary\", \"\"),\n",
    "            \"analysis\": doc.get(\"analysis\", \"\"),\n",
    "            \"category\": doc.get(\"category\", \"\"),\n",
    "            \"category_name\": doc.get(\"category_name\", \"\"),\n",
    "            \"source\": doc.get(\"source_name\", \"\"),\n",
    "            \"published_date\": doc.get(\"published_date\", datetime.now()).isoformat(),\n",
    "            \"keywords\": doc.get(\"keywords\", [])[:5],  # Chỉ lấy 5 keywords\n",
    "            \"url\": doc.get(\"url\", \"\")\n",
    "        })\n",
    "    \n",
    "    total = final_col.count_documents(query)\n",
    "    \n",
    "    return {\n",
    "        \"articles\": articles,\n",
    "        \"count\": len(articles),\n",
    "        \"total\": total,\n",
    "        \"skip\": skip,\n",
    "        \"limit\": limit,\n",
    "        \"has_more\": (skip + len(articles)) < total\n",
    "    }\n",
    "\n",
    "@app.get(\"/articles/{article_id}\")\n",
    "def get_article(article_id: str):\n",
    "    \"\"\"\n",
    "    Lấy chi tiết đầy đủ của 1 bài viết\n",
    "    \"\"\"\n",
    "    if final_col is None:\n",
    "        raise HTTPException(status_code=503, detail=\"Database không khả dụng\")\n",
    "    \n",
    "    try:\n",
    "        doc = final_col.find_one({\"_id\": ObjectId(article_id)})\n",
    "    except:\n",
    "        raise HTTPException(status_code=400, detail=\"Invalid article ID\")\n",
    "    \n",
    "    if doc is None:\n",
    "        raise HTTPException(status_code=404, detail=\"Article not found\")\n",
    "    \n",
    "    return serialize_doc(doc)\n",
    "\n",
    "@app.get(\"/search\")\n",
    "def search_articles(\n",
    "    q: str = Query(..., min_length=2, description=\"Từ khóa tìm kiếm\"),\n",
    "    limit: int = Query(20, le=100)\n",
    "):\n",
    "    \"\"\"\n",
    "    Tìm kiếm full-text trong title, summary, analysis\n",
    "    \"\"\"\n",
    "    if final_col is None:\n",
    "        raise HTTPException(status_code=503, detail=\"Database không khả dụng\")\n",
    "    \n",
    "    # Full-text search\n",
    "    cursor = final_col.find(\n",
    "        {\"$text\": {\"$search\": q}},\n",
    "        {\"score\": {\"$meta\": \"textScore\"}}\n",
    "    ).sort([(\"score\", {\"$meta\": \"textScore\"})]).limit(limit)\n",
    "    \n",
    "    results = []\n",
    "    for doc in cursor:\n",
    "        results.append({\n",
    "            \"id\": str(doc[\"_id\"]),\n",
    "            \"title\": doc.get(\"title_clean\", \"\"),\n",
    "            \"summary\": doc.get(\"summary\", \"\"),\n",
    "            \"category\": doc.get(\"category\", \"\"),\n",
    "            \"source\": doc.get(\"source_name\", \"\"),\n",
    "            \"published_date\": doc.get(\"published_date\", datetime.now()).isoformat(),\n",
    "            \"relevance_score\": doc.get(\"score\", 0),\n",
    "            \"url\": doc.get(\"url\", \"\")\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        \"query\": q,\n",
    "        \"results\": results,\n",
    "        \"count\": len(results)\n",
    "    }\n",
    "\n",
    "@app.get(\"/stats\")\n",
    "def get_statistics():\n",
    "    \"\"\"\n",
    "    Thống kê tổng quan\n",
    "    \"\"\"\n",
    "    if final_col is None:\n",
    "        raise HTTPException(status_code=503, detail=\"Database không khả dụng\")\n",
    "    \n",
    "    total = final_col.count_documents({\"processing_status\": \"completed\"})\n",
    "    \n",
    "    # By category\n",
    "    pipeline_category = [\n",
    "        {\"$match\": {\"processing_status\": \"completed\"}},\n",
    "        {\"$group\": {\"_id\": \"$category\", \"count\": {\"$sum\": 1}}},\n",
    "        {\"$sort\": {\"count\": -1}}\n",
    "    ]\n",
    "    by_category = {\n",
    "        doc[\"_id\"]: doc[\"count\"] \n",
    "        for doc in final_col.aggregate(pipeline_category)\n",
    "    }\n",
    "    \n",
    "    # By source\n",
    "    pipeline_source = [\n",
    "        {\"$match\": {\"processing_status\": \"completed\"}},\n",
    "        {\"$group\": {\"_id\": \"$source_name\", \"count\": {\"$sum\": 1}}},\n",
    "        {\"$sort\": {\"count\": -1}}\n",
    "    ]\n",
    "    by_source = {\n",
    "        doc[\"_id\"]: doc[\"count\"] \n",
    "        for doc in final_col.aggregate(pipeline_source)\n",
    "    }\n",
    "    \n",
    "    # By month\n",
    "    pipeline_monthly = [\n",
    "        {\"$match\": {\"processing_status\": \"completed\"}},\n",
    "        {\"$group\": {\n",
    "            \"_id\": {\n",
    "                \"year\": {\"$year\": \"$published_date\"},\n",
    "                \"month\": {\"$month\": \"$published_date\"}\n",
    "            },\n",
    "            \"count\": {\"$sum\": 1}\n",
    "        }},\n",
    "        {\"$sort\": {\"_id.year\": 1, \"_id.month\": 1}}\n",
    "    ]\n",
    "    monthly_data = []\n",
    "    for doc in final_col.aggregate(pipeline_monthly):\n",
    "        monthly_data.append({\n",
    "            \"month\": f\"{doc['_id']['year']}-{doc['_id']['month']:02d}\",\n",
    "            \"count\": doc[\"count\"]\n",
    "        })\n",
    "    \n",
    "    # Average summary length\n",
    "    pipeline_avg = [\n",
    "        {\"$match\": {\"processing_status\": \"completed\"}},\n",
    "        {\"$group\": {\n",
    "            \"_id\": None,\n",
    "            \"avg_summary_length\": {\"$avg\": \"$summary_length\"},\n",
    "            \"avg_num_sentences\": {\"$avg\": \"$num_sentences\"},\n",
    "            \"avg_num_words\": {\"$avg\": \"$num_words\"}\n",
    "        }}\n",
    "    ]\n",
    "    avg_stats = list(final_col.aggregate(pipeline_avg))\n",
    "    avg_data = avg_stats[0] if avg_stats else {}\n",
    "    \n",
    "    return {\n",
    "        \"total_articles\": total,\n",
    "        \"by_category\": by_category,\n",
    "        \"by_source\": by_source,\n",
    "        \"monthly_trend\": monthly_data,\n",
    "        \"averages\": {\n",
    "            \"summary_length\": round(avg_data.get(\"avg_summary_length\", 0), 1),\n",
    "            \"sentences_per_article\": round(avg_data.get(\"avg_num_sentences\", 0), 1),\n",
    "            \"words_per_article\": round(avg_data.get(\"avg_num_words\", 0), 1)\n",
    "        }\n",
    "    }\n",
    "\n",
    "@app.get(\"/categories\")\n",
    "def list_categories():\n",
    "    \"\"\"\n",
    "    Danh sách các categories và số lượng bài\n",
    "    \"\"\"\n",
    "    if final_col is None:\n",
    "        raise HTTPException(status_code=503, detail=\"Database không khả dụng\")\n",
    "    \n",
    "    categories = [\n",
    "        {\n",
    "            \"key\": \"projects\",\n",
    "            \"name\": \"Dự án giao thông\",\n",
    "            \"count\": final_col.count_documents({\"category\": \"projects\"})\n",
    "        },\n",
    "        {\n",
    "            \"key\": \"infrastructure\",\n",
    "            \"name\": \"Hạ tầng giao thông\",\n",
    "            \"count\": final_col.count_documents({\"category\": \"infrastructure\"})\n",
    "        },\n",
    "        {\n",
    "            \"key\": \"issues\",\n",
    "            \"name\": \"Vấn đề giao thông\",\n",
    "            \"count\": final_col.count_documents({\"category\": \"issues\"})\n",
    "        },\n",
    "        {\n",
    "            \"key\": \"planning\",\n",
    "            \"name\": \"Quy hoạch giao thông\",\n",
    "            \"count\": final_col.count_documents({\"category\": \"planning\"})\n",
    "        },\n",
    "        {\n",
    "            \"key\": \"transport\",\n",
    "            \"name\": \"Phương tiện vận tải\",\n",
    "            \"count\": final_col.count_documents({\"category\": \"transport\"})\n",
    "        },\n",
    "        {\n",
    "            \"key\": \"general\",\n",
    "            \"name\": \"Chung\",\n",
    "            \"count\": final_col.count_documents({\"category\": \"general\"})\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"categories\": categories,\n",
    "        \"total\": sum(c[\"count\"] for c in categories)\n",
    "    }\n",
    "\n",
    "@app.get(\"/trending\")\n",
    "def get_trending_keywords(top_n: int = Query(20, le=50)):\n",
    "    \"\"\"\n",
    "    Top keywords trending (xuất hiện nhiều nhất)\n",
    "    \"\"\"\n",
    "    if final_col is None:\n",
    "        raise HTTPException(status_code=503, detail=\"Database không khả dụng\")\n",
    "    \n",
    "    # Aggregate keywords\n",
    "    pipeline = [\n",
    "        {\"$match\": {\"processing_status\": \"completed\"}},\n",
    "        {\"$unwind\": \"$keywords\"},\n",
    "        {\"$group\": {\"_id\": \"$keywords\", \"count\": {\"$sum\": 1}}},\n",
    "        {\"$sort\": {\"count\": -1}},\n",
    "        {\"$limit\": top_n}\n",
    "    ]\n",
    "    \n",
    "    trending = []\n",
    "    for doc in final_col.aggregate(pipeline):\n",
    "        trending.append({\n",
    "            \"keyword\": doc[\"_id\"],\n",
    "            \"count\": doc[\"count\"]\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        \"trending_keywords\": trending,\n",
    "        \"count\": len(trending)\n",
    "    }\n",
    "\n",
    "@app.get(\"/sources\")\n",
    "def list_sources():\n",
    "    \"\"\"\n",
    "    Danh sách các nguồn tin\n",
    "    \"\"\"\n",
    "    if final_col is None:\n",
    "        raise HTTPException(status_code=503, detail=\"Database không khả dụng\")\n",
    "    \n",
    "    pipeline = [\n",
    "        {\"$match\": {\"processing_status\": \"completed\"}},\n",
    "        {\"$group\": {\n",
    "            \"_id\": {\n",
    "                \"source\": \"$source\",\n",
    "                \"source_name\": \"$source_name\"\n",
    "            },\n",
    "            \"count\": {\"$sum\": 1}\n",
    "        }},\n",
    "        {\"$sort\": {\"count\": -1}}\n",
    "    ]\n",
    "    \n",
    "    sources = []\n",
    "    for doc in final_col.aggregate(pipeline):\n",
    "        sources.append({\n",
    "            \"key\": doc[\"_id\"][\"source\"],\n",
    "            \"name\": doc[\"_id\"][\"source_name\"],\n",
    "            \"count\": doc[\"count\"]\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        \"sources\": sources,\n",
    "        \"total\": sum(s[\"count\"] for s in sources)\n",
    "    }\n",
    "\n",
    "@app.get(\"/export\")\n",
    "def export_data(\n",
    "    format: str = Query(\"json\", regex=\"^(json|csv)$\"),\n",
    "    category: Optional[str] = None,\n",
    "    limit: int = Query(1000, le=10000)\n",
    "):\n",
    "    \"\"\"\n",
    "    Export dữ liệu ra JSON hoặc CSV\n",
    "    \"\"\"\n",
    "    if final_col is None:\n",
    "        raise HTTPException(status_code=503, detail=\"Database không khả dụng\")\n",
    "    \n",
    "    query = {\"processing_status\": \"completed\"}\n",
    "    if category:\n",
    "        query[\"category\"] = category\n",
    "    \n",
    "    cursor = final_col.find(query).limit(limit)\n",
    "    \n",
    "    articles = []\n",
    "    for doc in cursor:\n",
    "        articles.append({\n",
    "            \"id\": str(doc[\"_id\"]),\n",
    "            \"title\": doc.get(\"title_clean\", \"\"),\n",
    "            \"summary\": doc.get(\"summary\", \"\"),\n",
    "            \"analysis\": doc.get(\"analysis\", \"\"),\n",
    "            \"category\": doc.get(\"category\", \"\"),\n",
    "            \"source\": doc.get(\"source_name\", \"\"),\n",
    "            \"published_date\": doc.get(\"published_date\", datetime.now()).isoformat(),\n",
    "            \"url\": doc.get(\"url\", \"\")\n",
    "        })\n",
    "    \n",
    "    if format == \"csv\":\n",
    "        # Simple CSV\n",
    "        csv_lines = [\"ID,Title,Summary,Category,Source,Date,URL\"]\n",
    "        for art in articles:\n",
    "            csv_lines.append(\n",
    "                f\"{art['id']},\"\n",
    "                f\"\\\"{art['title']}\\\",\"\n",
    "                f\"\\\"{art['summary']}\\\",\"\n",
    "                f\"{art['category']},\"\n",
    "                f\"{art['source']},\"\n",
    "                f\"{art['published_date']},\"\n",
    "                f\"{art['url']}\"\n",
    "            )\n",
    "        \n",
    "        return {\n",
    "            \"format\": \"csv\",\n",
    "            \"data\": \"\\n\".join(csv_lines),\n",
    "            \"count\": len(articles)\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        \"format\": \"json\",\n",
    "        \"articles\": articles,\n",
    "        \"count\": len(articles)\n",
    "    }\n",
    "\n",
    "# ==================== CHẠY SERVER ====================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"🚀 KHỞI ĐỘNG API SERVER\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\n📡 Server sẽ chạy tại: http://localhost:8000\")\n",
    "    print(f\"\\n📖 Swagger UI (API docs): http://localhost:8000/docs\")\n",
    "    print(f\"📖 ReDoc: http://localhost:8000/redoc\")\n",
    "    print(f\"\\n💡 Ví dụ sử dụng:\")\n",
    "    print(f\"   curl http://localhost:8000/stats\")\n",
    "    print(f\"   curl http://localhost:8000/articles?category=projects&limit=10\")\n",
    "    print(f\"   curl http://localhost:8000/search?q=metro\")\n",
    "    print(f\"\\n{'='*70}\\n\")\n",
    "    \n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
